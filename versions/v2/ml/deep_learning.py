#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Deep Learning Models - æ·±åº¦å­¦ä¹ æ¨¡å‹

LSTM/GRU æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹
"""
import os
import sys
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple

# å°è¯•å¯¼å…¥ PyTorch
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# æ·»åŠ çˆ¶ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)


def check_torch_available() -> bool:
    """æ£€æŸ¥ PyTorch æ˜¯å¦å¯ç”¨"""
    return TORCH_AVAILABLE


class StockDataset(Dataset):
    """è‚¡ç¥¨æ—¶åºæ•°æ®é›†"""
    
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class LSTMModel(nn.Module):
    """LSTM ä»·æ ¼é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size: int, hidden_size: int = 64, 
                 num_layers: int = 2, dropout: float = 0.2,
                 output_size: int = 1):
        super(LSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, output_size)
        )
    
    def forward(self, x):
        # x: (batch, seq_len, features)
        lstm_out, (h_n, c_n) = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        out = self.fc(last_output)
        return out


class GRUModel(nn.Module):
    """GRU ä»·æ ¼é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size: int, hidden_size: int = 64,
                 num_layers: int = 2, dropout: float = 0.2,
                 output_size: int = 1):
        super(GRUModel, self).__init__()
        
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, output_size)
        )
    
    def forward(self, x):
        gru_out, h_n = self.gru(x)
        last_output = gru_out[:, -1, :]
        out = self.fc(last_output)
        return out


def prepare_sequence_data(df: pd.DataFrame, seq_length: int = 20, 
                          target_col: str = 'Close', 
                          feature_cols: List[str] = None) -> Tuple[np.ndarray, np.ndarray]:
    """
    å‡†å¤‡æ—¶åºè®­ç»ƒæ•°æ®
    
    Args:
        df: åŒ…å« OHLCV çš„ DataFrame
        seq_length: åºåˆ—é•¿åº¦ (å›çœ‹å¤©æ•°)
        target_col: ç›®æ ‡åˆ—
        feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨
    
    Returns:
        (X, y): è®­ç»ƒæ•°æ®
    """
    if feature_cols is None:
        feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
    
    # å½’ä¸€åŒ–
    data = df[feature_cols].values
    target = df[target_col].values
    
    # Min-Max å½’ä¸€åŒ–
    from sklearn.preprocessing import MinMaxScaler
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    data_scaled = scaler_X.fit_transform(data)
    target_scaled = scaler_y.fit_transform(target.reshape(-1, 1))
    
    # åˆ›å»ºåºåˆ—
    X, y = [], []
    for i in range(len(data_scaled) - seq_length):
        X.append(data_scaled[i:i + seq_length])
        # é¢„æµ‹ä¸‹ä¸€å¤©çš„æ”¶ç›˜ä»·
        y.append(target_scaled[i + seq_length])
    
    return np.array(X), np.array(y), scaler_y


class DeepLearningTrainer:
    """æ·±åº¦å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, model_type: str = 'LSTM', 
                 hidden_size: int = 64, num_layers: int = 2):
        self.model_type = model_type
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.model = None
        self.scaler_y = None
        self.history = {'train_loss': [], 'val_loss': []}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def build_model(self, input_size: int):
        """æ„å»ºæ¨¡å‹"""
        if self.model_type == 'LSTM':
            self.model = LSTMModel(
                input_size=input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers
            )
        else:  # GRU
            self.model = GRUModel(
                input_size=input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers
            )
        
        self.model.to(self.device)
        return self.model
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: np.ndarray = None, y_val: np.ndarray = None,
              epochs: int = 50, batch_size: int = 32, 
              learning_rate: float = 0.001) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹
        
        Returns:
            è®­ç»ƒå†å²å’ŒæŒ‡æ ‡
        """
        if self.model is None:
            self.build_model(X_train.shape[2])
        
        # æ•°æ®é›†
        train_dataset = StockDataset(X_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        if X_val is not None:
            val_dataset = StockDataset(X_val, y_val)
            val_loader = DataLoader(val_dataset, batch_size=batch_size)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå¾ªç¯
        self.history = {'train_loss': [], 'val_loss': []}
        
        for epoch in range(epochs):
            self.model.train()
            train_losses = []
            
            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_losses.append(loss.item())
            
            avg_train_loss = np.mean(train_losses)
            self.history['train_loss'].append(avg_train_loss)
            
            # éªŒè¯
            if X_val is not None:
                self.model.eval()
                val_losses = []
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        batch_X = batch_X.to(self.device)
                        batch_y = batch_y.to(self.device)
                        outputs = self.model(batch_X)
                        loss = criterion(outputs, batch_y)
                        val_losses.append(loss.item())
                
                avg_val_loss = np.mean(val_losses)
                self.history['val_loss'].append(avg_val_loss)
        
        return {
            'final_train_loss': self.history['train_loss'][-1],
            'final_val_loss': self.history['val_loss'][-1] if self.history['val_loss'] else None,
            'epochs': epochs,
            'model_type': self.model_type
        }
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        self.model.eval()
        X_tensor = torch.FloatTensor(X).to(self.device)
        
        with torch.no_grad():
            predictions = self.model(X_tensor)
        
        return predictions.cpu().numpy()
    
    def get_training_chart_data(self) -> Dict:
        """è·å–è®­ç»ƒæ›²çº¿æ•°æ®"""
        return {
            'epochs': list(range(1, len(self.history['train_loss']) + 1)),
            'train_loss': self.history['train_loss'],
            'val_loss': self.history['val_loss']
        }


def train_price_predictor(symbol: str, days: int = 100, 
                          seq_length: int = 20, epochs: int = 50,
                          model_type: str = 'LSTM') -> Dict:
    """
    è®­ç»ƒä»·æ ¼é¢„æµ‹å™¨
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        days: ä½¿ç”¨å¤šå°‘å¤©çš„å†å²æ•°æ®
        seq_length: åºåˆ—é•¿åº¦
        epochs: è®­ç»ƒè½®æ•°
        model_type: 'LSTM' æˆ– 'GRU'
    
    Returns:
        è®­ç»ƒç»“æœ
    """
    from data_fetcher import get_us_stock_data
    from sklearn.model_selection import train_test_split
    
    # è·å–æ•°æ®
    df = get_us_stock_data(symbol, days=days)
    if df is None or len(df) < seq_length + 10:
        return {'error': f'Insufficient data for {symbol}'}
    
    # å‡†å¤‡åºåˆ—æ•°æ®
    X, y, scaler = prepare_sequence_data(df, seq_length=seq_length)
    
    if len(X) < 20:
        return {'error': 'Not enough sequences'}
    
    # åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)
    
    # è®­ç»ƒ
    trainer = DeepLearningTrainer(model_type=model_type)
    trainer.build_model(X.shape[2])
    trainer.scaler_y = scaler
    
    metrics = trainer.train(X_train, y_train, X_val, y_val, epochs=epochs)
    
    # é¢„æµ‹æœ€åä¸€æ®µ
    predictions = trainer.predict(X_val)
    predictions_rescaled = scaler.inverse_transform(predictions)
    actuals_rescaled = scaler.inverse_transform(y_val)
    
    # è®¡ç®—æŒ‡æ ‡
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    mae = mean_absolute_error(actuals_rescaled, predictions_rescaled)
    rmse = np.sqrt(mean_squared_error(actuals_rescaled, predictions_rescaled))
    
    # æ–¹å‘å‡†ç¡®ç‡
    direction_correct = np.sum(
        (predictions_rescaled[1:] - predictions_rescaled[:-1]) * 
        (actuals_rescaled[1:] - actuals_rescaled[:-1]) > 0
    )
    direction_accuracy = direction_correct / (len(predictions_rescaled) - 1) if len(predictions_rescaled) > 1 else 0
    
    return {
        'symbol': symbol,
        'model_type': model_type,
        'epochs': epochs,
        'mae': float(mae),
        'rmse': float(rmse),
        'direction_accuracy': float(direction_accuracy),
        'train_loss': metrics['final_train_loss'],
        'val_loss': metrics['final_val_loss'],
        'chart_data': trainer.get_training_chart_data(),
        'predictions': predictions_rescaled.flatten().tolist()[-10:],
        'actuals': actuals_rescaled.flatten().tolist()[-10:]
    }


if __name__ == "__main__":
    if not TORCH_AVAILABLE:
        print("âŒ PyTorch not installed. Run: pip install torch")
    else:
        print("âœ… PyTorch available")
        print(f"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        
        # æµ‹è¯•è®­ç»ƒ
        result = train_price_predictor('AAPL', days=100, epochs=20)
        print(f"\nğŸ§  Training Result:")
        print(f"   MAE: ${result.get('mae', 0):.2f}")
        print(f"   RMSE: ${result.get('rmse', 0):.2f}")
        print(f"   Direction Accuracy: {result.get('direction_accuracy', 0):.1%}")
