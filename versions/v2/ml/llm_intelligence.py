#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM Intelligence Module - å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½åˆ†æ

åŠŸèƒ½:
- æ–°é—»æƒ…æ„Ÿåˆ†æ
- è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- å¸‚åœºæŠ¥å‘Šç”Ÿæˆ
- AI å†³ç­–ä»ªè¡¨ç›˜ (æ–°å¢)
"""
import os
import sys
import json
from typing import Dict, List, Optional

# å°è¯•å¯¼å…¥ OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# å°è¯•å¯¼å…¥ Anthropic
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# å°è¯•å¯¼å…¥ Google Generative AI (Gemini)
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


def check_llm_available() -> Dict[str, bool]:
    """æ£€æŸ¥ LLM åº“æ˜¯å¦å¯ç”¨"""
    return {
        'openai': OPENAI_AVAILABLE,
        'anthropic': ANTHROPIC_AVAILABLE,
        'gemini': GEMINI_AVAILABLE
    }


def get_openai_client() -> Optional['OpenAI']:
    """è·å– OpenAI å®¢æˆ·ç«¯"""
    if not OPENAI_AVAILABLE:
        return None
    
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        return None
    
    return OpenAI(api_key=api_key)


def get_anthropic_client() -> Optional['anthropic.Anthropic']:
    """è·å– Anthropic å®¢æˆ·ç«¯"""
    if not ANTHROPIC_AVAILABLE:
        return None
    
    api_key = os.environ.get('ANTHROPIC_API_KEY')
    if not api_key:
        return None
    
    return anthropic.Anthropic(api_key=api_key)


def get_gemini_model():
    """è·å– Gemini æ¨¡å‹"""
    if not GEMINI_AVAILABLE:
        return None
    
    # ä¼˜å…ˆä» Streamlit secrets è¯»å– (Streamlit Cloud)
    api_key = None
    try:
        import streamlit as st
        if hasattr(st, 'secrets') and 'GEMINI_API_KEY' in st.secrets:
            api_key = st.secrets['GEMINI_API_KEY']
    except:
        pass
    
    # å›é€€åˆ°ç¯å¢ƒå˜é‡ (æœ¬åœ°å¼€å‘)
    if not api_key:
        api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')
    
    if not api_key:
        return None
    
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('gemini-2.5-flash')


class LLMAnalyzer:
    """LLM åˆ†æå™¨"""
    
    def __init__(self, provider: str = 'gemini'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            provider: 'openai', 'anthropic', æˆ– 'gemini'
        """
        self.provider = provider
        self.client = None
        
        if provider == 'openai':
            self.client = get_openai_client()
            self.model = 'gpt-4o-mini'
        elif provider == 'anthropic':
            self.client = get_anthropic_client()
            self.model = 'claude-3-haiku-20240307'
        elif provider == 'gemini':
            self.client = get_gemini_model()
            self.model = 'gemini-2.5-flash'
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    def _call_llm(self, prompt: str, system_prompt: str = "") -> str:
        """ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£"""
        if not self.is_available():
            return ""
        
        try:
            if self.provider == 'openai':
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages
                )
                return response.choices[0].message.content
            
            elif self.provider == 'anthropic':
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=1500,
                    system=system_prompt if system_prompt else "",
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text
            
            elif self.provider == 'gemini':
                full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
                response = self.client.generate_content(full_prompt)
                return response.text
        
        except Exception as e:
            return f"Error: {str(e)}"
        
        return ""
    
    def analyze_sentiment(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        if not self.is_available():
            return {'error': 'LLM client not available'}
        
        prompt = f"""åˆ†æä»¥ä¸‹è´¢ç»æ–‡æœ¬çš„å¸‚åœºæƒ…æ„Ÿã€‚

æ–‡æœ¬:
{text}

è¯·è¿”å›JSONæ ¼å¼:
{{
    "sentiment": "bullish" | "bearish" | "neutral",
    "confidence": 0.0-1.0,
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
    "reasoning": "åˆ†æåŸå› "
}}"""
        
        result = self._call_llm(prompt, "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚åªè¿”å›JSONã€‚")
        try:
            # å°è¯•æå– JSON
            if '{' in result:
                json_str = result[result.find('{'):result.rfind('}')+1]
                return json.loads(json_str)
        except:
            pass
        return {'error': 'Parse failed', 'raw': result}
    
    def natural_query(self, query: str, context: str = "") -> str:
        """è‡ªç„¶è¯­è¨€æŸ¥è¯¢"""
        if not self.is_available():
            return "LLM client not available"
        
        system_prompt = """ä½ æ˜¯ Coral Creek æ™ºèƒ½é‡åŒ–ç³»ç»Ÿçš„ AI åŠ©æ‰‹ã€‚
ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·:
1. è§£é‡ŠæŠ€æœ¯æŒ‡æ ‡ (BLUE ä¿¡å·, ADX, RSI ç­‰)
2. åˆ†æå¸‚åœºè¶‹åŠ¿
3. å›ç­”é‡åŒ–äº¤æ˜“ç›¸å…³é—®é¢˜

å½“å‰ç³»ç»Ÿæ”¯æŒ:
- BLUE ä¿¡å·: ç»¼åˆè¶…å–æŒ‡æ ‡ (>100 ä¸ºä¹°å…¥ä¿¡å·)
- é»‘é©¬/æ˜åº•: ç‰¹æ®Šåè½¬ä¿¡å·
- å‘¨çº¿/æœˆçº¿å…±æŒ¯: å¤šå‘¨æœŸç¡®è®¤

è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”ã€‚"""
        
        user_prompt = query
        if context:
            user_prompt = f"å½“å‰å¸‚åœºæ•°æ®:\n{context}\n\nç”¨æˆ·é—®é¢˜: {query}"
        
        return self._call_llm(user_prompt, system_prompt)
    
    def generate_market_report(self, signals: List[Dict]) -> str:
        """ç”Ÿæˆå¸‚åœºæŠ¥å‘Š"""
        if not self.is_available():
            return "LLM client not available"
        
        if not signals:
            signal_summary = "ä»Šæ—¥æ— è§¦å‘ä¿¡å·"
        else:
            signal_summary = f"ä»Šæ—¥å…±æœ‰ {len(signals)} ä¸ª BLUE ä¿¡å·:\n"
            for s in signals[:10]:
                blue_val = float(s.get('blue_daily', 0) or 0)
                price_val = float(s.get('price', 0) or 0)
                signal_summary += f"- {s.get('symbol', 'N/A')}: BLUE={blue_val:.1f}, ä»·æ ¼=${price_val:.2f}\n"
        
        prompt = f"""åŸºäºä»¥ä¸‹ä¿¡å·æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ¯æ—¥å¸‚åœºæŠ¥å‘Šã€‚

ä¿¡å·æ‘˜è¦:
{signal_summary}

è¯·ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Šï¼ŒåŒ…å«:
1. ğŸ“Š å¸‚åœºæ¦‚è§ˆ (2-3å¥è¯)
2. ğŸ”¥ çƒ­é—¨ä¿¡å· (å¦‚æœæœ‰)
3. âš ï¸ é£é™©æç¤º
4. ğŸ’¡ æ“ä½œå»ºè®®

ä¿æŒç®€æ´ä¸“ä¸šã€‚"""
        
        return self._call_llm(prompt, "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡åŒ–åˆ†æå¸ˆï¼Œè´Ÿè´£æ’°å†™æ¯æ—¥å¸‚åœºæŠ¥å‘Šã€‚")
    
    def generate_decision_dashboard(self, stock_data: Dict) -> Dict:
        """
        ç”Ÿæˆ AI å†³ç­–ä»ªè¡¨ç›˜ (ç±»ä¼¼ daily_stock_analysis)
        
        Args:
            stock_data: è‚¡ç¥¨æ•°æ®ï¼ŒåŒ…å«:
                - symbol: è‚¡ç¥¨ä»£ç 
                - price: å½“å‰ä»·æ ¼
                - blue_daily: BLUEæ—¥çº¿å€¼
                - blue_weekly: BLUEå‘¨çº¿å€¼ (å¯é€‰)
                - ma5, ma10, ma20: å‡çº¿
                - rsi: RSIå€¼ (å¯é€‰)
                - volume_ratio: é‡æ¯” (å¯é€‰)
                - sector: è¡Œä¸š (å¯é€‰)
        
        Returns:
            å†³ç­–ä»ªè¡¨ç›˜ Dict
        """
        if not self.is_available():
            return {'error': 'LLM client not available'}
        
        symbol = stock_data.get('symbol', 'N/A')
        # Convert to float safely to avoid format errors
        def safe_float(val, default=0):
            try:
                return float(val) if val is not None else default
            except (ValueError, TypeError):
                return default
        
        price = safe_float(stock_data.get('price'), 0)
        blue = safe_float(stock_data.get('blue_daily'), 0)
        blue_w = safe_float(stock_data.get('blue_weekly'), 0)
        ma5 = safe_float(stock_data.get('ma5'), 0)
        ma10 = safe_float(stock_data.get('ma10'), 0)
        ma20 = safe_float(stock_data.get('ma20'), 0)
        rsi = safe_float(stock_data.get('rsi'), 50)
        vol_ratio = safe_float(stock_data.get('volume_ratio'), 1)
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡åŒ–äº¤æ˜“åˆ†æå¸ˆã€‚åŸºäºä»¥ä¸‹è‚¡ç¥¨æ•°æ®ï¼Œç”Ÿæˆå†³ç­–ä»ªè¡¨ç›˜ã€‚

è‚¡ç¥¨: {symbol}
å½“å‰ä»·æ ¼: ${price:.2f}
BLUEä¿¡å·(æ—¥): {blue:.1f} (>100ä¸ºè¶…å–ä¹°å…¥ä¿¡å·)
BLUEä¿¡å·(å‘¨): {blue_w:.1f}
MA5: ${ma5:.2f}
MA10: ${ma10:.2f}  
MA20: ${ma20:.2f}
RSI: {rsi:.1f}
é‡æ¯”: {vol_ratio:.2f}

è¯·ç”ŸæˆJSONæ ¼å¼çš„å†³ç­–ä»ªè¡¨ç›˜:
{{
    "verdict": "ä¸€å¥è¯æ ¸å¿ƒç»“è®º (å¦‚: å¼ºçƒˆä¹°å…¥/è§‚æœ›/å›é¿)",
    "signal": "BUY" | "HOLD" | "SELL",
    "confidence": 0-100,
    "entry_price": å»ºè®®ä¹°å…¥ä»·,
    "stop_loss": æ­¢æŸä»·,
    "target_price": ç›®æ ‡ä»·,
    "checklist": [
        {{"item": "BLUEä¿¡å·", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "è¯´æ˜"}},
        {{"item": "å‡çº¿æ’åˆ—", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "è¯´æ˜"}},
        {{"item": "é‡ä»·é…åˆ", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "è¯´æ˜"}},
        {{"item": "è¶‹åŠ¿åˆ¤æ–­", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "è¯´æ˜"}}
    ],
    "risk_warning": "é£é™©æç¤º"
}}

è§„åˆ™:
- BLUE > 100 ä¸ºè¶…å–ä¹°å…¥åŒº
- MA5 > MA10 > MA20 ä¸ºå¤šå¤´æ’åˆ— âœ…
- é‡æ¯” > 1.5 ä¸ºæ”¾é‡
- ä¸¥ç¦è¿½é«˜ï¼šä¹–ç¦»ç‡ > 5% æ ‡è®°å±é™©"""
        
        result = self._call_llm(prompt)
        
        # å°è¯•è§£æ LLM å“åº”
        try:
            if result and '{' in result and 'Error' not in result:
                json_str = result[result.find('{'):result.rfind('}')+1]
                parsed = json.loads(json_str)
                if 'verdict' in parsed:
                    return parsed
        except Exception as e:
            pass
        
        # LLM å¤±è´¥æ—¶ï¼Œä½¿ç”¨æœ¬åœ°ç®—æ³•åˆ†æ
        # è®¡ç®—ä¹–ç¦»ç‡
        bias = (price - ma5) / ma5 * 100 if ma5 > 0 else 0
        
        # åˆ¤æ–­ä¿¡å·
        if blue > 100 and ma5 > ma10 > ma20 and abs(bias) < 5:
            signal = 'BUY'
            verdict = 'âœ… å¤šå¤´æ’åˆ— + BLUEè¶…å–ä¿¡å·ï¼Œå¯ä½å¸'
            confidence = min(80, int(blue / 2))
        elif blue > 80 and vol_ratio > 1.2:
            signal = 'BUY'
            verdict = 'ğŸ“ˆ æœ‰ä¼ç¨³è¿¹è±¡ï¼Œå…³æ³¨çªç ´'
            confidence = 60
        elif bias > 5:
            signal = 'SELL'
            verdict = 'âš ï¸ ä¹–ç¦»ç‡è¿‡é«˜ï¼Œä¸¥ç¦è¿½é«˜'
            confidence = 70
        elif ma5 < ma10 < ma20:
            signal = 'SELL'
            verdict = 'ğŸ“‰ ç©ºå¤´æ’åˆ—ï¼Œå»ºè®®è§‚æœ›'
            confidence = 65
        else:
            signal = 'HOLD'
            verdict = 'ğŸ”„ è¶‹åŠ¿ä¸æ˜ï¼Œå»ºè®®è§‚æœ›ç­‰å¾…'
            confidence = 40
        
        # æ„å»ºæ£€æŸ¥æ¸…å•
        checklist = [
            {
                'item': 'BLUEä¿¡å·',
                'status': 'âœ…' if blue > 100 else ('âš ï¸' if blue > 50 else 'âŒ'),
                'detail': f'BLUE={blue:.0f}' + (' (è¶…å–åŒº)' if blue > 100 else '')
            },
            {
                'item': 'å‡çº¿æ’åˆ—',
                'status': 'âœ…' if ma5 > ma10 > ma20 else 'âŒ',
                'detail': 'å¤šå¤´æ’åˆ—' if ma5 > ma10 > ma20 else 'ç©ºå¤´/ç¼ ç»•'
            },
            {
                'item': 'ä¹–ç¦»ç‡',
                'status': 'âœ…' if abs(bias) < 2 else ('âš ï¸' if abs(bias) < 5 else 'âŒ'),
                'detail': f'{bias:+.1f}%' + (' âš ï¸è¿½é«˜é£é™©' if bias > 5 else '')
            },
            {
                'item': 'é‡ä»·é…åˆ',
                'status': 'âœ…' if vol_ratio > 1.2 else 'âš ï¸',
                'detail': f'é‡æ¯”={vol_ratio:.1f}x'
            }
        ]
        
        return {
            'verdict': verdict,
            'signal': signal,
            'confidence': confidence,
            'entry_price': round(ma5, 2),  # å»ºè®®åœ¨MA5é™„è¿‘ä¹°å…¥
            'stop_loss': round(ma20 * 0.97, 2),  # æ­¢æŸåœ¨MA20ä¸‹æ–¹3%
            'target_price': round(price * 1.15, 2),  # ç›®æ ‡15%æ”¶ç›Š
            'checklist': checklist,
            'risk_warning': 'âš ï¸ æœ¬åœ°ç®—æ³•åˆ†æï¼Œå»ºè®®ç»“åˆAIå’Œäººå·¥åˆ¤æ–­',
            'analysis_mode': 'local'  # æ ‡è®°ä¸ºæœ¬åœ°åˆ†æ
        }


def quick_sentiment_check(text: str, provider: str = 'gemini') -> Dict:
    """å¿«é€Ÿæƒ…æ„Ÿåˆ†æ"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.analyze_sentiment(text)


def ask_ai(question: str, provider: str = 'gemini') -> str:
    """å¿«é€Ÿ AI é—®ç­”"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.natural_query(question)


def generate_stock_decision(stock_data: Dict, provider: str = 'gemini') -> Dict:
    """ç”Ÿæˆè‚¡ç¥¨å†³ç­–ä»ªè¡¨ç›˜"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.generate_decision_dashboard(stock_data)


if __name__ == "__main__":
    print("LLM Module Status:")
    status = check_llm_available()
    print(f"  OpenAI: {'âœ…' if status['openai'] else 'âŒ'}")
    print(f"  Anthropic: {'âœ…' if status['anthropic'] else 'âŒ'}")
    print(f"  Gemini: {'âœ…' if status['gemini'] else 'âŒ'}")
    
    # æµ‹è¯• Gemini
    if status['gemini'] and os.environ.get('GEMINI_API_KEY'):
        print("\nTesting Gemini Decision Dashboard...")
        test_data = {
            'symbol': 'NVDA',
            'price': 135.50,
            'blue_daily': 120,
            'blue_weekly': 85,
            'ma5': 134,
            'ma10': 132,
            'ma20': 128,
            'rsi': 35,
            'volume_ratio': 1.8
        }
        result = generate_stock_decision(test_data)
        print(f"Result: {json.dumps(result, indent=2, ensure_ascii=False)}")
