#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM Intelligence Module - å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½åˆ†æ

åŠŸèƒ½:
- æ–°é—»æƒ…æ„Ÿåˆ†æ
- è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- å¸‚åœºæŠ¥å‘Šç”Ÿæˆ
"""
import os
import sys
from typing import Dict, List, Optional

# å°è¯•å¯¼å…¥ OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# å°è¯•å¯¼å…¥ Anthropic
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False


def check_llm_available() -> Dict[str, bool]:
    """æ£€æŸ¥ LLM åº“æ˜¯å¦å¯ç”¨"""
    return {
        'openai': OPENAI_AVAILABLE,
        'anthropic': ANTHROPIC_AVAILABLE
    }


def get_openai_client() -> Optional['OpenAI']:
    """è·å– OpenAI å®¢æˆ·ç«¯"""
    if not OPENAI_AVAILABLE:
        return None
    
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        return None
    
    return OpenAI(api_key=api_key)


def get_anthropic_client() -> Optional['anthropic.Anthropic']:
    """è·å– Anthropic å®¢æˆ·ç«¯"""
    if not ANTHROPIC_AVAILABLE:
        return None
    
    api_key = os.environ.get('ANTHROPIC_API_KEY')
    if not api_key:
        return None
    
    return anthropic.Anthropic(api_key=api_key)


class LLMAnalyzer:
    """LLM åˆ†æå™¨"""
    
    def __init__(self, provider: str = 'openai'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            provider: 'openai' æˆ– 'anthropic'
        """
        self.provider = provider
        self.client = None
        
        if provider == 'openai':
            self.client = get_openai_client()
            self.model = 'gpt-4o-mini'
        elif provider == 'anthropic':
            self.client = get_anthropic_client()
            self.model = 'claude-3-haiku-20240307'
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    def analyze_sentiment(self, text: str) -> Dict:
        """
        åˆ†ææ–‡æœ¬æƒ…æ„Ÿ
        
        Args:
            text: æ–°é—»æˆ–è¯„è®ºæ–‡æœ¬
        
        Returns:
            Dict with sentiment, score, reasoning
        """
        if not self.is_available():
            return {'error': 'LLM client not available'}
        
        prompt = f"""åˆ†æä»¥ä¸‹è´¢ç»æ–‡æœ¬çš„å¸‚åœºæƒ…æ„Ÿã€‚

æ–‡æœ¬:
{text}

è¯·è¿”å›JSONæ ¼å¼:
{{
    "sentiment": "bullish" | "bearish" | "neutral",
    "confidence": 0.0-1.0,
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
    "reasoning": "åˆ†æåŸå› "
}}
"""
        
        try:
            if self.provider == 'openai':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    response_format={"type": "json_object"}
                )
                import json
                return json.loads(response.choices[0].message.content)
            
            elif self.provider == 'anthropic':
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=500,
                    messages=[{"role": "user", "content": prompt}]
                )
                import json
                return json.loads(response.content[0].text)
        
        except Exception as e:
            return {'error': str(e)}
    
    def natural_query(self, query: str, context: str = "") -> str:
        """
        è‡ªç„¶è¯­è¨€æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·é—®é¢˜ (å¦‚ "æ‰¾å‡ºè¶…å–çš„ç§‘æŠ€è‚¡")
            context: å½“å‰å¸‚åœºä¸Šä¸‹æ–‡
        
        Returns:
            å›ç­”æ–‡æœ¬
        """
        if not self.is_available():
            return "LLM client not available"
        
        system_prompt = """ä½ æ˜¯ Coral Creek æ™ºèƒ½é‡åŒ–ç³»ç»Ÿçš„ AI åŠ©æ‰‹ã€‚
ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·:
1. è§£é‡ŠæŠ€æœ¯æŒ‡æ ‡ (BLUE ä¿¡å·, ADX, RSI ç­‰)
2. åˆ†æå¸‚åœºè¶‹åŠ¿
3. å›ç­”é‡åŒ–äº¤æ˜“ç›¸å…³é—®é¢˜

å½“å‰ç³»ç»Ÿæ”¯æŒ:
- BLUE ä¿¡å·: ç»¼åˆè¶…å–æŒ‡æ ‡ (>100 ä¸ºä¹°å…¥ä¿¡å·)
- é»‘é©¬/æ˜åº•: ç‰¹æ®Šåè½¬ä¿¡å·
- å‘¨çº¿/æœˆçº¿å…±æŒ¯: å¤šå‘¨æœŸç¡®è®¤

è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”ã€‚"""
        
        user_prompt = query
        if context:
            user_prompt = f"å½“å‰å¸‚åœºæ•°æ®:\n{context}\n\nç”¨æˆ·é—®é¢˜: {query}"
        
        try:
            if self.provider == 'openai':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                return response.choices[0].message.content
            
            elif self.provider == 'anthropic':
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=1000,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                return response.content[0].text
        
        except Exception as e:
            return f"Error: {str(e)}"
    
    def generate_market_report(self, signals: List[Dict]) -> str:
        """
        ç”Ÿæˆå¸‚åœºæŠ¥å‘Š
        
        Args:
            signals: å½“æ—¥ä¿¡å·åˆ—è¡¨
        
        Returns:
            Markdown æ ¼å¼çš„å¸‚åœºæŠ¥å‘Š
        """
        if not self.is_available():
            return "LLM client not available"
        
        # æ„å»ºä¿¡å·æ‘˜è¦
        if not signals:
            signal_summary = "ä»Šæ—¥æ— è§¦å‘ä¿¡å·"
        else:
            signal_summary = f"ä»Šæ—¥å…±æœ‰ {len(signals)} ä¸ª BLUE ä¿¡å·:\n"
            for s in signals[:10]:  # æœ€å¤šå±•ç¤º 10 ä¸ª
                signal_summary += f"- {s.get('symbol', 'N/A')}: BLUE={s.get('blue_daily', 0):.1f}, ä»·æ ¼=${s.get('price', 0):.2f}\n"
        
        prompt = f"""åŸºäºä»¥ä¸‹ä¿¡å·æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ¯æ—¥å¸‚åœºæŠ¥å‘Šã€‚

ä¿¡å·æ‘˜è¦:
{signal_summary}

è¯·ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Šï¼ŒåŒ…å«:
1. ğŸ“Š å¸‚åœºæ¦‚è§ˆ (2-3å¥è¯)
2. ğŸ”¥ çƒ­é—¨ä¿¡å· (å¦‚æœæœ‰)
3. âš ï¸ é£é™©æç¤º
4. ğŸ’¡ æ“ä½œå»ºè®®

ä¿æŒç®€æ´ä¸“ä¸šã€‚"""
        
        try:
            if self.provider == 'openai':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡åŒ–åˆ†æå¸ˆï¼Œè´Ÿè´£æ’°å†™æ¯æ—¥å¸‚åœºæŠ¥å‘Šã€‚"},
                        {"role": "user", "content": prompt}
                    ]
                )
                return response.choices[0].message.content
            
            elif self.provider == 'anthropic':
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=1500,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text
        
        except Exception as e:
            return f"Error generating report: {str(e)}"


def quick_sentiment_check(text: str, provider: str = 'openai') -> Dict:
    """å¿«é€Ÿæƒ…æ„Ÿåˆ†æ"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.analyze_sentiment(text)


def ask_ai(question: str, provider: str = 'openai') -> str:
    """å¿«é€Ÿ AI é—®ç­”"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.natural_query(question)


if __name__ == "__main__":
    print("LLM Module Status:")
    status = check_llm_available()
    print(f"  OpenAI: {'âœ…' if status['openai'] else 'âŒ'}")
    print(f"  Anthropic: {'âœ…' if status['anthropic'] else 'âŒ'}")
    
    # æµ‹è¯•
    if status['openai'] and os.environ.get('OPENAI_API_KEY'):
        print("\nTesting OpenAI...")
        result = ask_ai("ä»€ä¹ˆæ˜¯ BLUE æŒ‡æ ‡ï¼Ÿ")
        print(f"Response: {result[:200]}...")
