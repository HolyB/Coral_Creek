#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM Intelligence Module - å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½åˆ†æ

åŠŸèƒ½:
- æ–°é—»æƒ…æ„Ÿåˆ†æ
- è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- å¸‚åœºæŠ¥å‘Šç”Ÿæˆ
- AI å†³ç­–ä»ªè¡¨ç›˜ (æ–°å¢)
"""
import os
import sys
import json
from typing import Dict, List, Optional

# å°è¯•å¯¼å…¥ OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# å°è¯•å¯¼å…¥ Anthropic
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# å°è¯•å¯¼å…¥ Google Generative AI (Gemini)
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


def check_llm_available() -> Dict[str, bool]:
    """æ£€æŸ¥ LLM åº“æ˜¯å¦å¯ç”¨"""
    return {
        'openai': OPENAI_AVAILABLE,
        'anthropic': ANTHROPIC_AVAILABLE,
        'gemini': GEMINI_AVAILABLE
    }


def get_openai_client() -> Optional['OpenAI']:
    """è·å– OpenAI å®¢æˆ·ç«¯"""
    if not OPENAI_AVAILABLE:
        return None
    
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        return None
    
    return OpenAI(api_key=api_key)


def get_anthropic_client() -> Optional['anthropic.Anthropic']:
    """è·å– Anthropic å®¢æˆ·ç«¯"""
    if not ANTHROPIC_AVAILABLE:
        return None
    
    api_key = os.environ.get('ANTHROPIC_API_KEY')
    if not api_key:
        return None
    
    return anthropic.Anthropic(api_key=api_key)


def get_gemini_model():
    """è·å– Gemini æ¨¡å‹"""
    if not GEMINI_AVAILABLE:
        return None
    
    # ä¼˜å…ˆä» Streamlit secrets è¯»å– (Streamlit Cloud)
    api_key = None
    try:
        import streamlit as st
        if hasattr(st, 'secrets') and 'GEMINI_API_KEY' in st.secrets:
            api_key = st.secrets['GEMINI_API_KEY']
    except:
        pass
    
    # å›é€€åˆ°ç¯å¢ƒå˜é‡ (æœ¬åœ°å¼€å‘)
    if not api_key:
        api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')
    
    if not api_key:
        return None
    
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('gemini-2.5-flash')


class LLMAnalyzer:
    """LLM åˆ†æå™¨"""
    
    def __init__(self, provider: str = 'gemini'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            provider: 'openai', 'anthropic', æˆ– 'gemini'
        """
        self.provider = provider
        self.client = None
        
        if provider == 'openai':
            self.client = get_openai_client()
            self.model = 'gpt-4o-mini'
        elif provider == 'anthropic':
            self.client = get_anthropic_client()
            self.model = 'claude-3-haiku-20240307'
        elif provider == 'gemini':
            self.client = get_gemini_model()
            self.model = 'gemini-2.5-flash'
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    def _call_llm(self, prompt: str, system_prompt: str = "") -> str:
        """ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£"""
        if not self.is_available():
            return ""
        
        try:
            if self.provider == 'openai':
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages
                )
                return response.choices[0].message.content
            
            elif self.provider == 'anthropic':
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=1500,
                    system=system_prompt if system_prompt else "",
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text
            
            elif self.provider == 'gemini':
                full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
                response = self.client.generate_content(full_prompt)
                return response.text
        
        except Exception as e:
            return f"Error: {str(e)}"
        
        return ""
    
    def analyze_sentiment(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        if not self.is_available():
            return {'error': 'LLM client not available'}
        
        prompt = f"""åˆ†æä»¥ä¸‹è´¢ç»æ–‡æœ¬çš„å¸‚åœºæƒ…æ„Ÿã€‚

æ–‡æœ¬:
{text}

è¯·è¿”å›JSONæ ¼å¼:
{{
    "sentiment": "bullish" | "bearish" | "neutral",
    "confidence": 0.0-1.0,
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
    "reasoning": "åˆ†æåŸå› "
}}"""
        
        result = self._call_llm(prompt, "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚åªè¿”å›JSONã€‚")
        try:
            # å°è¯•æå– JSON
            if '{' in result:
                json_str = result[result.find('{'):result.rfind('}')+1]
                return json.loads(json_str)
        except:
            pass
        return {'error': 'Parse failed', 'raw': result}
    
    def natural_query(self, query: str, context: str = "") -> str:
        """è‡ªç„¶è¯­è¨€æŸ¥è¯¢"""
        if not self.is_available():
            return "LLM client not available"
        
        system_prompt = """ä½ æ˜¯ Coral Creek æ™ºèƒ½é‡åŒ–ç³»ç»Ÿçš„ AI åŠ©æ‰‹ã€‚
ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·:
1. è§£é‡ŠæŠ€æœ¯æŒ‡æ ‡ (BLUE ä¿¡å·, ADX, RSI ç­‰)
2. åˆ†æå¸‚åœºè¶‹åŠ¿
3. å›ç­”é‡åŒ–äº¤æ˜“ç›¸å…³é—®é¢˜

å½“å‰ç³»ç»Ÿæ”¯æŒ:
- BLUE ä¿¡å·: ç»¼åˆè¶…å–æŒ‡æ ‡ (>100 ä¸ºä¹°å…¥ä¿¡å·)
- é»‘é©¬/æ˜åº•: ç‰¹æ®Šåè½¬ä¿¡å·
- å‘¨çº¿/æœˆçº¿å…±æŒ¯: å¤šå‘¨æœŸç¡®è®¤

è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”ã€‚"""
        
        user_prompt = query
        if context:
            user_prompt = f"å½“å‰å¸‚åœºæ•°æ®:\n{context}\n\nç”¨æˆ·é—®é¢˜: {query}"
        
        return self._call_llm(user_prompt, system_prompt)
    
    def generate_market_report(self, signals: List[Dict]) -> str:
        """ç”Ÿæˆå¸‚åœºæŠ¥å‘Š"""
        if not self.is_available():
            return "LLM client not available"
        
        if not signals:
            signal_summary = "ä»Šæ—¥æ— è§¦å‘ä¿¡å·"
        else:
            signal_summary = f"ä»Šæ—¥å…±æœ‰ {len(signals)} ä¸ª BLUE ä¿¡å·:\n"
            for s in signals[:10]:
                blue_val = float(s.get('blue_daily', 0) or 0)
                price_val = float(s.get('price', 0) or 0)
                signal_summary += f"- {s.get('symbol', 'N/A')}: BLUE={blue_val:.1f}, ä»·æ ¼=${price_val:.2f}\n"
        
        prompt = f"""åŸºäºä»¥ä¸‹ä¿¡å·æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ¯æ—¥å¸‚åœºæŠ¥å‘Šã€‚

ä¿¡å·æ‘˜è¦:
{signal_summary}

è¯·ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Šï¼ŒåŒ…å«:
1. ğŸ“Š å¸‚åœºæ¦‚è§ˆ (2-3å¥è¯)
2. ğŸ”¥ çƒ­é—¨ä¿¡å· (å¦‚æœæœ‰)
3. âš ï¸ é£é™©æç¤º
4. ğŸ’¡ æ“ä½œå»ºè®®

ä¿æŒç®€æ´ä¸“ä¸šã€‚"""
        
        return self._call_llm(prompt, "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡åŒ–åˆ†æå¸ˆï¼Œè´Ÿè´£æ’°å†™æ¯æ—¥å¸‚åœºæŠ¥å‘Šã€‚")
    
    def generate_decision_dashboard(self, stock_data: Dict, news_context: str = "") -> Dict:
        """
        ç”Ÿæˆ AI å†³ç­–ä»ªè¡¨ç›˜ (ç±»ä¼¼ daily_stock_analysis)
        
        Args:
            stock_data: è‚¡ç¥¨æ•°æ®
            news_context: æ–°é—»ä¸Šä¸‹æ–‡ (æ–°å¢)
        
        Returns:
            å†³ç­–ä»ªè¡¨ç›˜ Dict
        """
        if not self.is_available():
            return {'error': 'LLM client not available'}
        
        symbol = stock_data.get('symbol', 'N/A')
        # Convert to float safely to avoid format errors
        def safe_float(val, default=0):
            try:
                return float(val) if val is not None else default
            except (ValueError, TypeError):
                return default
        
        price = safe_float(stock_data.get('price'), 0)
        blue = safe_float(stock_data.get('blue_daily'), 0)
        blue_w = safe_float(stock_data.get('blue_weekly'), 0)
        ma5 = safe_float(stock_data.get('ma5'), 0)
        ma10 = safe_float(stock_data.get('ma10'), 0)
        ma20 = safe_float(stock_data.get('ma20'), 0)
        rsi = safe_float(stock_data.get('rsi'), 50)
        vol_ratio = safe_float(stock_data.get('volume_ratio'), 1)
        
        # è®¡ç®—ä¹–ç¦»ç‡
        bias = ((price - ma5) / ma5 * 100) if ma5 > 0 else 0
        bias_status = "å®‰å…¨" if abs(bias) < 2 else ("è­¦æˆ’" if abs(bias) < 5 else "å±é™©")
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“æ³¨äºè¶‹åŠ¿äº¤æ˜“çš„ä¸“ä¸šé‡åŒ–åˆ†æå¸ˆï¼Œè´Ÿè´£ç”Ÿæˆã€å†³ç­–ä»ªè¡¨ç›˜ã€‘ã€‚

## æ ¸å¿ƒäº¤æ˜“ç†å¿µï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

### 1. ä¸¥ç¦è¿½é«˜
- ä¹–ç¦»ç‡ = (ç°ä»· - MA5) / MA5 Ã— 100%
- ä¹–ç¦»ç‡ < 2%ï¼šæœ€ä½³ä¹°ç‚¹ âœ…
- ä¹–ç¦»ç‡ 2-5%ï¼šå¯å°ä»“ä»‹å…¥ âš ï¸
- ä¹–ç¦»ç‡ > 5%ï¼šä¸¥ç¦è¿½é«˜ï¼ç›´æ¥åˆ¤å®šä¸º"è§‚æœ›" âŒ

### 2. è¶‹åŠ¿äº¤æ˜“
- å¤šå¤´æ’åˆ—ï¼šMA5 > MA10 > MA20 âœ…
- ç©ºå¤´æ’åˆ—åšå†³ä¸ç¢° âŒ

### 3. BLUE ä¿¡å·ç³»ç»Ÿ
- BLUE > 100ï¼šè¶…å–åå¼¹ä¿¡å· âœ…
- BLUE 50-100ï¼šè§‚æœ›åŒºåŸŸ âš ï¸
- BLUE < 50ï¼šå¼±åŠ¿ âŒ

========== è‚¡ç¥¨æ•°æ® ==========
è‚¡ç¥¨ä»£ç : {symbol}
å½“å‰ä»·æ ¼: ${price:.2f}
BLUEä¿¡å·(æ—¥): {blue:.1f}
BLUEä¿¡å·(å‘¨): {blue_w:.1f}
MA5: ${ma5:.2f}
MA10: ${ma10:.2f}
MA20: ${ma20:.2f}
ä¹–ç¦»ç‡(MA5): {bias:.1f}% ({bias_status})
RSI: {rsi:.1f}
é‡æ¯”: {vol_ratio:.2f}

========== è¿‘æœŸæƒ…æŠ¥ ==========
{news_context if news_context else "æš‚æ— æ–°é—»"}
==============================

è¯·ç”ŸæˆJSONæ ¼å¼çš„å†³ç­–ä»ªè¡¨ç›˜:
{{
    "verdict": "ä¸€å¥è¯æ ¸å¿ƒç»“è®ºï¼ˆ30å­—ä»¥å†…ï¼Œç›´æ¥å‘Šè¯‰ç”¨æˆ·è¯¥ä¹°è¯¥å–ï¼‰",
    "signal": "BUY" | "HOLD" | "SELL",
    "confidence": 0-100,
    "entry_price": å»ºè®®ä¹°å…¥ä»·ï¼ˆåœ¨MA5é™„è¿‘ï¼‰,
    "stop_loss": æ­¢æŸä»·ï¼ˆè·Œç ´MA20æˆ–X%ï¼‰,
    "target_price": ç›®æ ‡ä»·,
    "news_summary": "èˆ†æƒ…åˆ†æï¼šæ˜¯å¦æœ‰å‡æŒ/ä¸šç»©é›·/åˆ©å¥½ (1-2å¥è¯)",
    "checklist": [
        {{"item": "BLUEä¿¡å·", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "BLUE=XX (è¶…å–åŒº/è§‚æœ›åŒº/å¼±åŠ¿)"}},
        {{"item": "å‡çº¿æ’åˆ—", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "å¤šå¤´æ’åˆ—/ç©ºå¤´æ’åˆ—/ç¼ ç»•"}},
        {{"item": "ä¹–ç¦»ç‡", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "X.X% (å®‰å…¨/è­¦æˆ’/å±é™©)"}},
        {{"item": "é‡ä»·é…åˆ", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "é‡æ¯”=X.X (æ”¾é‡/ç¼©é‡/æ­£å¸¸)"}},
        {{"item": "è¶‹åŠ¿å¼ºåº¦", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "RSI=XX (è¶…ä¹°/ä¸­æ€§/è¶…å–)"}},
        {{"item": "èˆ†æƒ…é£æ§", "status": "âœ…" | "âš ï¸" | "âŒ", "detail": "åˆ©å¥½/æ— é£é™©/å‡æŒé£é™©/ä¸šç»©é›·"}}
    ],
    "position_advice": {{
        "no_position": "ç©ºä»“è€…å»ºè®®ï¼šå…·ä½“æ“ä½œ",
        "has_position": "æŒä»“è€…å»ºè®®ï¼šå…·ä½“æ“ä½œ"
    }},
    "risk_warning": "é£é™©æç¤º"
}}

## è¯„åˆ†æ ‡å‡†
- 80-100åˆ†ï¼ˆä¹°å…¥ï¼‰ï¼šå¤šå¤´æ’åˆ— + BLUE>100 + ä¹–ç¦»ç‡<2% + é‡èƒ½é…åˆ
- 60-79åˆ†ï¼ˆè§‚æœ›åå¤šï¼‰ï¼šå…è®¸ä¸€é¡¹ä¸æ»¡è¶³
- 40-59åˆ†ï¼ˆè§‚æœ›ï¼‰ï¼šä¹–ç¦»ç‡>5% æˆ– å‡çº¿ç¼ ç»•
- 0-39åˆ†ï¼ˆå–å‡ºï¼‰ï¼šç©ºå¤´æ’åˆ— æˆ– è·Œç ´MA20 æˆ– é‡å¤§åˆ©ç©º"""
        
        
        result = self._call_llm(prompt)
        
        # å°è¯•è§£æ LLM å“åº”
        try:
            if result and '{' in result and 'Error' not in result:
                json_str = result[result.find('{'):result.rfind('}')+1]
                parsed = json.loads(json_str)
                if 'verdict' in parsed:
                    return parsed
        except Exception as e:
            pass
        
        # LLM å¤±è´¥æ—¶ï¼Œä½¿ç”¨æœ¬åœ°ç®—æ³•åˆ†æ
        # è®¡ç®—ä¹–ç¦»ç‡
        bias = (price - ma5) / ma5 * 100 if ma5 > 0 else 0
        
        # åˆ¤æ–­ä¿¡å·
        if blue > 100 and ma5 > ma10 > ma20 and abs(bias) < 5:
            signal = 'BUY'
            verdict = 'âœ… å¤šå¤´æ’åˆ— + BLUEè¶…å–ä¿¡å·ï¼Œå¯ä½å¸'
            confidence = min(80, int(blue / 2))
        elif blue > 80 and vol_ratio > 1.2:
            signal = 'BUY'
            verdict = 'ğŸ“ˆ æœ‰ä¼ç¨³è¿¹è±¡ï¼Œå…³æ³¨çªç ´'
            confidence = 60
        elif bias > 5:
            signal = 'SELL'
            verdict = 'âš ï¸ ä¹–ç¦»ç‡è¿‡é«˜ï¼Œä¸¥ç¦è¿½é«˜'
            confidence = 70
        elif ma5 < ma10 < ma20:
            signal = 'SELL'
            verdict = 'ğŸ“‰ ç©ºå¤´æ’åˆ—ï¼Œå»ºè®®è§‚æœ›'
            confidence = 65
        else:
            signal = 'HOLD'
            verdict = 'ğŸ”„ è¶‹åŠ¿ä¸æ˜ï¼Œå»ºè®®è§‚æœ›ç­‰å¾…'
            confidence = 40
        
        # æ„å»ºæ£€æŸ¥æ¸…å• (5é¡¹)
        checklist = [
            {
                'item': 'BLUEä¿¡å·',
                'status': 'âœ…' if blue > 100 else ('âš ï¸' if blue > 50 else 'âŒ'),
                'detail': f'BLUE={blue:.0f}' + (' (è¶…å–åŒº)' if blue > 100 else (' (è§‚æœ›åŒº)' if blue > 50 else ' (å¼±åŠ¿)'))
            },
            {
                'item': 'å‡çº¿æ’åˆ—',
                'status': 'âœ…' if ma5 > ma10 > ma20 else ('âš ï¸' if ma5 > ma10 else 'âŒ'),
                'detail': 'å¤šå¤´æ’åˆ—' if ma5 > ma10 > ma20 else ('å¼±åŠ¿å¤šå¤´' if ma5 > ma10 else 'ç©ºå¤´/ç¼ ç»•')
            },
            {
                'item': 'ä¹–ç¦»ç‡',
                'status': 'âœ…' if abs(bias) < 2 else ('âš ï¸' if abs(bias) < 5 else 'âŒ'),
                'detail': f'{bias:+.1f}% ' + ('å®‰å…¨' if abs(bias) < 2 else ('è­¦æˆ’' if abs(bias) < 5 else 'âŒä¸¥ç¦è¿½é«˜'))
            },
            {
                'item': 'é‡ä»·é…åˆ',
                'status': 'âœ…' if vol_ratio > 1.5 else ('âš ï¸' if vol_ratio > 0.8 else 'âŒ'),
                'detail': f'é‡æ¯”={vol_ratio:.1f}x ' + ('æ”¾é‡' if vol_ratio > 1.5 else ('æ­£å¸¸' if vol_ratio > 0.8 else 'ç¼©é‡'))
            },
            {
                'item': 'è¶‹åŠ¿å¼ºåº¦',
                'status': 'âœ…' if 30 < rsi < 70 else ('âš ï¸' if rsi > 70 else 'âŒ'),
                'detail': f'RSI={rsi:.0f} ' + ('ä¸­æ€§' if 30 < rsi < 70 else ('è¶…ä¹°' if rsi > 70 else 'è¶…å–'))
            }
        ]
        
        # ç”ŸæˆæŒä»“å»ºè®®
        if signal == 'BUY':
            position_advice = {
                'no_position': f'ç©ºä»“è€…ï¼šå¯åœ¨${ma5:.2f}é™„è¿‘ä½å¸å»ºä»“',
                'has_position': 'æŒä»“è€…ï¼šç»§ç»­æŒæœ‰ï¼Œé€‚å½“åŠ ä»“'
            }
        elif signal == 'SELL':
            position_advice = {
                'no_position': 'ç©ºä»“è€…ï¼šæš‚æ—¶è§‚æœ›ï¼Œä¸è¦è¿½å…¥',
                'has_position': 'æŒä»“è€…ï¼šè€ƒè™‘å‡ä»“æˆ–è®¾æ­¢æŸ'
            }
        else:
            position_advice = {
                'no_position': 'ç©ºä»“è€…ï¼šç­‰å¾…æ›´å¥½çš„ä¹°ç‚¹',
                'has_position': 'æŒä»“è€…ï¼šæŒè‚¡è§‚æœ›'
            }
        
        return {
            'verdict': verdict,
            'signal': signal,
            'confidence': confidence,
            'entry_price': round(ma5, 2),  # å»ºè®®åœ¨MA5é™„è¿‘ä¹°å…¥
            'stop_loss': round(ma20 * 0.97, 2),  # æ­¢æŸåœ¨MA20ä¸‹æ–¹3%
            'target_price': round(price * 1.15, 2),  # ç›®æ ‡15%æ”¶ç›Š
            'checklist': checklist,
            'position_advice': position_advice,
            'risk_warning': 'âš ï¸ æœ¬åœ°ç®—æ³•åˆ†æï¼Œå»ºè®®ç»“åˆAIå’Œäººå·¥åˆ¤æ–­',
            'analysis_mode': 'local'  # æ ‡è®°ä¸ºæœ¬åœ°åˆ†æ
        }


def quick_sentiment_check(text: str, provider: str = 'gemini') -> Dict:
    """å¿«é€Ÿæƒ…æ„Ÿåˆ†æ"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.analyze_sentiment(text)


def ask_ai(question: str, provider: str = 'gemini') -> str:
    """å¿«é€Ÿ AI é—®ç­”"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.natural_query(question)


def generate_stock_decision(stock_data: Dict, provider: str = 'gemini') -> Dict:
    """ç”Ÿæˆè‚¡ç¥¨å†³ç­–ä»ªè¡¨ç›˜"""
    analyzer = LLMAnalyzer(provider)
    return analyzer.generate_decision_dashboard(stock_data)


if __name__ == "__main__":
    print("LLM Module Status:")
    status = check_llm_available()
    print(f"  OpenAI: {'âœ…' if status['openai'] else 'âŒ'}")
    print(f"  Anthropic: {'âœ…' if status['anthropic'] else 'âŒ'}")
    print(f"  Gemini: {'âœ…' if status['gemini'] else 'âŒ'}")
    
    # æµ‹è¯• Gemini
    if status['gemini'] and os.environ.get('GEMINI_API_KEY'):
        print("\nTesting Gemini Decision Dashboard...")
        test_data = {
            'symbol': 'NVDA',
            'price': 135.50,
            'blue_daily': 120,
            'blue_weekly': 85,
            'ma5': 134,
            'ma10': 132,
            'ma20': 128,
            'rsi': 35,
            'volume_ratio': 1.8
        }
        result = generate_stock_decision(test_data)
        print(f"Result: {json.dumps(result, indent=2, ensure_ascii=False)}")
