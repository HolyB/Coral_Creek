"""
Kronos é¢„è®¡ç®—è„šæœ¬
==================
ç‹¬ç«‹äº Streamlit è¿è¡Œï¼Œæå‰ä¸ºæŒ‡å®šè‚¡ç¥¨æ‰¹é‡è®¡ç®— Kronos é¢„æµ‹ç»“æœï¼Œ
å°†ç»“æœç¼“å­˜åˆ° SQLite æ•°æ®åº“ï¼Œç½‘é¡µç«¯ç›´æ¥è¯»å–å³å¯ç§’å¼€ã€‚

ç”¨æ³•:
    # é¢„æµ‹æŒ‡å®šè‚¡ç¥¨
    python scripts/kronos_precompute.py HIMS AAPL NVDA TSLA

    # é¢„æµ‹ä»Šæ—¥æ‰«æå‡ºçš„æ‰€æœ‰ BLUE ä¿¡å·è‚¡ç¥¨
    python scripts/kronos_precompute.py --from-signals

    # é¢„æµ‹å…¨éƒ¨ (ä» signals ä¸­ + è‡ªé€‰åˆ—è¡¨)
    python scripts/kronos_precompute.py --from-signals HIMS AAPL
"""
import os
import sys
import json
import sqlite3
import argparse
import time
from datetime import datetime

# ç¡®ä¿é¡¹ç›®è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
V3_DIR = os.path.dirname(SCRIPT_DIR)
if V3_DIR not in sys.path:
    sys.path.insert(0, V3_DIR)

from ml.kronos_integration import get_kronos_engine
from ml.data_cache import DataCache

# é¢„æµ‹ç»“æœç¼“å­˜æ•°æ®åº“
CACHE_DB = os.path.join(V3_DIR, "db", "kronos_cache.db")


def init_cache_db():
    """åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“è¡¨"""
    os.makedirs(os.path.dirname(CACHE_DB), exist_ok=True)
    conn = sqlite3.connect(CACHE_DB)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS kronos_predictions (
            symbol TEXT NOT NULL,
            market TEXT NOT NULL DEFAULT 'US',
            pred_date TEXT NOT NULL,
            pred_len INTEGER NOT NULL DEFAULT 20,
            temperature REAL NOT NULL DEFAULT 0.5,
            prediction_json TEXT NOT NULL,
            last_hist_date TEXT,
            last_hist_close REAL,
            created_at TEXT NOT NULL,
            PRIMARY KEY (symbol, market, pred_date, pred_len)
        )
    """)
    conn.commit()
    conn.close()


def save_prediction(symbol: str, market: str, pred_df, last_hist_date: str, last_hist_close: float,
                    pred_len: int = 20, temperature: float = 0.5):
    """ä¿å­˜é¢„æµ‹ç»“æœåˆ°ç¼“å­˜æ•°æ®åº“"""
    conn = sqlite3.connect(CACHE_DB)
    today = datetime.now().strftime("%Y-%m-%d")
    
    # å°† DataFrame è½¬æˆ JSON
    pred_records = []
    for idx, row in pred_df.iterrows():
        pred_records.append({
            "date": str(idx)[:10],
            "Open": round(float(row["Open"]), 4),
            "High": round(float(row["High"]), 4),
            "Low": round(float(row["Low"]), 4),
            "Close": round(float(row["Close"]), 4),
            "Volume": round(float(row["Volume"]), 2),
        })
    
    conn.execute("""
        INSERT OR REPLACE INTO kronos_predictions 
        (symbol, market, pred_date, pred_len, temperature, prediction_json, last_hist_date, last_hist_close, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (symbol, market, today, pred_len, temperature, json.dumps(pred_records),
          last_hist_date, last_hist_close, datetime.now().isoformat()))
    conn.commit()
    conn.close()


def load_prediction(symbol: str, market: str = "US", pred_date: str = None):
    """ä»ç¼“å­˜æ•°æ®åº“è¯»å–é¢„æµ‹ç»“æœ"""
    import pandas as pd
    if not os.path.exists(CACHE_DB):
        return None
    if pred_date is None:
        pred_date = datetime.now().strftime("%Y-%m-%d")
    
    conn = sqlite3.connect(CACHE_DB)
    row = conn.execute("""
        SELECT prediction_json, last_hist_date, last_hist_close, temperature, pred_len, created_at
        FROM kronos_predictions 
        WHERE symbol=? AND market=? AND pred_date=?
        ORDER BY created_at DESC LIMIT 1
    """, (symbol, market, pred_date)).fetchone()
    conn.close()
    
    if row is None:
        return None
    
    pred_records = json.loads(row[0])
    pred_df = pd.DataFrame(pred_records)
    pred_df.index = pd.to_datetime(pred_df["date"])
    pred_df.drop(columns=["date"], inplace=True)
    
    return {
        "pred_df": pred_df,
        "last_hist_date": row[1],
        "last_hist_close": float(row[2]),
        "temperature": float(row[3]),
        "pred_len": int(row[4]),
        "created_at": row[5],
    }


def get_signal_symbols(market: str = "US") -> list:
    """ä»ä»Šæ—¥æ‰«æä¿¡å·ä¸­æå–å€™é€‰è‚¡ç¥¨ä»£ç """
    db_path = os.path.join(V3_DIR, "db", "coral_creek.db")
    if not os.path.exists(db_path):
        return []
    conn = sqlite3.connect(db_path)
    today = datetime.now().strftime("%Y-%m-%d")
    rows = conn.execute("""
        SELECT DISTINCT symbol FROM signals 
        WHERE scan_date = ? AND market = ?
        ORDER BY symbol
    """, (today, market)).fetchall()
    conn.close()
    return [r[0] for r in rows]


def predict_single(engine, cache, symbol: str, market: str = "US", pred_len: int = 20, temperature: float = 0.5):
    """å¯¹å•åªè‚¡ç¥¨è¿è¡Œ Kronos é¢„æµ‹å¹¶ç¼“å­˜"""
    import pandas as pd
    
    print(f"  ğŸ“Š è·å– {symbol} å†å²æ•°æ®...")
    df = cache.get_stock_history(symbol, market=market)
    
    if df is None or len(df) < 60:
        print(f"  âš ï¸ {symbol} æ•°æ®ä¸è¶³, è·³è¿‡ (len={len(df) if df is not None else 0})")
        return False
    
    # å‡†å¤‡è¾“å…¥
    df_input = df.copy()
    df_input.columns = [c.lower() for c in df_input.columns]
    if "date" in df_input.columns:
        df_input.rename(columns={"date": "timestamps"}, inplace=True)
    df_input = df_input.tail(400)
    
    print(f"  ğŸ§  Kronos æ¨ç†ä¸­ ({len(df_input)} æ ¹Kçº¿ â†’ {pred_len}å¤©é¢„æµ‹)...")
    t0 = time.time()
    pred_df = engine.predict_future_klines(df_input, pred_len=pred_len, temperature=temperature, top_p=0.8)
    elapsed = time.time() - t0
    
    if pred_df is None:
        print(f"  âŒ {symbol} é¢„æµ‹å¤±è´¥")
        return False
    
    # ä¿å­˜
    last_close = float(df_input["close"].iloc[-1])
    last_date = str(df_input["timestamps"].iloc[-1])[:10] if "timestamps" in df_input.columns else "unknown"
    save_prediction(symbol, market, pred_df, last_date, last_close, pred_len, temperature)
    
    pred_chg = (float(pred_df["Close"].iloc[-1]) / last_close - 1) * 100
    direction = "ğŸ“ˆ" if pred_chg > 0 else "ğŸ“‰"
    print(f"  âœ… {symbol} å®Œæˆ ({elapsed:.1f}s) {direction} é¢„æµ‹{pred_len}æ—¥å˜å¹…: {pred_chg:+.2f}%")
    return True


def main():
    parser = argparse.ArgumentParser(description="Kronos æ‰¹é‡é¢„è®¡ç®—")
    parser.add_argument("symbols", nargs="*", help="è¦é¢„æµ‹çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨")
    parser.add_argument("--from-signals", action="store_true", help="ä»ä»Šæ—¥æ‰«æä¿¡å·ä¸­è‡ªåŠ¨æå–")
    parser.add_argument("--market", default="US", help="å¸‚åœº (US/CN)")
    parser.add_argument("--pred-len", type=int, default=20, help="é¢„æµ‹å¤©æ•°")
    parser.add_argument("--temperature", type=float, default=0.5, help="éšæœºåº¦")
    args = parser.parse_args()
    
    symbols = list(args.symbols)
    
    if args.from_signals:
        sig_symbols = get_signal_symbols(args.market)
        print(f"ğŸ“¡ ä»ä»Šæ—¥æ‰«æä¿¡å·ä¸­å‘ç° {len(sig_symbols)} åªè‚¡ç¥¨")
        symbols = list(set(symbols + sig_symbols))
    
    if not symbols:
        symbols = ["HIMS", "AAPL", "NVDA", "TSLA", "PLTR"]  # é»˜è®¤çƒ­é—¨
        print(f"â„¹ï¸ æœªæŒ‡å®šè‚¡ç¥¨, ä½¿ç”¨é»˜è®¤åˆ—è¡¨: {symbols}")
    
    print(f"\nğŸª Kronos æ‰¹é‡é¢„è®¡ç®—å¯åŠ¨")
    print(f"   è‚¡ç¥¨æ•°é‡: {len(symbols)}")
    print(f"   é¢„æµ‹å¤©æ•°: {args.pred_len}")
    print(f"   æ¸©åº¦å‚æ•°: {args.temperature}")
    print(f"   ç¼“å­˜è·¯å¾„: {CACHE_DB}")
    print()
    
    # åˆå§‹åŒ–
    init_cache_db()
    cache = DataCache()
    
    print("ğŸš€ åŠ è½½ Kronos å¤§æ¨¡å‹å¼•æ“...")
    t_start = time.time()
    engine = get_kronos_engine()
    print(f"âœ… å¼•æ“åŠ è½½å®Œæˆ ({time.time() - t_start:.1f}s)\n")
    
    success = 0
    fail = 0
    for i, sym in enumerate(symbols, 1):
        print(f"[{i}/{len(symbols)}] {sym}")
        try:
            if predict_single(engine, cache, sym, args.market, args.pred_len, args.temperature):
                success += 1
            else:
                fail += 1
        except Exception as e:
            print(f"  âŒ {sym} å¼‚å¸¸: {e}")
            fail += 1
    
    total_time = time.time() - t_start
    print(f"\n{'='*50}")
    print(f"ğŸ æ‰¹é‡é¢„è®¡ç®—å®Œæˆ!")
    print(f"   æˆåŠŸ: {success} | å¤±è´¥: {fail} | æ€»è€—æ—¶: {total_time:.1f}s")
    print(f"   ç»“æœå·²ä¿å­˜åˆ°: {CACHE_DB}")


if __name__ == "__main__":
    main()
