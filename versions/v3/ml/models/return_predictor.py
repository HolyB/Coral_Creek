"""
æ”¶ç›Šé¢„æµ‹æ¨¡å‹
Return Predictor

é¢„æµ‹ 1/5/10/30/60 å¤©çš„æ”¶ç›Šç‡
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import json

try:
    import xgboost as xgb
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False


class ReturnPredictor:
    """å¤šå‘¨æœŸæ”¶ç›Šé¢„æµ‹å™¨"""
    
    # ç»Ÿä¸€ç›®æ ‡: ä»¥ä¸­é•¿çº¿ä¸ºä¸»ï¼ˆ5/20/60ï¼‰
    HORIZONS = {
        '5d': 5,    # çŸ­çº¿è¾…åŠ©
        '20d': 20,  # ä¸­çº¿æ ¸å¿ƒ
        '60d': 60,  # é•¿çº¿æ ¸å¿ƒ
    }
    
    # é»˜è®¤å‚æ•°
    DEFAULT_PARAMS = {
        'n_estimators': 200,
        'max_depth': 5,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42
    }
    
    def __init__(self, custom_params: Dict[str, Dict] = None):
        """
        Args:
            custom_params: è‡ªå®šä¹‰å‚æ•° {'1d': {...}, '5d': {...}}
        """
        self.models: Dict[str, xgb.XGBRegressor] = {}
        self.feature_names: List[str] = []
        self.metrics: Dict[str, Dict] = {}
        self.is_trained = False
        self.custom_params = custom_params or {}
    
    def _get_params(self, horizon: str) -> Dict:
        """è·å–æŒ‡å®šå‘¨æœŸçš„æ¨¡å‹å‚æ•°"""
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰å‚æ•°
        if horizon in self.custom_params:
            params = self.DEFAULT_PARAMS.copy()
            params.update(self.custom_params[horizon])
            return params
        
        # å°è¯•ä»è°ƒä¼˜ç»“æœåŠ è½½
        tuning_path = Path(__file__).parent.parent / 'tuning_results'
        for market in ['us', 'cn']:
            params_file = tuning_path / market / 'best_params.json'
            if params_file.exists():
                try:
                    with open(params_file) as f:
                        best_params = json.load(f)
                    key = f'regressor_{horizon}'
                    if key in best_params:
                        params = self.DEFAULT_PARAMS.copy()
                        params.update(best_params[key])
                        return params
                except:
                    pass
        
        return self.DEFAULT_PARAMS.copy()
    
    def train(self, 
              X: np.ndarray, 
              y_dict: Dict[str, np.ndarray],
              feature_names: List[str],
              groups: Optional[np.ndarray] = None,
              test_size: float = 0.2) -> Dict:
        """
        è®­ç»ƒæ‰€æœ‰å‘¨æœŸçš„æ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ (n_samples, n_features)
            y_dict: æ ‡ç­¾å­—å…¸ {'1d': array, '5d': array, ...}
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
        
        Returns:
            è®­ç»ƒæŒ‡æ ‡
        """
        if not ML_AVAILABLE:
            raise RuntimeError("XGBoost æœªå®‰è£…")
        
        self.feature_names = feature_names
        
        print(f"\n{'='*50}")
        print("ğŸ¯ æ”¶ç›Šé¢„æµ‹æ¨¡å‹è®­ç»ƒ")
        print(f"   æ ·æœ¬æ•°: {len(X)}")
        print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"{'='*50}\n")
        
        for horizon_name, horizon_days in self.HORIZONS.items():
            if horizon_name not in y_dict:
                print(f"âš ï¸ è·³è¿‡ {horizon_name}: æ— æ ‡ç­¾")
                continue
            
            y = y_dict[horizon_name]
            
            # è¿‡æ»¤æ— æ•ˆæ ·æœ¬
            valid_mask = ~np.isnan(y)
            X_valid = X[valid_mask]
            y_valid = y[valid_mask]
            
            if len(X_valid) < 100:
                print(f"âš ï¸ è·³è¿‡ {horizon_name}: æ ·æœ¬ä¸è¶³ ({len(X_valid)})")
                continue
            
            # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›† (ä¼˜å…ˆä½¿ç”¨æ—¶åºåˆ‡åˆ†ï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„æ¼)
            if groups is not None:
                groups_valid = groups[valid_mask]
                unique_groups = np.unique(groups_valid)
                unique_groups = np.sort(unique_groups)

                if len(unique_groups) >= 10:
                    split_idx = max(1, int(len(unique_groups) * (1 - test_size)))
                    split_idx = min(split_idx, len(unique_groups) - 1)
                    train_groups = unique_groups[:split_idx]
                    test_groups = unique_groups[split_idx:]

                    train_mask = np.isin(groups_valid, train_groups)
                    test_mask = np.isin(groups_valid, test_groups)

                    X_train, X_test = X_valid[train_mask], X_valid[test_mask]
                    y_train, y_test = y_valid[train_mask], y_valid[test_mask]
                else:
                    X_train, X_test, y_train, y_test = train_test_split(
                        X_valid, y_valid, test_size=test_size, random_state=42
                    )
            else:
                X_train, X_test, y_train, y_test = train_test_split(
                    X_valid, y_valid, test_size=test_size, random_state=42
                )
            
            # è·å–å‚æ•° (ä¼˜å…ˆç”¨è°ƒä¼˜åçš„)
            params = self._get_params(horizon_name)
            
            print(f"ğŸ“ˆ è®­ç»ƒ {horizon_name} æ¨¡å‹...")
            print(f"   è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")
            print(f"   å‚æ•°: max_depth={params.get('max_depth')}, lr={params.get('learning_rate')}")
            
            # è®­ç»ƒæ¨¡å‹
            model = xgb.XGBRegressor(**params)
            
            model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                verbose=False
            )
            
            # è¯„ä¼°
            y_pred = model.predict(X_test)
            
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            # æ–¹å‘å‡†ç¡®ç‡ (é¢„æµ‹æ¶¨è·Œå¯¹ä¸å¯¹)
            direction_acc = ((y_pred > 0) == (y_test > 0)).mean()
            
            self.models[horizon_name] = model
            self.metrics[horizon_name] = {
                'mse': mse,
                'rmse': np.sqrt(mse),
                'mae': mae,
                'r2': r2,
                'direction_accuracy': direction_acc,
                'train_samples': len(X_train),
                'test_samples': len(X_test)
            }
            
            print(f"   RMSE: {np.sqrt(mse):.2f}%, MAE: {mae:.2f}%, RÂ²: {r2:.3f}")
            print(f"   æ–¹å‘å‡†ç¡®ç‡: {direction_acc:.1%}")
            print()
        
        self.is_trained = len(self.models) > 0
        
        return self.metrics
    
    def predict(self, X: np.ndarray) -> Dict[str, np.ndarray]:
        """
        é¢„æµ‹æ‰€æœ‰å‘¨æœŸçš„æ”¶ç›Šç‡
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
        
        Returns:
            {'1d': array, '5d': array, ...}
        """
        if not self.is_trained:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒ")
        
        predictions = {}
        for horizon_name, model in self.models.items():
            predictions[horizon_name] = model.predict(X)
        
        return predictions
    
    def predict_single(self, features: Dict) -> Dict[str, float]:
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        if not self.is_trained:
            return {}
        
        X = np.array([[features.get(f, 0) for f in self.feature_names]])
        X = np.nan_to_num(X, nan=0.0)
        
        predictions = self.predict(X)
        return {k: float(v[0]) for k, v in predictions.items()}
    
    def get_feature_importance(self, horizon: str = '5d') -> Dict[str, float]:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if horizon not in self.models:
            return {}
        
        model = self.models[horizon]
        importance = dict(zip(self.feature_names, model.feature_importances_))
        return dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        import joblib
        
        save_dir = Path(path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¯ä¸ªå‘¨æœŸçš„æ¨¡å‹
        for horizon_name, model in self.models.items():
            joblib.dump(model, save_dir / f"return_{horizon_name}.joblib")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'feature_names': self.feature_names,
            'metrics': self.metrics,
            'horizons': list(self.models.keys())
        }
        with open(save_dir / "return_predictor_meta.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"âœ… æ”¶ç›Šé¢„æµ‹æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        import joblib
        
        save_dir = Path(path)
        meta_path = save_dir / "return_predictor_meta.json"
        
        if not meta_path.exists():
            return False
        
        with open(meta_path) as f:
            metadata = json.load(f)
        
        self.feature_names = metadata['feature_names']
        self.metrics = metadata['metrics']
        
        for horizon_name in metadata['horizons']:
            model_path = save_dir / f"return_{horizon_name}.joblib"
            if model_path.exists():
                self.models[horizon_name] = joblib.load(model_path)
        
        self.is_trained = len(self.models) > 0
        print(f"âœ… æ”¶ç›Šé¢„æµ‹æ¨¡å‹å·²åŠ è½½: {list(self.models.keys())}")
        return self.is_trained


# === æµ‹è¯• ===
if __name__ == "__main__":
    print("=== Return Predictor æµ‹è¯• ===\n")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 50
    
    X = np.random.randn(n_samples, n_features)
    feature_names = [f'f_{i}' for i in range(n_features)]
    
    # æ¨¡æ‹Ÿæ ‡ç­¾ (å¸¦å™ªå£°çš„çº¿æ€§ç»„åˆ)
    y_dict = {
        '1d': X[:, 0] * 0.5 + X[:, 1] * 0.3 + np.random.randn(n_samples) * 2,
        '5d': X[:, 0] * 1.0 + X[:, 2] * 0.5 + np.random.randn(n_samples) * 3,
        '10d': X[:, 0] * 1.5 + X[:, 3] * 0.8 + np.random.randn(n_samples) * 4,
    }
    
    predictor = ReturnPredictor()
    metrics = predictor.train(X, y_dict, feature_names)
    
    print("\n=== ç‰¹å¾é‡è¦æ€§ (5d) ===")
    importance = predictor.get_feature_importance('5d')
    for feat, imp in list(importance.items())[:5]:
        print(f"  {feat}: {imp:.3f}")
