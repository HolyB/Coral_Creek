"""
ä¿¡å·æ’åºæ¨¡å‹
Signal Ranker

ä½¿ç”¨ Learning to Rank å¯¹ä¿¡å·è¿›è¡Œæ’åº
æ‰¾å‡ºæœ€å¯èƒ½èµšé’±çš„è‚¡ç¥¨
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import json
from dataclasses import dataclass
from enum import Enum

try:
    import xgboost as xgb
    from sklearn.model_selection import GroupKFold
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False


class TradingHorizon(Enum):
    """äº¤æ˜“å‘¨æœŸ"""
    SHORT = "short"   # çŸ­çº¿: 1-5å¤©
    MEDIUM = "medium" # ä¸­çº¿: 10-30å¤©
    LONG = "long"     # é•¿çº¿: 60å¤©+


@dataclass
class HorizonConfig:
    """å‘¨æœŸé…ç½®"""
    name: str
    days: List[int]           # è®¡ç®—æ”¶ç›Šçš„å¤©æ•°
    target_return: float      # ç›®æ ‡æ”¶ç›Šç‡ (%)
    stop_loss: float          # æ­¢æŸçº¿ (%)
    weight_return: float      # æ”¶ç›Šæƒé‡
    weight_risk: float        # é£é™©æƒé‡


# ä¸“ä¸šäº¤æ˜“è€…çš„å‘¨æœŸé…ç½®
HORIZON_CONFIGS = {
    TradingHorizon.SHORT: HorizonConfig(
        name="çŸ­çº¿ (1-5å¤©)",
        days=[1, 3, 5],
        target_return=3.0,    # 3% ç›®æ ‡
        stop_loss=-3.0,       # 3% æ­¢æŸ
        weight_return=0.7,    # æ›´çœ‹é‡å¿«é€Ÿæ”¶ç›Š
        weight_risk=0.3
    ),
    TradingHorizon.MEDIUM: HorizonConfig(
        name="ä¸­çº¿ (10-30å¤©)",
        days=[10, 20, 30],
        target_return=10.0,   # 10% ç›®æ ‡
        stop_loss=-5.0,       # 5% æ­¢æŸ
        weight_return=0.5,
        weight_risk=0.5
    ),
    TradingHorizon.LONG: HorizonConfig(
        name="é•¿çº¿ (60å¤©+)",
        days=[30, 60],
        target_return=25.0,   # 25% ç›®æ ‡
        stop_loss=-8.0,       # 8% æ­¢æŸ
        weight_return=0.4,
        weight_risk=0.6       # æ›´çœ‹é‡é£é™©æ§åˆ¶
    )
}


class SignalRanker:
    """ä¿¡å·æ’åºå™¨ (Learning to Rank)"""
    
    def __init__(self):
        self.models: Dict[TradingHorizon, xgb.XGBRanker] = {}
        self.feature_names: List[str] = []
        self.metrics: Dict[TradingHorizon, Dict] = {}
        self.is_trained = False
    
    def _create_ranking_labels(self, 
                               returns: np.ndarray, 
                               max_drawdowns: np.ndarray,
                               config: HorizonConfig) -> np.ndarray:
        """
        åˆ›å»ºæ’åºæ ‡ç­¾
        
        ç»¼åˆè€ƒè™‘æ”¶ç›Šå’Œé£é™©:
        score = w_return * æ”¶ç›Šåˆ†æ•° + w_risk * é£é™©åˆ†æ•°
        
        æ”¶ç›Šåˆ†æ•°: åŸºäºæ”¶ç›Šç‡çš„ç™¾åˆ†ä½
        é£é™©åˆ†æ•°: åŸºäºæœ€å¤§å›æ’¤çš„ç™¾åˆ†ä½ (è¶Šå°è¶Šå¥½)
        """
        n = len(returns)
        
        # å¤„ç† NaN
        returns = np.nan_to_num(returns, nan=0.0)
        max_drawdowns = np.nan_to_num(max_drawdowns, nan=0.0)
        
        # æ”¶ç›Šåˆ†æ•° (0-100, è¶Šé«˜è¶Šå¥½)
        return_rank = pd.Series(returns).rank(pct=True) * 100
        
        # é£é™©åˆ†æ•° (0-100, å›æ’¤è¶Šå°åˆ†æ•°è¶Šé«˜)
        risk_rank = (1 - pd.Series(max_drawdowns).rank(pct=True)) * 100
        
        # ç»¼åˆåˆ†æ•°
        score = config.weight_return * return_rank + config.weight_risk * risk_rank
        
        # å¤„ç† NaN
        score = score.fillna(score.median())
        
        # è½¬ä¸ºæ•´æ•°æ ‡ç­¾ (0-4 äº”æ¡£)
        try:
            labels = pd.qcut(score, q=5, labels=[0, 1, 2, 3, 4], duplicates='drop')
            return labels.values.astype(int)
        except:
            # å¦‚æœåˆ†ä½æ•°å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ†æ¡£
            labels = pd.cut(score, bins=5, labels=[0, 1, 2, 3, 4])
            return labels.fillna(2).values.astype(int)
    
    def train(self,
              X: np.ndarray,
              returns_dict: Dict[str, np.ndarray],
              drawdowns_dict: Dict[str, np.ndarray],
              groups: np.ndarray,
              feature_names: List[str]) -> Dict:
        """
        è®­ç»ƒæ’åºæ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            returns_dict: å„å‘¨æœŸæ”¶ç›Šç‡ {'1d': array, '5d': array, ...}
            drawdowns_dict: å„å‘¨æœŸæœ€å¤§å›æ’¤ {'5d': array, '30d': array, ...}
            groups: åˆ†ç»„ (åŒä¸€å¤©çš„ä¿¡å·ä¸ºä¸€ç»„)
            feature_names: ç‰¹å¾åç§°
        
        Returns:
            è®­ç»ƒæŒ‡æ ‡
        """
        if not ML_AVAILABLE:
            raise RuntimeError("XGBoost æœªå®‰è£…")
        
        self.feature_names = feature_names
        
        print(f"\n{'='*50}")
        print("ğŸ¯ ä¿¡å·æ’åºæ¨¡å‹è®­ç»ƒ (Learning to Rank)")
        print(f"   æ ·æœ¬æ•°: {len(X)}")
        print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"   åˆ†ç»„æ•°: {len(np.unique(groups))}")
        print(f"{'='*50}\n")
        
        for horizon, config in HORIZON_CONFIGS.items():
            print(f"ğŸ“Š è®­ç»ƒ {config.name} æ’åºæ¨¡å‹...")
            
            # è®¡ç®—è¯¥å‘¨æœŸçš„ç»¼åˆæ”¶ç›Š
            horizon_returns = []
            for day in config.days:
                key = f'{day}d'
                if key in returns_dict:
                    horizon_returns.append(returns_dict[key])
            
            if not horizon_returns:
                print(f"   âš ï¸ è·³è¿‡: æ— æ”¶ç›Šæ•°æ®")
                continue
            
            # å¹³å‡æ”¶ç›Š
            avg_returns = np.nanmean(horizon_returns, axis=0)
            
            # æœ€å¤§å›æ’¤ (å–æœ€é•¿å‘¨æœŸçš„)
            max_day = max(config.days)
            dd_key = f'{max_day}d'
            if dd_key in drawdowns_dict:
                max_dd = drawdowns_dict[dd_key]
            else:
                max_dd = np.zeros_like(avg_returns)
            
            # åˆ›å»ºæ’åºæ ‡ç­¾
            labels = self._create_ranking_labels(avg_returns, max_dd, config)
            
            # è¿‡æ»¤æ— æ•ˆæ ·æœ¬
            valid_mask = ~np.isnan(avg_returns)
            X_valid = X[valid_mask]
            y_valid = labels[valid_mask]
            groups_valid = groups[valid_mask]
            
            if len(X_valid) < 100:
                print(f"   âš ï¸ è·³è¿‡: æ ·æœ¬ä¸è¶³")
                continue

            # æ—¶åºåˆ‡åˆ† (æŒ‰æ—¥æœŸç»„)ï¼Œå…ˆåš OOS è¯„ä¼°å†è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            unique_groups = np.unique(groups_valid)
            unique_groups = np.sort(unique_groups)
            if len(unique_groups) < 10:
                print(f"   âš ï¸ è·³è¿‡: åˆ†ç»„ä¸è¶³ ({len(unique_groups)})")
                continue

            split_idx = max(1, int(len(unique_groups) * 0.8))
            split_idx = min(split_idx, len(unique_groups) - 1)
            train_groups = unique_groups[:split_idx]
            test_groups = unique_groups[split_idx:]

            train_mask = np.isin(groups_valid, train_groups)
            test_mask = np.isin(groups_valid, test_groups)

            X_train = X_valid[train_mask]
            y_train = y_valid[train_mask]
            groups_train = groups_valid[train_mask]

            X_test = X_valid[test_mask]
            y_test = y_valid[test_mask]
            groups_test = groups_valid[test_mask]
            returns_test = avg_returns[valid_mask][test_mask]

            if len(X_train) < 100 or len(X_test) < 30:
                print(f"   âš ï¸ è·³è¿‡: è®­ç»ƒ/æµ‹è¯•æ ·æœ¬ä¸è¶³")
                continue

            # è®­ç»ƒ XGBoost Ranker (ä»…è®­ç»ƒçª—å£)
            model = xgb.XGBRanker(
                objective='rank:pairwise',
                n_estimators=200,
                max_depth=5,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42
            )
            
            model.fit(
                X_train, y_train,
                group=self._get_group_sizes(groups_train),
                verbose=False
            )
            
            # OOS è¯„ä¼°: NDCG@10 + Top10 å¹³å‡æ”¶ç›Š
            scores_test = model.predict(X_test)
            ndcg = self._calculate_ndcg(y_test, scores_test, groups_test, k=10)
            
            # OOS è¯„ä¼°: Top 10 å¹³å‡æ”¶ç›Š
            top10_return = self._calculate_top_k_return(
                returns_test, scores_test, groups_test, k=10
            )

            # æœ€ç»ˆæ¨¡å‹: ç”¨å…¨éƒ¨æ ·æœ¬è®­ç»ƒï¼Œç”¨äºçº¿ä¸Šæ¨ç†
            final_model = xgb.XGBRanker(
                objective='rank:pairwise',
                n_estimators=200,
                max_depth=5,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42
            )
            final_model.fit(
                X_valid, y_valid,
                group=self._get_group_sizes(groups_valid),
                verbose=False
            )
            
            self.models[horizon] = final_model
            self.metrics[horizon] = {
                'ndcg@10': ndcg,
                'top10_avg_return': top10_return,
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'n_train_groups': len(np.unique(groups_train)),
                'n_test_groups': len(np.unique(groups_test))
            }
            
            print(f"   NDCG@10: {ndcg:.3f}")
            print(f"   Top10 å¹³å‡æ”¶ç›Š: {top10_return:.2f}%")
            print()
        
        self.is_trained = len(self.models) > 0
        return self.metrics
    
    def _get_group_sizes(self, groups: np.ndarray) -> List[int]:
        """è·å–æ¯ä¸ªç»„çš„å¤§å°"""
        unique_groups = np.unique(groups)
        return [np.sum(groups == g) for g in unique_groups]
    
    def _calculate_ndcg(self, y_true: np.ndarray, y_pred: np.ndarray, 
                        groups: np.ndarray, k: int = 10) -> float:
        """è®¡ç®— NDCG@k"""
        unique_groups = np.unique(groups)
        ndcgs = []
        
        for g in unique_groups:
            mask = groups == g
            if mask.sum() < k:
                continue
            
            true = y_true[mask]
            pred = y_pred[mask]
            
            # æŒ‰é¢„æµ‹åˆ†æ•°æ’åº
            order = np.argsort(-pred)[:k]
            dcg = np.sum((2**true[order] - 1) / np.log2(np.arange(2, k + 2)))
            
            # ç†æƒ³æ’åº
            ideal_order = np.argsort(-true)[:k]
            idcg = np.sum((2**true[ideal_order] - 1) / np.log2(np.arange(2, k + 2)))
            
            if idcg > 0:
                ndcgs.append(dcg / idcg)
        
        return np.mean(ndcgs) if ndcgs else 0
    
    def _calculate_top_k_return(self, returns: np.ndarray, scores: np.ndarray,
                                groups: np.ndarray, k: int = 10) -> float:
        """è®¡ç®— Top K çš„å¹³å‡æ”¶ç›Š"""
        unique_groups = np.unique(groups)
        top_returns = []
        
        for g in unique_groups:
            mask = groups == g
            if mask.sum() < k:
                continue
            
            ret = returns[mask]
            pred = scores[mask]
            
            # é€‰æ‹© Top K
            top_idx = np.argsort(-pred)[:k]
            top_returns.extend(ret[top_idx])
        
        return np.mean(top_returns) if top_returns else 0
    
    def rank(self, X: np.ndarray, horizon: TradingHorizon = TradingHorizon.SHORT) -> np.ndarray:
        """
        å¯¹ä¿¡å·è¿›è¡Œæ’åº
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            horizon: äº¤æ˜“å‘¨æœŸ
        
        Returns:
            æ’åºåˆ†æ•° (è¶Šé«˜è¶Šå¥½)
        """
        if horizon not in self.models:
            # è¿”å›é»˜è®¤åˆ†æ•°
            return np.zeros(len(X))
        
        return self.models[horizon].predict(X)
    
    def get_top_signals(self, 
                        df: pd.DataFrame,
                        X: np.ndarray,
                        horizon: TradingHorizon,
                        top_n: int = 10) -> pd.DataFrame:
        """
        è·å– Top N ä¿¡å·
        
        Args:
            df: åŸå§‹ä¿¡å· DataFrame
            X: ç‰¹å¾çŸ©é˜µ
            horizon: äº¤æ˜“å‘¨æœŸ
            top_n: è¿”å›æ•°é‡
        
        Returns:
            æ’åºåçš„ Top N DataFrame
        """
        scores = self.rank(X, horizon)
        
        result = df.copy()
        result['rank_score'] = scores
        result['rank'] = result['rank_score'].rank(ascending=False, method='first').astype(int)
        
        return result.nsmallest(top_n, 'rank')
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        import joblib
        
        save_dir = Path(path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        for horizon, model in self.models.items():
            joblib.dump(model, save_dir / f"ranker_{horizon.value}.joblib")
        
        metadata = {
            'feature_names': self.feature_names,
            'metrics': {h.value: m for h, m in self.metrics.items()},
            'horizons': [h.value for h in self.models.keys()]
        }
        with open(save_dir / "ranker_meta.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"âœ… æ’åºæ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        import joblib
        
        save_dir = Path(path)
        meta_path = save_dir / "ranker_meta.json"
        
        if not meta_path.exists():
            return False
        
        with open(meta_path) as f:
            metadata = json.load(f)
        
        self.feature_names = metadata['feature_names']
        self.metrics = {TradingHorizon(h): m for h, m in metadata['metrics'].items()}
        
        for horizon_str in metadata['horizons']:
            horizon = TradingHorizon(horizon_str)
            model_path = save_dir / f"ranker_{horizon_str}.joblib"
            if model_path.exists():
                self.models[horizon] = joblib.load(model_path)
        
        self.is_trained = len(self.models) > 0
        print(f"âœ… æ’åºæ¨¡å‹å·²åŠ è½½: {[h.value for h in self.models.keys()]}")
        return self.is_trained


# === æµ‹è¯• ===
if __name__ == "__main__":
    print("=== Signal Ranker æµ‹è¯• ===\n")
    
    # æ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 500
    n_features = 30
    n_days = 20  # 20ä¸ªäº¤æ˜“æ—¥
    
    X = np.random.randn(n_samples, n_features)
    feature_names = [f'f_{i}' for i in range(n_features)]
    
    # æ¨¡æ‹Ÿæ”¶ç›Šå’Œå›æ’¤
    returns_dict = {
        '1d': X[:, 0] * 0.5 + np.random.randn(n_samples) * 2,
        '5d': X[:, 0] * 1.0 + np.random.randn(n_samples) * 3,
        '10d': X[:, 0] * 1.5 + np.random.randn(n_samples) * 4,
        '30d': X[:, 0] * 2.0 + np.random.randn(n_samples) * 5,
        '60d': X[:, 0] * 3.0 + np.random.randn(n_samples) * 6,
    }
    
    drawdowns_dict = {
        '5d': np.abs(np.random.randn(n_samples) * 3),
        '30d': np.abs(np.random.randn(n_samples) * 5),
        '60d': np.abs(np.random.randn(n_samples) * 8),
    }
    
    # åˆ†ç»„ (æ¯å¤©25ä¸ªä¿¡å·)
    groups = np.repeat(np.arange(n_days), n_samples // n_days)
    
    ranker = SignalRanker()
    metrics = ranker.train(X, returns_dict, drawdowns_dict, groups, feature_names)
    
    print("\n=== æ’åºæµ‹è¯• ===")
    test_X = np.random.randn(10, n_features)
    for horizon in TradingHorizon:
        if horizon in ranker.models:
            scores = ranker.rank(test_X, horizon)
            print(f"{horizon.value}: {scores[:5].round(2)}")
