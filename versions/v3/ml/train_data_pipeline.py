"""
è®­ç»ƒæ•°æ®æ„å»ºæµæ°´çº¿ (Training Data Pipeline)
===========================================
ä» Parquet ç¼“å­˜è¯»å–å†å²æ•°æ®ï¼Œåº”ç”¨ FeatureEngineerï¼Œç”Ÿæˆè®­ç»ƒé›† (X, y, groups)ã€‚
è¾“å‡ºæ ¼å¼ä¸º joblibï¼Œå¯ç›´æ¥ä¾› SignalRanker è®­ç»ƒã€‚

æµç¨‹:
1. éå† data/parquet/us/*.parquet
2. å¹¶è¡Œå¤„ç†ï¼šFeature Engineering + Label Generation
3. åˆå¹¶æ•°æ®ï¼ŒæŒ‰æ—¥æœŸæ’åº
4. ç”Ÿæˆ Query Groups (ç”¨äº Learning to Rank)
5. åˆ’åˆ† Train/Val/Test
6. ä¿å­˜åˆ° data/ml/dataset_v1.joblib
"""

import os
import sys
import glob
import joblib
import numpy as np
import pandas as pd
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# Add parent dir to path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from ml.feature_engineering import FeatureEngineer

# é…ç½®
PARQUET_DIR = os.path.join(parent_dir, 'data', 'parquet', 'us')
OUTPUT_DIR = os.path.join(parent_dir, 'data', 'ml')
MIN_DATE = '2022-01-01'
MAX_WORKERS = max(1, os.cpu_count() - 2)

def process_single_stock(file_path: str) -> pd.DataFrame:
    """
    å¤„ç†å•åªè‚¡ç¥¨ï¼šè¯»å– -> ç‰¹å¾å·¥ç¨‹ -> ç”Ÿæˆæ ‡ç­¾ -> æ¸…æ´—
    """
    try:
        df = pd.read_parquet(file_path)
        if df.empty or len(df) < 100: # è‡³å°‘è¦æœ‰è¶³å¤Ÿå¾—å†å²è®¡ç®— MA100
            return None
            
        # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
        df = df.sort_values('date').reset_index(drop=True)
        
        # è¿‡æ»¤æ—¥æœŸ (å‡å°‘å†…å­˜å ç”¨)
        # æ³¨æ„: æˆ‘ä»¬éœ€è¦ä¿ç•™ MIN_DATE ä¹‹å‰çš„æ•°æ®ç”¨äºè®¡ç®— MAï¼Œ
        # æ‰€ä»¥å…ˆè®¡ç®—ç‰¹å¾ï¼Œå†è¿‡æ»¤æ—¥æœŸã€‚
        
        # 1. ç‰¹å¾å·¥ç¨‹
        fe = FeatureEngineer()
        # è¿™é‡Œæš‚æ—¶ä¸ä¼ å…¥ market_dfï¼Œåç»­å¯ä»¥ä¼˜åŒ–çš„ç‚¹
        df = fe.transform(df)
        
        # 2. ç”Ÿæˆæ ‡ç­¾ (Labels)
        # é¢„æµ‹æœªæ¥ 5å¤©, 10å¤©, 20å¤© æ”¶ç›Šç‡
        for h in [5, 10, 20]:
            df = fe.create_labels(df, horizon=h)
            
        # 3. è¿‡æ»¤æ— æ•ˆæ•°æ®
        # å»é™¤ NaN (ç”±äº rolling window å’Œ shift äº§ç”Ÿçš„)
        df = df.dropna()
        
        # 4. æˆªå–æ—¶é—´èŒƒå›´ (åªä¿ç•™ 2022 ä¹‹åçš„æ•°æ®ç”¨äºè®­ç»ƒ)
        df = df[df['date'] >= pd.Timestamp(MIN_DATE)]
        
        if df.empty:
            return None
            
        # 5. å†…å­˜ä¼˜åŒ–: Float64 -> Float32
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = df[col].astype(np.float32)
            
        # åªä¿ç•™éœ€è¦çš„åˆ—
        # Features + Labels + Metadata
        # Metadata: date, symbol
        # Labels: ret_5d, dd_5d, ...
        # Features: feature_names
        
        return df
        
    except Exception as e:
        # print(f"Error processing {file_path}: {e}")
        return None

def build_dataset():
    """æ„å»ºå®Œæ•´æ•°æ®é›†"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # è·å– Parquet æ–‡ä»¶åˆ—è¡¨
    files = glob.glob(os.path.join(PARQUET_DIR, "*.parquet"))
    print(f"Found {len(files)} parquet files in {PARQUET_DIR}")
    
    if not files:
        print("âŒ No data found! Run backfill first.")
        return
        
    all_dfs = []
    
    print(f"ğŸš€ Processing stocks with {MAX_WORKERS} workers...")
    
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_single_stock, f): f for f in files}
        
        for future in tqdm(as_completed(futures), total=len(files), unit="stock"):
            res = future.result()
            if res is not None:
                all_dfs.append(res)
                
    if not all_dfs:
        print("âŒ No valid data generated.")
        return

    print("ğŸ”„ Concatenating dataframes...")
    full_df = pd.concat(all_dfs, ignore_index=True)
    
    print(f"âœ… Raw Dataset Shape: {full_df.shape}")
    
    # æŒ‰ç…§ Date æ’åº (å¯¹äº Learning to Rank è‡³å…³é‡è¦)
    # XGBoost Ranker è¦æ±‚åŒä¸€ä¸ª group (query) çš„æ•°æ®å¿…é¡»è¿ç»­å­˜æ”¾
    print("ğŸ”„ Sorting by Date...")
    full_df = full_df.sort_values(['date', 'symbol']).reset_index(drop=True)
    
    # æå– Feature Names
    # å‡è®¾é™¤äº† date, symbol, ret_*, dd_* ä¹‹å¤–çš„éƒ½æ˜¯ç‰¹å¾
    exclude_cols = {'date', 'symbol', 'open', 'high', 'low', 'close', 'volume'}
    label_cols = {c for c in full_df.columns if c.startswith('ret_') or c.startswith('dd_')}
    feature_cols = [c for c in full_df.columns if c not in exclude_cols and c not in label_cols]
    
    print(f"Features ({len(feature_cols)}): {feature_cols[:5]} ...")
    
    # æ„å»º X, y, groups
    print("ğŸ“¦ Building (X, y, groups)...")
    
    # X
    X = full_df[feature_cols].values
    
    # y (Dict of multiple horizons)
    returns_dict = {}
    drawdowns_dict = {}
    for h in [5, 10, 20]:
        if f'ret_{h}d' in full_df.columns:
            returns_dict[f'{h}d'] = full_df[f'ret_{h}d'].values
        if f'dd_{h}d' in full_df.columns:
            drawdowns_dict[f'{h}d'] = full_df[f'dd_{h}d'].values
            
    # Groups (æ¯ä¸ªæ—¥æœŸæœ‰å¤šå°‘ä¸ªè‚¡ç¥¨)
    # è¿™ç§æ–¹æ³•æ¯” groupby ç¨å¾®å¿«ä¸€ç‚¹
    group_counts = full_df.groupby('date').size().values
    
    # å¦å¤–ä¿ç•™æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç ï¼Œç”¨äºå›æµ‹åˆ†æ
    meta_df = full_df[['date', 'symbol']].copy()
    
    # ä¿å­˜
    print("ğŸ’¾ Saving to disk...")
    save_path = os.path.join(OUTPUT_DIR, 'dataset_v1.joblib')
    joblib.dump({
        'X': X,
        'returns': returns_dict,
        'drawdowns': drawdowns_dict,
        'groups': group_counts,
        'feature_names': feature_cols,
        'meta': meta_df
    }, save_path)
    
    print(f"âœ… Dataset saved to {save_path}")
    print(f"   X shape: {X.shape}")
    print(f"   Unique dates: {len(group_counts)}")

if __name__ == "__main__":
    build_dataset()
