#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RL Trading Agent â€” å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç­–ç•¥
====================================

ä½¿ç”¨ PPO å­¦ä¹ æœ€ä¼˜äº¤æ˜“ç­–ç•¥:
- State: MMoE/LGB é¢„æµ‹ + æŠ€æœ¯å› å­ + æŒä»“çŠ¶æ€
- Action: ä»“ä½è°ƒæ•´ (è¿ç»­ [-1, 1], è´Ÿ=åšç©º)
- Reward: é£é™©è°ƒæ•´æ”¶ç›Š (Sharpe-like)

æ¶æ„:
  observation = [MMoE_dir_prob, MMoE_return_5d, LGB_dir_prob, 
                 æŠ€æœ¯å› å­(20ä¸ªç²¾é€‰), æŒä»“æ¯”ä¾‹, æµ®ç›ˆ, æŒä»“å¤©æ•°]
  action = ç›®æ ‡ä»“ä½æ¯”ä¾‹ [-1, 1]
  reward = æ—¥æ”¶ç›Šç‡ - Î» * |ä»“ä½å˜åŒ–| (é¼“åŠ±ç¨³å®šæŒä»“)
"""
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

try:
    import gymnasium as gym
    from gymnasium import spaces
    GYM_AVAILABLE = True
except ImportError:
    GYM_AVAILABLE = False

try:
    from stable_baselines3 import PPO, A2C
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import EvalCallback
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False


MODEL_DIR = Path(__file__).parent / 'saved_models' / 'rl_agent'


class TradingEnv(gym.Env):
    """
    è‚¡ç¥¨äº¤æ˜“å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
    
    å•è‚¡ç¥¨ç‰ˆæœ¬ â€” æ¯æ­¥å†³å®šä¸€åªè‚¡ç¥¨çš„ä»“ä½
    
    State (è§‚æµ‹ç©ºé—´):
        - æŠ€æœ¯ç‰¹å¾ (20 ä¸ªç²¾é€‰): RSI, MACD, å¸ƒæ—å¸¦, æ³¢åŠ¨ç‡, åŠ¨é‡ç­‰
        - ML é¢„æµ‹: direction_prob, pred_return_5d, pred_return_20d
        - æŒä»“çŠ¶æ€: position_pct, unrealized_pnl, holding_days
        - å¸‚åœºçŠ¶æ€: recent_return_5d, volatility
        æ€»è®¡ ~28 ç»´
    
    Action (åŠ¨ä½œç©ºé—´):
        - è¿ç»­ [-1, 1]: ç›®æ ‡ä»“ä½æ¯”ä¾‹
          -1 = æ»¡ä»“åšç©º, 0 = ç©ºä»“, 1 = æ»¡ä»“åšå¤š
    
    Reward:
        - æ—¥æ”¶ç›Šç‡ * 100 (æ”¾å¤§)
        - äº¤æ˜“æˆæœ¬æƒ©ç½š: -|ä»“ä½å˜åŒ–| * cost_penalty
        - å›æ’¤æƒ©ç½š: æµ®äºè¶…è¿‡é˜ˆå€¼æ—¶é¢å¤–æƒ©ç½š
    """
    
    metadata = {'render_modes': ['human']}
    
    def __init__(self, 
                 price_data: pd.DataFrame,
                 feature_data: np.ndarray,
                 feature_names: List[str],
                 initial_cash: float = 100000,
                 max_position: float = 1.0,
                 commission: float = 0.001,
                 cost_penalty: float = 0.5,
                 drawdown_penalty: float = 2.0,
                 max_steps: int = 200):
        """
        Args:
            price_data: DataFrame with 'Close' column
            feature_data: ç‰¹å¾çŸ©é˜µ (T, N_features)
            feature_names: ç‰¹å¾åç§°
            initial_cash: åˆå§‹èµ„é‡‘
            max_position: æœ€å¤§ä»“ä½æ¯”ä¾‹
            commission: å•è¾¹æ‰‹ç»­è´¹ç‡
            cost_penalty: æ¢æ‰‹æƒ©ç½šç³»æ•°
            drawdown_penalty: å›æ’¤æƒ©ç½šç³»æ•°
            max_steps: æœ€å¤§æ­¥æ•°
        """
        super().__init__()
        
        self.prices = price_data['Close'].values.astype(float)
        self.features = feature_data.astype(float)
        self.feature_names = feature_names
        self.initial_cash = initial_cash
        self.max_position = max_position
        self.commission = commission
        self.cost_penalty = cost_penalty
        self.drawdown_penalty = drawdown_penalty
        self.max_steps = min(max_steps, len(self.prices) - 1)
        
        # è§‚æµ‹ç©ºé—´: ç‰¹å¾ + æŒä»“çŠ¶æ€
        n_features = self.features.shape[1]
        self.n_obs = n_features + 5  # +5 for position state
        
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.n_obs,), dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´: ç›®æ ‡ä»“ä½ [-1, 1]
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(1,), dtype=np.float32
        )
        
        # çŠ¶æ€
        self.current_step = 0
        self.position = 0.0  # ä»“ä½æ¯”ä¾‹ [-1, 1]
        self.cash = initial_cash
        self.portfolio_value = initial_cash
        self.peak_value = initial_cash
        self.entry_price = 0.0
        self.holding_days = 0
        self.trades = []
        self.equity_curve = []
    
    def _get_obs(self) -> np.ndarray:
        """æ„å»ºè§‚æµ‹"""
        feat = self.features[self.current_step].copy()
        
        # æŒä»“çŠ¶æ€ç‰¹å¾
        unrealized_pnl = 0.0
        if abs(self.position) > 0.01 and self.entry_price > 0:
            current_price = self.prices[self.current_step]
            unrealized_pnl = (current_price / self.entry_price - 1) * np.sign(self.position) * 100
        
        drawdown = (self.portfolio_value / self.peak_value - 1) * 100 if self.peak_value > 0 else 0
        
        state = np.concatenate([
            feat,
            [self.position,           # å½“å‰ä»“ä½
             unrealized_pnl,          # æµ®ç›ˆ (%)
             min(self.holding_days / 20.0, 1.0),  # æŒä»“å¤©æ•° (å½’ä¸€åŒ–)
             drawdown,                # ç»„åˆå›æ’¤ (%)
             (self.portfolio_value / self.initial_cash - 1) * 100,  # ç´¯è®¡æ”¶ç›Š (%)
            ]
        ])
        
        return state.astype(np.float32)
    
    def _calculate_reward(self, old_value: float, new_value: float, 
                         position_change: float) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # 1. æ—¥æ”¶ç›Šç‡ (ä¸»è¦å¥–åŠ±)
        daily_return = (new_value / old_value - 1) * 100 if old_value > 0 else 0
        
        # 2. äº¤æ˜“æˆæœ¬æƒ©ç½š
        trade_cost = abs(position_change) * self.cost_penalty
        
        # 3. å›æ’¤æƒ©ç½š
        dd_penalty = 0
        if new_value < self.peak_value * 0.95:  # å›æ’¤è¶…è¿‡ 5%
            dd = (1 - new_value / self.peak_value) * 100
            dd_penalty = dd * self.drawdown_penalty * 0.01
        
        # 4. æŒä»“è¿‡ä¹…æƒ©ç½š
        hold_penalty = 0
        if self.holding_days > 30:
            hold_penalty = 0.01 * (self.holding_days - 30)
        
        reward = daily_return - trade_cost - dd_penalty - hold_penalty
        return float(reward)
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥"""
        target_position = float(np.clip(action[0], -self.max_position, self.max_position))
        
        old_value = self.portfolio_value
        position_change = target_position - self.position
        
        # æ‰§è¡Œäº¤æ˜“
        current_price = self.prices[self.current_step]
        
        if abs(position_change) > 0.01:
            # äº¤æ˜“æˆæœ¬
            trade_value = abs(position_change) * self.portfolio_value
            cost = trade_value * self.commission
            self.cash -= cost
            
            if abs(target_position) > 0.01 and abs(self.position) < 0.01:
                self.entry_price = current_price
                self.holding_days = 0
            elif abs(target_position) < 0.01:
                self.entry_price = 0
                self.holding_days = 0
        
        self.position = target_position
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥
        self.current_step += 1
        
        if self.current_step >= len(self.prices):
            return self._get_obs() if self.current_step < len(self.features) else np.zeros(self.n_obs, dtype=np.float32), 0, True, True, {}
        
        new_price = self.prices[self.current_step]
        
        # æ›´æ–°ç»„åˆä»·å€¼
        if abs(self.position) > 0.01 and self.entry_price > 0:
            price_change = (new_price / current_price - 1)
            position_pnl = self.position * price_change * self.portfolio_value
            self.portfolio_value += position_pnl
            self.holding_days += 1
        
        self.peak_value = max(self.peak_value, self.portfolio_value)
        self.equity_curve.append(self.portfolio_value)
        
        # å¥–åŠ±
        reward = self._calculate_reward(old_value, self.portfolio_value, position_change)
        
        # è®°å½•äº¤æ˜“
        if abs(position_change) > 0.05:
            self.trades.append({
                'step': self.current_step,
                'price': new_price,
                'position': self.position,
                'change': position_change,
                'portfolio_value': self.portfolio_value,
            })
        
        # ç»“æŸæ¡ä»¶
        done = (self.current_step >= self.max_steps + self.start_step or 
                self.portfolio_value < self.initial_cash * 0.8)  # äºæŸ 20% å¼ºåˆ¶ç»“æŸ
        
        truncated = self.current_step >= self.max_steps + self.start_step
        
        info = {
            'portfolio_value': self.portfolio_value,
            'position': self.position,
            'total_return': (self.portfolio_value / self.initial_cash - 1) * 100,
        }
        
        return self._get_obs(), reward, done, truncated, info
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        super().reset(seed=seed)
        
        # éšæœºèµ·å§‹ç‚¹ (é¿å…æ€»ä»å¤´å¼€å§‹)
        max_start = max(0, len(self.prices) - self.max_steps - 10)
        self.start_step = np.random.randint(0, max(1, max_start))
        self.current_step = self.start_step
        
        self.position = 0.0
        self.cash = self.initial_cash
        self.portfolio_value = self.initial_cash
        self.peak_value = self.initial_cash
        self.entry_price = 0.0
        self.holding_days = 0
        self.trades = []
        self.equity_curve = [self.initial_cash]
        
        return self._get_obs(), {}
    
    def get_stats(self) -> Dict:
        """è·å–å›æµ‹ç»Ÿè®¡"""
        if not self.equity_curve:
            return {}
        
        equity = np.array(self.equity_curve)
        returns = np.diff(equity) / equity[:-1] if len(equity) > 1 else np.array([0])
        
        total_return = (equity[-1] / equity[0] - 1) * 100
        sharpe = (np.mean(returns) / (np.std(returns) + 1e-8)) * np.sqrt(252)
        max_dd = np.max(1 - equity / np.maximum.accumulate(equity)) * 100
        
        return {
            'total_return_pct': float(total_return),
            'sharpe_ratio': float(sharpe),
            'max_drawdown_pct': float(max_dd),
            'n_trades': len(self.trades),
            'final_value': float(equity[-1]),
        }


def prepare_rl_data(symbol: str, market: str = 'US', days: int = 250) -> Tuple:
    """
    å‡†å¤‡ RL è®­ç»ƒæ•°æ®
    
    Returns:
        (price_df, feature_matrix, feature_names)
    """
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    
    from db.stock_history import get_stock_history
    from ml.advanced_features import AdvancedFeatureEngineer
    from ml.alpha_factors import Alpha158Factors
    
    history = get_stock_history(symbol, market, days=days)
    if history is None or history.empty or len(history) < 60:
        return None, None, None
    
    # æ ‡å‡†åŒ–
    if not isinstance(history.index, pd.DatetimeIndex):
        if 'Date' in history.columns:
            history = history.set_index('Date')
        elif 'date' in history.columns:
            history = history.set_index('date')
        history.index = pd.to_datetime(history.index)
    
    col_map = {}
    for c in history.columns:
        cl = c.lower()
        if cl == 'close': col_map[c] = 'Close'
        elif cl == 'open': col_map[c] = 'Open'
        elif cl == 'high': col_map[c] = 'High'
        elif cl == 'low': col_map[c] = 'Low'
        elif cl == 'volume': col_map[c] = 'Volume'
    if col_map:
        history = history.rename(columns=col_map)
    
    # è®¡ç®—ç‰¹å¾
    adv = AdvancedFeatureEngineer()
    alpha = Alpha158Factors()
    
    df_adv = adv.transform(history)
    df_alpha = alpha.compute(history)
    
    # ç²¾é€‰ 20 ä¸ªæœ€é‡è¦çš„ç‰¹å¾ (åŸºäº LGB feature importance)
    selected_features = [
        # æ³¢åŠ¨ç‡
        'volatility_20', 'atr_pct', 'vol_ratio',
        # åŠ¨é‡
        'roc_5', 'roc_20', 'momentum_acceleration',
        # è¶‹åŠ¿
        'ma_distance_20', 'adx_14', 'trend_consistency',
        # é‡ä»·
        'relative_volume_5', 'obv_slope_10',
        # Alpha158
        'rsi_12', 'macd_dif_pct', 'kdj_j_9',
        'bband_pctb_20', 'vwap_bias',
        'mfi_14', 'cci_14', 'willr_14',
        'return_skew_20',
    ]
    
    # åˆå¹¶ç‰¹å¾
    combined = pd.DataFrame(index=history.index)
    for f in selected_features:
        if f in df_adv.columns:
            combined[f] = df_adv[f]
        elif f in df_alpha.columns:
            combined[f] = df_alpha[f]
    
    # æ·»åŠ ç®€å•ä»·æ ¼ç‰¹å¾
    combined['return_1d'] = history['Close'].pct_change() * 100
    combined['return_5d'] = history['Close'].pct_change(5) * 100
    combined['return_20d'] = history['Close'].pct_change(20) * 100
    
    # ä¸¢å¼ƒ NaN
    valid_mask = combined.notna().all(axis=1)
    combined = combined[valid_mask]
    price_df = history.loc[combined.index][['Close']].copy()
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    feature_matrix = combined.values.astype(np.float32)
    feature_matrix = np.nan_to_num(feature_matrix, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Z-score æ ‡å‡†åŒ–
    mean = np.mean(feature_matrix, axis=0)
    std = np.std(feature_matrix, axis=0) + 1e-8
    feature_matrix = (feature_matrix - mean) / std
    
    return price_df, feature_matrix, list(combined.columns)


def train_rl_agent(symbols: List[str] = None,
                   market: str = 'US',
                   total_timesteps: int = 100000,
                   algorithm: str = 'PPO') -> Dict:
    """
    è®­ç»ƒ RL äº¤æ˜“æ™ºèƒ½ä½“
    
    Args:
        symbols: è®­ç»ƒç”¨çš„è‚¡ç¥¨åˆ—è¡¨
        market: å¸‚åœº
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        algorithm: 'PPO' æˆ– 'A2C'
    
    Returns:
        è®­ç»ƒç»“æœ
    """
    if not GYM_AVAILABLE or not SB3_AVAILABLE:
        return {'status': 'error', 'reason': 'Please install: pip install gymnasium stable-baselines3'}
    
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from db.database import init_db, get_scanned_dates, query_scan_results
    
    init_db()
    
    # é»˜è®¤ç”¨æœ€æ´»è·ƒçš„è‚¡ç¥¨è®­ç»ƒ
    if symbols is None:
        dates = get_scanned_dates(market=market)
        from collections import Counter
        counts = Counter()
        for d in dates[:30]:
            results = query_scan_results(scan_date=d, market=market, limit=500)
            for r in results:
                sym = r.get('symbol', '')
                price = float(r.get('price', 0) or 0)
                if sym and 5 < price < 300:
                    counts[sym] += 1
        symbols = [s for s, _ in counts.most_common(20)]
    
    print(f"ğŸ¤– RL Agent è®­ç»ƒ")
    print(f"   ç®—æ³•: {algorithm}")
    print(f"   è®­ç»ƒè‚¡ç¥¨: {len(symbols)} åª")
    print(f"   æ€»æ­¥æ•°: {total_timesteps:,}")
    
    # å‡†å¤‡å¤šä¸ªç¯å¢ƒ (ä¸åŒè‚¡ç¥¨)
    envs_data = []
    for sym in symbols:
        try:
            price_df, feat_matrix, feat_names = prepare_rl_data(sym, market)
            if price_df is not None and len(price_df) > 60:
                envs_data.append((sym, price_df, feat_matrix, feat_names))
        except Exception:
            continue
    
    if not envs_data:
        return {'status': 'error', 'reason': 'No valid training data'}
    
    print(f"   æœ‰æ•ˆè‚¡ç¥¨: {len(envs_data)} åª")
    
    # åˆ›å»ºç¯å¢ƒå·¥å‚ (æ¯æ¬¡éšæœºé€‰ä¸€åªè‚¡ç¥¨)
    def make_env():
        def _init():
            idx = np.random.randint(len(envs_data))
            sym, price_df, feat_matrix, feat_names = envs_data[idx]
            env = TradingEnv(
                price_data=price_df,
                feature_data=feat_matrix,
                feature_names=feat_names,
                max_steps=min(len(price_df) - 1, 120),
            )
            return env
        return _init
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    vec_env = DummyVecEnv([make_env() for _ in range(4)])
    
    # é€‰æ‹©ç®—æ³•
    if algorithm == 'A2C':
        model = A2C(
            'MlpPolicy', vec_env,
            learning_rate=3e-4,
            n_steps=128,
            gamma=0.99,
            ent_coef=0.01,
            verbose=1,
        )
    else:
        model = PPO(
            'MlpPolicy', vec_env,
            learning_rate=3e-4,
            n_steps=256,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            verbose=1,
        )
    
    # è®­ç»ƒ
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    model.learn(total_timesteps=total_timesteps)
    
    # ä¿å­˜æ¨¡å‹
    MODEL_DIR.mkdir(parents=True, exist_ok=True)
    model_path = str(MODEL_DIR / f'{algorithm.lower()}_trader')
    model.save(model_path)
    
    # ä¿å­˜å…ƒæ•°æ®
    meta = {
        'algorithm': algorithm,
        'total_timesteps': total_timesteps,
        'n_training_stocks': len(envs_data),
        'training_symbols': [d[0] for d in envs_data],
        'feature_names': envs_data[0][3] if envs_data else [],
        'n_features': len(envs_data[0][3]) if envs_data else 0,
    }
    with open(MODEL_DIR / 'rl_meta.json', 'w') as f:
        json.dump(meta, f, indent=2)
    
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # è¯„ä¼°
    print(f"\nğŸ“Š è¯„ä¼° (åœ¨è®­ç»ƒè‚¡ç¥¨ä¸Š)...")
    eval_results = evaluate_rl_agent(model, envs_data[:5])
    
    meta['eval_results'] = eval_results
    with open(MODEL_DIR / 'rl_meta.json', 'w') as f:
        json.dump(meta, f, indent=2)
    
    return {
        'status': 'success',
        'model_path': model_path,
        **meta,
        'eval': eval_results,
    }


def evaluate_rl_agent(model, envs_data: List, n_episodes: int = 3) -> Dict:
    """è¯„ä¼° RL æ™ºèƒ½ä½“"""
    all_stats = []
    
    for sym, price_df, feat_matrix, feat_names in envs_data:
        env = TradingEnv(
            price_data=price_df,
            feature_data=feat_matrix,
            feature_names=feat_names,
            max_steps=min(len(price_df) - 1, 120),
        )
        
        for ep in range(n_episodes):
            obs, _ = env.reset()
            done = False
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
            
            stats = env.get_stats()
            stats['symbol'] = sym
            stats['episode'] = ep
            all_stats.append(stats)
    
    if not all_stats:
        return {}
    
    df = pd.DataFrame(all_stats)
    
    result = {
        'avg_return_pct': float(df['total_return_pct'].mean()),
        'avg_sharpe': float(df['sharpe_ratio'].mean()),
        'avg_max_dd_pct': float(df['max_drawdown_pct'].mean()),
        'avg_trades': float(df['n_trades'].mean()),
        'win_rate_pct': float((df['total_return_pct'] > 0).mean() * 100),
        'best_return': float(df['total_return_pct'].max()),
        'worst_return': float(df['total_return_pct'].min()),
        'episodes': len(df),
    }
    
    print(f"  å¹³å‡æ”¶ç›Š: {result['avg_return_pct']:.1f}%")
    print(f"  Sharpe: {result['avg_sharpe']:.2f}")
    print(f"  æœ€å¤§å›æ’¤: {result['avg_max_dd_pct']:.1f}%")
    print(f"  èƒœç‡: {result['win_rate_pct']:.0f}%")
    print(f"  å¹³å‡äº¤æ˜“: {result['avg_trades']:.0f} æ¬¡")
    
    return result


# === å‘½ä»¤è¡Œå…¥å£ ===
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='RL Trading Agent')
    parser.add_argument('--market', default='US')
    parser.add_argument('--algo', default='PPO', choices=['PPO', 'A2C'])
    parser.add_argument('--steps', type=int, default=50000, help='è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--symbols', nargs='+', help='è®­ç»ƒè‚¡ç¥¨ (å¯é€‰)')
    
    args = parser.parse_args()
    
    from db.database import init_db
    init_db()
    
    result = train_rl_agent(
        symbols=args.symbols,
        market=args.market,
        total_timesteps=args.steps,
        algorithm=args.algo,
    )
    
    print(f"\n{'='*50}")
    print(json.dumps(result, indent=2, default=str))
