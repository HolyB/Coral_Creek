"""
ML è®­ç»ƒç®¡é“
ML Training Pipeline

å®Œæ•´æµç¨‹:
1. æ‹‰å–å†å² K çº¿æ•°æ®
2. è®¡ç®—æŠ€æœ¯ç‰¹å¾
3. è®¡ç®—æ ‡ç­¾ (æœªæ¥æ”¶ç›Š/å›æ’¤)
4. è®­ç»ƒæ”¶ç›Šé¢„æµ‹æ¨¡å‹
5. è®­ç»ƒæ’åºæ¨¡å‹
"""

import os
import sys
import numpy as np
import pandas as pd
from datetime import date, datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class MLPipeline:
    """ML è®­ç»ƒç®¡é“"""
    
    def __init__(self,
                 market: str = 'US',
                 days_back: int = 180,
                 commission_bps: float = 5.0,
                 slippage_bps: float = 10.0):
        self.market = market
        self.days_back = days_back
        self.commission_bps = float(commission_bps)
        self.slippage_bps = float(slippage_bps)
        # åŒè¾¹æˆæœ¬: å¼€ä»“ + å¹³ä»“
        self.round_trip_cost_pct = 2.0 * (self.commission_bps + self.slippage_bps) / 100.0
        self.model_dir = Path(__file__).parent / "saved_models" / f"v2_{market.lower()}"
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.label_cost_profile: Dict[str, Dict] = {}
        # ç»Ÿä¸€ç›®æ ‡å£å¾„: ä¸­é•¿çº¿ä¼˜å…ˆ + è¶…é¢æ”¶ç›Š + å›æ’¤æƒ©ç½š
        self.objective_config = {
            "primary_horizons": ["20d", "60d"],
            "excess_baseline": "cross_sectional_median_by_scan_date",
            "risk_penalty_lambda": {
                "20d": 0.35,
                "60d": 0.45,
            },
        }

    @staticmethod
    def _compute_group_excess(returns: np.ndarray, groups: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—æŒ‰ scan_date åˆ†ç»„çš„è¶…é¢æ”¶ç›Š:
        excess = stock_return - group_median_return
        """
        excess = np.full_like(returns, np.nan, dtype=float)
        unique_groups = np.unique(groups)
        for g in unique_groups:
            mask = groups == g
            vals = returns[mask]
            valid = ~np.isnan(vals)
            if valid.sum() == 0:
                continue
            median_val = np.nanmedian(vals)
            tmp = np.full(vals.shape, np.nan, dtype=float)
            tmp[valid] = vals[valid] - median_val
            excess[mask] = tmp
        return excess
    
    def fetch_and_store_history(self, symbols: List[str], 
                                 days: int = 365,
                                 batch_size: int = 50) -> int:
        """
        æ‹‰å–å¹¶å­˜å‚¨å†å² K çº¿æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨åˆ—è¡¨
            days: æ‹‰å–å¤©æ•°
            batch_size: æ‰¹é‡å¤§å° (é¿å… API é™åˆ¶)
        
        Returns:
            æˆåŠŸå­˜å‚¨çš„è‚¡ç¥¨æ•°
        """
        from db.stock_history import save_stock_history
        from data_fetcher import get_stock_data
        
        print(f"\nğŸ“¥ æ‹‰å– {len(symbols)} åªè‚¡ç¥¨çš„å†å²æ•°æ®...")
        
        success_count = 0
        
        for i, symbol in enumerate(symbols):
            try:
                # API é™æµ
                if i > 0 and i % batch_size == 0:
                    print(f"   è¿›åº¦: {i}/{len(symbols)}, ä¼‘æ¯ 5 ç§’...")
                    time.sleep(5)
                
                df = get_stock_data(symbol, market=self.market, days=days)
                
                if df is not None and len(df) > 60:
                    count = save_stock_history(symbol, self.market, df)
                    success_count += 1
                    
                    if (i + 1) % 100 == 0:
                        print(f"   âœ“ {i+1}/{len(symbols)}: {symbol} ({len(df)} å¤©)")
                
            except Exception as e:
                print(f"   âœ— {symbol}: {e}")
                continue
        
        print(f"âœ… æˆåŠŸå­˜å‚¨ {success_count}/{len(symbols)} åªè‚¡ç¥¨")
        return success_count
    
    def prepare_dataset(self) -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray, List[str], pd.DataFrame]:
        """
        å‡†å¤‡å®Œæ•´æ•°æ®é›†
        
        Returns:
            X: ç‰¹å¾çŸ©é˜µ
            returns_dict: æœªæ¥æ”¶ç›Šç‡å­—å…¸
            drawdowns_dict: æœªæ¥æœ€å¤§å›æ’¤å­—å…¸
            groups: æ—¥æœŸåˆ†ç»„
            feature_names: ç‰¹å¾åç§°
            df: åŸå§‹æ•°æ®
        """
        from db.database import query_scan_results, get_scanned_dates, init_db
        from db.stock_history import get_stock_history, save_stock_history
        from ml.features.feature_calculator import FeatureCalculator, FEATURE_COLUMNS
        
        print(f"\nğŸ“Š å‡†å¤‡æ•°æ®é›†...")
        
        # åˆå§‹åŒ–æ•°æ®åº“ (ç¡®ä¿è¡¨å­˜åœ¨)
        try:
            init_db()
        except:
            pass
        
        # 1. è·å–æœ‰ä¿¡å·çš„è‚¡ç¥¨ (è‡ªåŠ¨é€‰æ‹© Supabase æˆ– SQLite)
        dates = get_scanned_dates(market=self.market)
        if not dates:
            print("âŒ æ— æ‰«ææ—¥æœŸæ•°æ®")
            return None, None, None, None, None, None
        
        db_max_date = datetime.strptime(dates[0], '%Y-%m-%d').date()
        end_date = db_max_date - timedelta(days=5)  # ç•™5å¤©ç»™æ ‡ç­¾è®¡ç®—
        start_date = end_date - timedelta(days=self.days_back)
        
        print(f"   æœ€æ–°æ‰«æ: {dates[0]}, æŸ¥è¯¢èŒƒå›´: {start_date} ~ {end_date}")
        
        # æ”¶é›†å¤šå¤©çš„æ‰«æç»“æœ
        all_signals = []
        target_dates = [d for d in dates if start_date.strftime('%Y-%m-%d') <= d <= end_date.strftime('%Y-%m-%d')]
        print(f"   ç›®æ ‡æ—¥æœŸ: {len(target_dates)} å¤©")
        
        for d in target_dates:
            results = query_scan_results(scan_date=d, market=self.market, limit=1000)
            for r in results:
                all_signals.append({
                    'symbol': r.get('symbol', ''),
                    'scan_date': d,
                    'price': float(r.get('price', 0) or 0),
                    'blue_daily': float(r.get('blue_daily', 0) or 0),
                    'blue_weekly': float(r.get('blue_weekly', 0) or 0),
                    'blue_monthly': float(r.get('blue_monthly', 0) or 0),
                    'is_heima': bool(r.get('is_heima', False) or r.get('heima_daily', False)),
                })
        
        signals_df = pd.DataFrame(all_signals)
        
        if signals_df.empty:
            print("âŒ æ— ä¿¡å·æ•°æ®")
            return None, None, None, None, None, None
        
        print(f"   ä¿¡å·æ•°: {len(signals_df)}")
        
        # 2. ä¸ºæ¯ä¸ªä¿¡å·è®¡ç®—ç‰¹å¾å’Œæ ‡ç­¾
        calculator = FeatureCalculator()
        
        all_features = []
        all_returns_gross = {f'{d}d': [] for d in [1, 5, 10, 20, 30, 60]}
        all_returns_net = {f'{d}d': [] for d in [1, 5, 10, 20, 30, 60]}
        all_drawdowns = {f'{d}d': [] for d in [5, 20, 30, 60]}
        all_groups = []
        all_info = []
        
        symbols = signals_df['symbol'].unique()
        print(f"   è‚¡ç¥¨æ•°: {len(symbols)}")
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡ (é¿å… API è¶…æ—¶)
        max_symbols = 200
        if len(symbols) > max_symbols:
            # é€‰æ‹©ä¿¡å·æœ€å¤šçš„è‚¡ç¥¨
            symbol_counts = signals_df['symbol'].value_counts()
            symbols = symbol_counts.head(max_symbols).index.tolist()
            print(f"   é™åˆ¶ä¸º Top {max_symbols} è‚¡ç¥¨")
        
        # æŒ‰è‚¡ç¥¨å¤„ç†
        processed = 0
        for i, symbol in enumerate(symbols):
            # è·å–å†å²æ•°æ® (ä¼˜å…ˆæœ¬åœ°ï¼Œå¦åˆ™ API)
            history = get_stock_history(symbol, self.market, days=250)
            
            # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œä» API è·å–
            if history.empty or len(history) < 60:
                try:
                    from data_fetcher import get_stock_data
                    history = get_stock_data(symbol, market=self.market, days=250)
                    if history is not None and len(history) >= 60:
                        # ç¡®ä¿ Date æ˜¯åˆ—è€Œä¸æ˜¯ index
                        if history.index.name == 'Date':
                            history = history.reset_index()
                        if 'Date' not in history.columns and history.index.name:
                            history = history.reset_index()
                            history = history.rename(columns={history.columns[0]: 'Date'})
                        # å­˜å‚¨åˆ°æœ¬åœ°
                        save_stock_history(symbol, self.market, history)
                except Exception as e:
                    continue
                
                # API é™æµ
                if (i + 1) % 5 == 0:
                    time.sleep(0.5)
            
            if history is None or history.empty or len(history) < 60:
                continue
            
            # è®¡ç®—ç‰¹å¾
            features_df = calculator.calculate_all(history)
            if features_df.empty:
                continue
            
            # è·å–è¯¥è‚¡ç¥¨çš„ä¿¡å·
            symbol_signals = signals_df[signals_df['symbol'] == symbol]
            
            for _, signal in symbol_signals.iterrows():
                signal_date = pd.to_datetime(signal['scan_date'])
                
                # é˜²æ­¢æœªæ¥æ³„æ¼: ä»…ä½¿ç”¨ signal_date å½“å¤©æˆ–ä¹‹å‰æœ€è¿‘äº¤æ˜“æ—¥
                eligible_idx = features_df.index[features_df['Date'] <= signal_date]
                if len(eligible_idx) == 0:
                    continue
                closest_idx = int(eligible_idx[-1])
                ref_date = features_df.loc[closest_idx, 'Date']
                if (signal_date - ref_date).days > 3:
                    continue
                
                # æå–ç‰¹å¾
                feature_row = features_df.loc[closest_idx]
                
                # æ·»åŠ  BLUE ä¿¡å·ç‰¹å¾
                feature_dict = {col: feature_row.get(col, np.nan) for col in features_df.columns 
                               if col not in ['Date', 'date_diff']}
                feature_dict['blue_daily'] = signal.get('blue_daily', 0)
                feature_dict['blue_weekly'] = signal.get('blue_weekly', 0)
                feature_dict['blue_monthly'] = signal.get('blue_monthly', 0)
                feature_dict['is_heima'] = signal.get('is_heima', 0)
                
                # è®¡ç®—æœªæ¥æ”¶ç›Š (æ ‡ç­¾)
                # å…¥åœºå£å¾„: ä¿¡å·åçš„ä¸‹ä¸€äº¤æ˜“æ—¥å¼€ç›˜ä»·ï¼Œæ›´æ¥è¿‘çœŸå®æ‰§è¡Œ
                signal_idx = closest_idx
                entry_idx = signal_idx + 1
                if entry_idx >= len(features_df):
                    continue
                entry_price = features_df.loc[entry_idx, 'Open']
                if pd.isna(entry_price) or float(entry_price) <= 0:
                    continue
                
                for days in [1, 5, 10, 20, 30, 60]:
                    # ä»¥å…¥åœºæ—¥ä¸º t0ï¼ŒæŒæœ‰ N å¤©åæŒ‰æ”¶ç›˜ä»·ç¦»åœº
                    future_idx = entry_idx + days
                    if future_idx < len(features_df):
                        future_price = features_df.loc[future_idx, 'Close']
                        if pd.isna(future_price) or float(future_price) <= 0:
                            all_returns_gross[f'{days}d'].append(np.nan)
                            all_returns_net[f'{days}d'].append(np.nan)
                            continue
                        gross_return_pct = (future_price - entry_price) / entry_price * 100
                        net_return_pct = gross_return_pct - self.round_trip_cost_pct
                        all_returns_gross[f'{days}d'].append(gross_return_pct)
                        all_returns_net[f'{days}d'].append(net_return_pct)
                    else:
                        all_returns_gross[f'{days}d'].append(np.nan)
                        all_returns_net[f'{days}d'].append(np.nan)
                
                # è®¡ç®—æœªæ¥æœ€å¤§å›æ’¤
                for days in [5, 20, 30, 60]:
                    future_end = min(entry_idx + days, len(features_df) - 1)
                    if future_end > entry_idx:
                        future_prices = features_df.loc[entry_idx:future_end, 'Close'].values
                        future_prices = np.asarray(future_prices, dtype=float)
                        future_prices = future_prices[~np.isnan(future_prices)]
                        if len(future_prices) == 0:
                            all_drawdowns[f'{days}d'].append(np.nan)
                            continue
                        cummax = np.maximum.accumulate(future_prices)
                        drawdown = (cummax - future_prices) / cummax * 100
                        max_dd = np.max(drawdown)
                        all_drawdowns[f'{days}d'].append(max_dd)
                    else:
                        all_drawdowns[f'{days}d'].append(np.nan)
                
                all_features.append(feature_dict)
                all_groups.append(signal_date.toordinal())  # åŒä¸€å¤©ä¸ºä¸€ç»„
                all_info.append({
                    'symbol': symbol,
                    'scan_date': signal['scan_date'],
                    'price': entry_price
                })
        
        if not all_features:
            print("âŒ æ— æœ‰æ•ˆç‰¹å¾")
            return None, None, None, None, None, None
        
        # è½¬æ¢ä¸ºæ•°ç»„
        features_df = pd.DataFrame(all_features)
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()
        feature_names = [c for c in numeric_cols if c not in ['Date']]
        
        X = features_df[feature_names].values
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        # é™åˆ¶æç«¯å€¼
        X = np.clip(X, -1e6, 1e6)
        
        returns_dict = {k: np.array(v) for k, v in all_returns_net.items()}
        gross_returns_dict = {k: np.array(v) for k, v in all_returns_gross.items()}
        drawdowns_dict = {k: np.array(v) for k, v in all_drawdowns.items()}
        groups = np.array(all_groups)

        # ç»Ÿä¸€è®­ç»ƒç›®æ ‡:
        # 1) å¯¹ 20d/60d ä½¿ç”¨æŒ‰æ—¥æ¨ªæˆªé¢è¶…é¢æ”¶ç›Š
        # 2) åŠ å…¥æœ€å¤§å›æ’¤æƒ©ç½šï¼Œé™ä½â€œé«˜æ”¶ç›Šé«˜æ³¢åŠ¨â€æ ·æœ¬æ’å
        if len(groups) > 0:
            for horizon in ["20d", "60d"]:
                if horizon in returns_dict:
                    excess = self._compute_group_excess(returns_dict[horizon], groups)
                    dd = drawdowns_dict.get(horizon, np.full_like(excess, np.nan))
                    lam = float(self.objective_config["risk_penalty_lambda"].get(horizon, 0.0))
                    adj = excess - lam * np.nan_to_num(dd, nan=0.0)
                    returns_dict[horizon] = adj
        
        info_df = pd.DataFrame(all_info)
        
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(X)}")
        print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"   åˆ†ç»„æ•°: {len(np.unique(groups))}")
        print(f"   è®­ç»ƒæ ‡ç­¾: å‡€æ”¶ç›Š (å·²æ‰£åŒè¾¹æˆæœ¬ {self.round_trip_cost_pct:.2f}%)")

        # ä¿å­˜æ¯›/å‡€æ”¶ç›Šå¯¹æ¯”ç”»åƒï¼Œä¾› UI å±•ç¤º
        profile = {
            'market': self.market,
            'commission_bps': self.commission_bps,
            'slippage_bps': self.slippage_bps,
            'round_trip_cost_pct': self.round_trip_cost_pct,
            'objective': self.objective_config,
            'horizons': {}
        }
        for k in gross_returns_dict.keys():
            gross = gross_returns_dict[k]
            net = returns_dict[k]
            valid_mask = ~np.isnan(gross) & ~np.isnan(net)
            if valid_mask.sum() == 0:
                continue
            gross_v = gross[valid_mask]
            net_v = net[valid_mask]
            profile['horizons'][k] = {
                'samples': int(valid_mask.sum()),
                'avg_gross_return_pct': float(np.mean(gross_v)),
                'avg_net_return_pct': float(np.mean(net_v)),
                'cost_drag_pct': float(np.mean(gross_v - net_v)),
                'gross_win_rate_pct': float((gross_v > 0).mean() * 100),
                'net_win_rate_pct': float((net_v > 0).mean() * 100),
            }
        self.label_cost_profile = profile
        
        return X, returns_dict, drawdowns_dict, groups, feature_names, info_df
    
    def train_all(self, upload: bool = False) -> Dict:
        """
        è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        
        Args:
            upload: æ˜¯å¦ä¸Šä¼ åˆ° HuggingFace Hub
        
        Returns:
            è®­ç»ƒç»“æœ
        """
        from ml.models.return_predictor import ReturnPredictor
        from ml.models.signal_ranker import SignalRanker
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Coral Creek ML è®­ç»ƒç®¡é“")
        print(f"   å¸‚åœº: {self.market}")
        print(f"   æ•°æ®èŒƒå›´: è¿‘ {self.days_back} å¤©")
        print(f"{'='*60}")
        
        # 1. å‡†å¤‡æ•°æ®
        X, returns_dict, drawdowns_dict, groups, feature_names, info_df = self.prepare_dataset()
        
        if X is None:
            return {'status': 'failed', 'reason': 'æ•°æ®å‡†å¤‡å¤±è´¥'}
        
        results = {'status': 'success', 'samples': len(X), 'features': len(feature_names)}
        
        # 2. è®­ç»ƒæ”¶ç›Šé¢„æµ‹æ¨¡å‹
        print("\n" + "="*60)
        return_predictor = ReturnPredictor()
        return_metrics = return_predictor.train(X, returns_dict, feature_names, groups=groups)
        return_predictor.save(str(self.model_dir))
        results['return_predictor'] = return_metrics
        results['label_cost_profile'] = self.label_cost_profile
        
        # 3. è®­ç»ƒæ’åºæ¨¡å‹
        print("\n" + "="*60)
        ranker = SignalRanker()
        ranker_metrics = ranker.train(X, returns_dict, drawdowns_dict, groups, feature_names)
        ranker.save(str(self.model_dir))
        results['signal_ranker'] = {h.value: m for h, m in ranker_metrics.items()}
        
        # 4. ä¿å­˜ç‰¹å¾åç§°
        import json
        with open(self.model_dir / "feature_names.json", 'w') as f:
            json.dump(feature_names, f)
        if self.label_cost_profile:
            with open(self.model_dir / "training_cost_profile.json", 'w') as f:
                json.dump(self.label_cost_profile, f, indent=2)
        with open(self.model_dir / "training_objective.json", 'w') as f:
            json.dump(self.objective_config, f, indent=2)
        
        print(f"\n{'='*60}")
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {self.model_dir}")
        print(f"{'='*60}")
        
        # 5. ä¸Šä¼ åˆ° Hub (å¯é€‰)
        if upload:
            try:
                from ml.model_registry import get_registry
                registry = get_registry()
                # TODO: å®ç°æ‰¹é‡ä¸Šä¼ 
                print("ğŸ“¤ ä¸Šä¼ åŠŸèƒ½å¾…å®ç°")
            except Exception as e:
                print(f"âš ï¸ ä¸Šä¼ å¤±è´¥: {e}")
        
        return results


def train_pipeline(market: str = 'US',
                   days_back: int = 180,
                   upload: bool = False,
                   commission_bps: float = 5.0,
                   slippage_bps: float = 10.0):
    """ä¾¿æ·è®­ç»ƒå‡½æ•°"""
    pipeline = MLPipeline(
        market=market,
        days_back=days_back,
        commission_bps=commission_bps,
        slippage_bps=slippage_bps
    )
    return pipeline.train_all(upload=upload)


# === å‘½ä»¤è¡Œå…¥å£ ===
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ML è®­ç»ƒç®¡é“')
    parser.add_argument('--market', type=str, default='US', choices=['US', 'CN'])
    parser.add_argument('--days', type=int, default=180, help='æ•°æ®å¤©æ•°')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° Hub')
    parser.add_argument('--fetch', action='store_true', help='å…ˆæ‹‰å–å†å²æ•°æ®')
    parser.add_argument('--commission-bps', type=float, default=5.0, help='å•è¾¹æ‰‹ç»­è´¹ (bps)')
    parser.add_argument('--slippage-bps', type=float, default=10.0, help='å•è¾¹æ»‘ç‚¹ (bps)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ•°æ®åº“
    from db.database import init_db, query_scan_results, get_scanned_dates
    try:
        init_db()
        print("âœ… å†å²æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ æ•°æ®åº“åˆå§‹åŒ–: {e}")
    
    pipeline = MLPipeline(
        market=args.market,
        days_back=args.days,
        commission_bps=args.commission_bps,
        slippage_bps=args.slippage_bps
    )
    
    if args.fetch:
        # è·å–ä¿¡å·è‚¡ç¥¨åˆ—è¡¨ (ä» Supabase æˆ– SQLite)
        dates = get_scanned_dates(market=args.market)
        symbols = set()
        for d in dates[:30]:  # æœ€è¿‘30å¤©
            results = query_scan_results(scan_date=d, market=args.market, limit=1000)
            for r in results:
                symbols.add(r.get('symbol', ''))
        symbols = sorted([s for s in symbols if s])
        print(f"   æ‰¾åˆ° {len(symbols)} åªè‚¡ç¥¨")
        
        pipeline.fetch_and_store_history(symbols)
    
    results = pipeline.train_all(upload=args.upload)
    print(f"\nç»“æœ: {results}")
