"""
ML è®­ç»ƒç®¡é“
ML Training Pipeline

å®Œæ•´æµç¨‹:
1. æ‹‰å–å†å² K çº¿æ•°æ®
2. è®¡ç®—æŠ€æœ¯ç‰¹å¾
3. è®¡ç®—æ ‡ç­¾ (æœªæ¥æ”¶ç›Š/å›æ’¤)
4. è®­ç»ƒæ”¶ç›Šé¢„æµ‹æ¨¡å‹
5. è®­ç»ƒæ’åºæ¨¡å‹
"""

import os
import sys
import numpy as np
import pandas as pd
from datetime import date, datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class MLPipeline:
    """ML è®­ç»ƒç®¡é“"""
    
    def __init__(self, market: str = 'US', days_back: int = 180):
        self.market = market
        self.days_back = days_back
        self.model_dir = Path(__file__).parent / "saved_models" / f"v2_{market.lower()}"
        self.model_dir.mkdir(parents=True, exist_ok=True)
    
    def fetch_and_store_history(self, symbols: List[str], 
                                 days: int = 365,
                                 batch_size: int = 50) -> int:
        """
        æ‹‰å–å¹¶å­˜å‚¨å†å² K çº¿æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨åˆ—è¡¨
            days: æ‹‰å–å¤©æ•°
            batch_size: æ‰¹é‡å¤§å° (é¿å… API é™åˆ¶)
        
        Returns:
            æˆåŠŸå­˜å‚¨çš„è‚¡ç¥¨æ•°
        """
        from db.stock_history import save_stock_history
        from data_fetcher import get_stock_data
        
        print(f"\nğŸ“¥ æ‹‰å– {len(symbols)} åªè‚¡ç¥¨çš„å†å²æ•°æ®...")
        
        success_count = 0
        
        for i, symbol in enumerate(symbols):
            try:
                # API é™æµ
                if i > 0 and i % batch_size == 0:
                    print(f"   è¿›åº¦: {i}/{len(symbols)}, ä¼‘æ¯ 5 ç§’...")
                    time.sleep(5)
                
                df = get_stock_data(symbol, market=self.market, days=days)
                
                if df is not None and len(df) > 60:
                    count = save_stock_history(symbol, self.market, df)
                    success_count += 1
                    
                    if (i + 1) % 100 == 0:
                        print(f"   âœ“ {i+1}/{len(symbols)}: {symbol} ({len(df)} å¤©)")
                
            except Exception as e:
                print(f"   âœ— {symbol}: {e}")
                continue
        
        print(f"âœ… æˆåŠŸå­˜å‚¨ {success_count}/{len(symbols)} åªè‚¡ç¥¨")
        return success_count
    
    def prepare_dataset(self) -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray, List[str], pd.DataFrame]:
        """
        å‡†å¤‡å®Œæ•´æ•°æ®é›†
        
        Returns:
            X: ç‰¹å¾çŸ©é˜µ
            returns_dict: æœªæ¥æ”¶ç›Šç‡å­—å…¸
            drawdowns_dict: æœªæ¥æœ€å¤§å›æ’¤å­—å…¸
            groups: æ—¥æœŸåˆ†ç»„
            feature_names: ç‰¹å¾åç§°
            df: åŸå§‹æ•°æ®
        """
        from db.database import query_scan_results, get_scanned_dates, init_db
        from db.stock_history import get_stock_history, save_stock_history
        from ml.features.feature_calculator import FeatureCalculator, FEATURE_COLUMNS
        
        print(f"\nğŸ“Š å‡†å¤‡æ•°æ®é›†...")
        
        # åˆå§‹åŒ–æ•°æ®åº“ (ç¡®ä¿è¡¨å­˜åœ¨)
        try:
            init_db()
        except:
            pass
        
        # 1. è·å–æœ‰ä¿¡å·çš„è‚¡ç¥¨ (è‡ªåŠ¨é€‰æ‹© Supabase æˆ– SQLite)
        dates = get_scanned_dates(market=self.market)
        if not dates:
            print("âŒ æ— æ‰«ææ—¥æœŸæ•°æ®")
            return None, None, None, None, None, None
        
        db_max_date = datetime.strptime(dates[0], '%Y-%m-%d').date()
        end_date = db_max_date - timedelta(days=5)  # ç•™5å¤©ç»™æ ‡ç­¾è®¡ç®—
        start_date = end_date - timedelta(days=self.days_back)
        
        print(f"   æœ€æ–°æ‰«æ: {dates[0]}, æŸ¥è¯¢èŒƒå›´: {start_date} ~ {end_date}")
        
        # æ”¶é›†å¤šå¤©çš„æ‰«æç»“æœ
        all_signals = []
        target_dates = [d for d in dates if start_date.strftime('%Y-%m-%d') <= d <= end_date.strftime('%Y-%m-%d')]
        print(f"   ç›®æ ‡æ—¥æœŸ: {len(target_dates)} å¤©")
        
        for d in target_dates:
            results = query_scan_results(scan_date=d, market=self.market, limit=1000)
            for r in results:
                all_signals.append({
                    'symbol': r.get('symbol', ''),
                    'scan_date': d,
                    'price': float(r.get('price', 0) or 0),
                    'blue_daily': float(r.get('blue_daily', 0) or 0),
                    'blue_weekly': float(r.get('blue_weekly', 0) or 0),
                    'blue_monthly': float(r.get('blue_monthly', 0) or 0),
                    'is_heima': bool(r.get('is_heima', False) or r.get('heima_daily', False)),
                })
        
        signals_df = pd.DataFrame(all_signals)
        
        if signals_df.empty:
            print("âŒ æ— ä¿¡å·æ•°æ®")
            return None, None, None, None, None, None
        
        print(f"   ä¿¡å·æ•°: {len(signals_df)}")
        
        # 2. ä¸ºæ¯ä¸ªä¿¡å·è®¡ç®—ç‰¹å¾å’Œæ ‡ç­¾
        calculator = FeatureCalculator()
        
        all_features = []
        all_returns = {f'{d}d': [] for d in [1, 5, 10, 30, 60]}
        all_drawdowns = {f'{d}d': [] for d in [5, 30, 60]}
        all_groups = []
        all_info = []
        
        symbols = signals_df['symbol'].unique()
        print(f"   è‚¡ç¥¨æ•°: {len(symbols)}")
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡ (é¿å… API è¶…æ—¶)
        max_symbols = 200
        if len(symbols) > max_symbols:
            # é€‰æ‹©ä¿¡å·æœ€å¤šçš„è‚¡ç¥¨
            symbol_counts = signals_df['symbol'].value_counts()
            symbols = symbol_counts.head(max_symbols).index.tolist()
            print(f"   é™åˆ¶ä¸º Top {max_symbols} è‚¡ç¥¨")
        
        # æŒ‰è‚¡ç¥¨å¤„ç†
        processed = 0
        for i, symbol in enumerate(symbols):
            # è·å–å†å²æ•°æ® (ä¼˜å…ˆæœ¬åœ°ï¼Œå¦åˆ™ API)
            history = get_stock_history(symbol, self.market, days=250)
            
            # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œä» API è·å–
            if history.empty or len(history) < 60:
                try:
                    from data_fetcher import get_stock_data
                    history = get_stock_data(symbol, market=self.market, days=250)
                    if history is not None and len(history) >= 60:
                        # ç¡®ä¿ Date æ˜¯åˆ—è€Œä¸æ˜¯ index
                        if history.index.name == 'Date':
                            history = history.reset_index()
                        if 'Date' not in history.columns and history.index.name:
                            history = history.reset_index()
                            history = history.rename(columns={history.columns[0]: 'Date'})
                        # å­˜å‚¨åˆ°æœ¬åœ°
                        save_stock_history(symbol, self.market, history)
                except Exception as e:
                    continue
                
                # API é™æµ
                if (i + 1) % 5 == 0:
                    time.sleep(0.5)
            
            if history is None or history.empty or len(history) < 60:
                continue
            
            # è®¡ç®—ç‰¹å¾
            features_df = calculator.calculate_all(history)
            if features_df.empty:
                continue
            
            # è·å–è¯¥è‚¡ç¥¨çš„ä¿¡å·
            symbol_signals = signals_df[signals_df['symbol'] == symbol]
            
            for _, signal in symbol_signals.iterrows():
                signal_date = pd.to_datetime(signal['scan_date'])
                
                # æ‰¾åˆ°ä¿¡å·æ—¥æœŸåœ¨å†å²æ•°æ®ä¸­çš„ä½ç½®
                date_mask = features_df['Date'] == signal_date
                if not date_mask.any():
                    # å°è¯•æ‰¾æœ€è¿‘çš„æ—¥æœŸ
                    features_df['date_diff'] = abs(features_df['Date'] - signal_date)
                    closest_idx = features_df['date_diff'].idxmin()
                    if features_df.loc[closest_idx, 'date_diff'].days > 3:
                        continue
                else:
                    closest_idx = features_df[date_mask].index[0]
                
                # æå–ç‰¹å¾
                feature_row = features_df.loc[closest_idx]
                
                # æ·»åŠ  BLUE ä¿¡å·ç‰¹å¾
                feature_dict = {col: feature_row.get(col, np.nan) for col in features_df.columns 
                               if col not in ['Date', 'date_diff']}
                feature_dict['blue_daily'] = signal.get('blue_daily', 0)
                feature_dict['blue_weekly'] = signal.get('blue_weekly', 0)
                feature_dict['blue_monthly'] = signal.get('blue_monthly', 0)
                feature_dict['is_heima'] = signal.get('is_heima', 0)
                
                # è®¡ç®—æœªæ¥æ”¶ç›Š (æ ‡ç­¾)
                entry_price = feature_row['Close']
                signal_idx = closest_idx
                
                for days in [1, 5, 10, 30, 60]:
                    future_idx = signal_idx + days
                    if future_idx < len(features_df):
                        future_price = features_df.loc[future_idx, 'Close']
                        return_pct = (future_price - entry_price) / entry_price * 100
                        all_returns[f'{days}d'].append(return_pct)
                    else:
                        all_returns[f'{days}d'].append(np.nan)
                
                # è®¡ç®—æœªæ¥æœ€å¤§å›æ’¤
                for days in [5, 30, 60]:
                    future_end = min(signal_idx + days, len(features_df) - 1)
                    if future_end > signal_idx:
                        future_prices = features_df.loc[signal_idx:future_end, 'Close'].values
                        cummax = np.maximum.accumulate(future_prices)
                        drawdown = (cummax - future_prices) / cummax * 100
                        max_dd = np.max(drawdown)
                        all_drawdowns[f'{days}d'].append(max_dd)
                    else:
                        all_drawdowns[f'{days}d'].append(np.nan)
                
                all_features.append(feature_dict)
                all_groups.append(signal_date.toordinal())  # åŒä¸€å¤©ä¸ºä¸€ç»„
                all_info.append({
                    'symbol': symbol,
                    'scan_date': signal['scan_date'],
                    'price': entry_price
                })
        
        if not all_features:
            print("âŒ æ— æœ‰æ•ˆç‰¹å¾")
            return None, None, None, None, None, None
        
        # è½¬æ¢ä¸ºæ•°ç»„
        features_df = pd.DataFrame(all_features)
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()
        feature_names = [c for c in numeric_cols if c not in ['Date']]
        
        X = features_df[feature_names].values
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        # é™åˆ¶æç«¯å€¼
        X = np.clip(X, -1e6, 1e6)
        
        returns_dict = {k: np.array(v) for k, v in all_returns.items()}
        drawdowns_dict = {k: np.array(v) for k, v in all_drawdowns.items()}
        groups = np.array(all_groups)
        
        info_df = pd.DataFrame(all_info)
        
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(X)}")
        print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"   åˆ†ç»„æ•°: {len(np.unique(groups))}")
        
        return X, returns_dict, drawdowns_dict, groups, feature_names, info_df
    
    def train_all(self, upload: bool = False) -> Dict:
        """
        è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        
        Args:
            upload: æ˜¯å¦ä¸Šä¼ åˆ° HuggingFace Hub
        
        Returns:
            è®­ç»ƒç»“æœ
        """
        from ml.models.return_predictor import ReturnPredictor
        from ml.models.signal_ranker import SignalRanker
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Coral Creek ML è®­ç»ƒç®¡é“")
        print(f"   å¸‚åœº: {self.market}")
        print(f"   æ•°æ®èŒƒå›´: è¿‘ {self.days_back} å¤©")
        print(f"{'='*60}")
        
        # 1. å‡†å¤‡æ•°æ®
        X, returns_dict, drawdowns_dict, groups, feature_names, info_df = self.prepare_dataset()
        
        if X is None:
            return {'status': 'failed', 'reason': 'æ•°æ®å‡†å¤‡å¤±è´¥'}
        
        results = {'status': 'success', 'samples': len(X), 'features': len(feature_names)}
        
        # 2. è®­ç»ƒæ”¶ç›Šé¢„æµ‹æ¨¡å‹
        print("\n" + "="*60)
        return_predictor = ReturnPredictor()
        return_metrics = return_predictor.train(X, returns_dict, feature_names)
        return_predictor.save(str(self.model_dir))
        results['return_predictor'] = return_metrics
        
        # 3. è®­ç»ƒæ’åºæ¨¡å‹
        print("\n" + "="*60)
        ranker = SignalRanker()
        ranker_metrics = ranker.train(X, returns_dict, drawdowns_dict, groups, feature_names)
        ranker.save(str(self.model_dir))
        results['signal_ranker'] = {h.value: m for h, m in ranker_metrics.items()}
        
        # 4. ä¿å­˜ç‰¹å¾åç§°
        import json
        with open(self.model_dir / "feature_names.json", 'w') as f:
            json.dump(feature_names, f)
        
        print(f"\n{'='*60}")
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {self.model_dir}")
        print(f"{'='*60}")
        
        # 5. ä¸Šä¼ åˆ° Hub (å¯é€‰)
        if upload:
            try:
                from ml.model_registry import get_registry
                registry = get_registry()
                # TODO: å®ç°æ‰¹é‡ä¸Šä¼ 
                print("ğŸ“¤ ä¸Šä¼ åŠŸèƒ½å¾…å®ç°")
            except Exception as e:
                print(f"âš ï¸ ä¸Šä¼ å¤±è´¥: {e}")
        
        return results


def train_pipeline(market: str = 'US', days_back: int = 180, upload: bool = False):
    """ä¾¿æ·è®­ç»ƒå‡½æ•°"""
    pipeline = MLPipeline(market=market, days_back=days_back)
    return pipeline.train_all(upload=upload)


# === å‘½ä»¤è¡Œå…¥å£ ===
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ML è®­ç»ƒç®¡é“')
    parser.add_argument('--market', type=str, default='US', choices=['US', 'CN'])
    parser.add_argument('--days', type=int, default=180, help='æ•°æ®å¤©æ•°')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° Hub')
    parser.add_argument('--fetch', action='store_true', help='å…ˆæ‹‰å–å†å²æ•°æ®')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ•°æ®åº“
    from db.database import init_db, query_scan_results, get_scanned_dates
    try:
        init_db()
        print("âœ… å†å²æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ æ•°æ®åº“åˆå§‹åŒ–: {e}")
    
    pipeline = MLPipeline(market=args.market, days_back=args.days)
    
    if args.fetch:
        # è·å–ä¿¡å·è‚¡ç¥¨åˆ—è¡¨ (ä» Supabase æˆ– SQLite)
        dates = get_scanned_dates(market=args.market)
        symbols = set()
        for d in dates[:30]:  # æœ€è¿‘30å¤©
            results = query_scan_results(scan_date=d, market=args.market, limit=1000)
            for r in results:
                symbols.add(r.get('symbol', ''))
        symbols = sorted([s for s in symbols if s])
        print(f"   æ‰¾åˆ° {len(symbols)} åªè‚¡ç¥¨")
        
        pipeline.fetch_and_store_history(symbols)
    
    results = pipeline.train_all(upload=args.upload)
    print(f"\nç»“æœ: {results}")
