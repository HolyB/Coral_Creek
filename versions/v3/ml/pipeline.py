"""
ML è®­ç»ƒç®¡é“
ML Training Pipeline

å®Œæ•´æµç¨‹:
1. æ‹‰å–å†å² K çº¿æ•°æ®
2. è®¡ç®—æŠ€æœ¯ç‰¹å¾
3. è®¡ç®—æ ‡ç­¾ (æœªæ¥æ”¶ç›Š/å›æ’¤)
4. è®­ç»ƒæ”¶ç›Šé¢„æµ‹æ¨¡å‹
5. è®­ç»ƒæ’åºæ¨¡å‹
"""

import os
import sys
import numpy as np
import pandas as pd
from datetime import date, datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import time
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class MLPipeline:
    """ML è®­ç»ƒç®¡é“"""
    
    # ä»·æ ¼åˆ†å±‚é˜ˆå€¼
    PRICE_TIERS = {
        'US': {'standard': (5.0, 9999), 'penny': (0.01, 5.0)},
        'CN': {'standard': (3.0, 9999), 'penny': (0.01, 3.0)},
    }

    def __init__(self,
                 market: str = 'US',
                 days_back: int = 180,
                 commission_bps: float = 5.0,
                 slippage_bps: float = 10.0,
                 use_fundamental_features: bool = True,
                 enable_fundamental_api: bool = False,
                 price_tier: str = 'standard'):
        """
        Args:
            price_tier: 'standard' (>=5), 'penny' (<5), or 'all' (ä¸è¿‡æ»¤)
        """
        self.market = market
        self.days_back = days_back
        self.price_tier = price_tier
        self.commission_bps = float(commission_bps)
        self.slippage_bps = float(slippage_bps)
        # åŒè¾¹æˆæœ¬: å¼€ä»“ + å¹³ä»“ (ä½ä»·è‚¡æ»‘ç‚¹æ›´å¤§)
        if price_tier == 'penny':
            self.round_trip_cost_pct = 2.0 * (self.commission_bps + self.slippage_bps * 3) / 100.0
        else:
            self.round_trip_cost_pct = 2.0 * (self.commission_bps + self.slippage_bps) / 100.0
        # åŸºæœ¬é¢ç‰¹å¾å¼€å…³
        self.use_fundamental_features = bool(use_fundamental_features)
        self.enable_fundamental_api = bool(enable_fundamental_api)
        # æ¨¡å‹ç›®å½•: standard â†’ v2_us, penny â†’ v2_us_penny
        tier_suffix = f"_{price_tier}" if price_tier != 'standard' else ''
        self.model_dir = Path(__file__).parent / "saved_models" / f"v2_{market.lower()}{tier_suffix}"
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.label_cost_profile: Dict[str, Dict] = {}
        self.feature_stability_report: Dict[str, Dict] = {}
        self.walk_forward_report: Dict[str, Dict] = {}
        # ç»Ÿä¸€ç›®æ ‡å£å¾„: ä¸­é•¿çº¿ä¼˜å…ˆ + è¶…é¢æ”¶ç›Š + å›æ’¤æƒ©ç½š
        self.objective_config = {
            "primary_horizons": ["20d", "60d"],
            "excess_baseline": "cross_sectional_median_by_scan_date",
            "risk_penalty_lambda": {
                "20d": 0.35,
                "60d": 0.45,
            },
        }

    @staticmethod
    def _compute_group_excess(returns: np.ndarray, groups: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—æŒ‰ scan_date åˆ†ç»„çš„è¶…é¢æ”¶ç›Š:
        excess = stock_return - group_median_return
        """
        excess = np.full_like(returns, np.nan, dtype=float)
        unique_groups = np.unique(groups)
        for g in unique_groups:
            mask = groups == g
            vals = returns[mask]
            valid = ~np.isnan(vals)
            if valid.sum() == 0:
                continue
            median_val = np.nanmedian(vals)
            tmp = np.full(vals.shape, np.nan, dtype=float)
            tmp[valid] = vals[valid] - median_val
            excess[mask] = tmp
        return excess

    def _build_feature_stability_report(
        self,
        X_raw: np.ndarray,
        feature_names: List[str],
        groups: np.ndarray,
        returns_dict: Dict[str, np.ndarray],
    ) -> Dict:
        """ç”Ÿæˆç‰¹å¾ç¨³å®šæ€§æŠ¥å‘Šï¼ˆç¼ºå¤±ç‡/æ¼‚ç§»/ICï¼‰"""
        if X_raw is None or len(X_raw) == 0 or len(feature_names) == 0:
            return {}

        report_rows = []
        n = len(X_raw)
        split = int(n * 0.6)
        split = min(max(split, 1), n - 1) if n > 1 else 1

        y20 = returns_dict.get('20d', np.array([]))
        y60 = returns_dict.get('60d', np.array([]))

        for i, feat in enumerate(feature_names):
            col = X_raw[:, i]
            col = np.asarray(col, dtype=float)
            missing_rate = float(np.isnan(col).mean()) if len(col) else 1.0

            first = col[:split]
            last = col[split:]
            first_valid = first[~np.isnan(first)]
            last_valid = last[~np.isnan(last)]

            mean_first = float(np.mean(first_valid)) if len(first_valid) else 0.0
            mean_last = float(np.mean(last_valid)) if len(last_valid) else 0.0
            std_first = float(np.std(first_valid)) if len(first_valid) else 0.0
            std_last = float(np.std(last_valid)) if len(last_valid) else 0.0

            pooled_std = (std_first + std_last) / 2.0 if (std_first + std_last) > 0 else 1.0
            drift_score = abs(mean_last - mean_first) / pooled_std

            ic20 = np.nan
            if len(y20) == len(col):
                valid = ~np.isnan(col) & ~np.isnan(y20)
                if valid.sum() > 20:
                    ic20 = pd.Series(col[valid]).corr(pd.Series(y20[valid]), method='spearman')

            ic60 = np.nan
            if len(y60) == len(col):
                valid = ~np.isnan(col) & ~np.isnan(y60)
                if valid.sum() > 20:
                    ic60 = pd.Series(col[valid]).corr(pd.Series(y60[valid]), method='spearman')

            report_rows.append({
                'feature': feat,
                'missing_rate': float(missing_rate),
                'drift_score': float(drift_score),
                'ic_20d': float(ic20) if pd.notna(ic20) else np.nan,
                'ic_60d': float(ic60) if pd.notna(ic60) else np.nan,
            })

        df = pd.DataFrame(report_rows)
        if df.empty:
            return {}

        # ç»Ÿä¸€ç¨³å®šæ€§è¯„åˆ†ï¼šä½ç¼ºå¤±ã€ä½æ¼‚ç§»ã€é«˜|IC|
        ic_abs = (df['ic_20d'].abs().fillna(0) + df['ic_60d'].abs().fillna(0)) / 2.0
        df['stability_score'] = (
            (1.0 - df['missing_rate'].clip(0, 1)) * 0.35
            + (1.0 / (1.0 + df['drift_score'].clip(lower=0))) * 0.30
            + (ic_abs.clip(0, 1)) * 0.35
        )

        top_stable = df.sort_values('stability_score', ascending=False).head(30)
        top_unstable = df.sort_values('stability_score', ascending=True).head(30)

        summary = {
            'feature_count': int(len(df)),
            'avg_missing_rate': float(df['missing_rate'].mean()),
            'avg_drift_score': float(df['drift_score'].mean()),
            'avg_abs_ic20': float(df['ic_20d'].abs().mean(skipna=True)),
            'avg_abs_ic60': float(df['ic_60d'].abs().mean(skipna=True)),
        }

        return {
            'summary': summary,
            'top_stable_features': top_stable.to_dict('records'),
            'top_unstable_features': top_unstable.to_dict('records'),
        }

    def _run_walk_forward_eval(
        self,
        X: np.ndarray,
        returns_dict: Dict[str, np.ndarray],
        groups: np.ndarray,
    ) -> Dict:
        """æ»šåŠ¨è®­ç»ƒ-æµ‹è¯•è¯„ä¼°ï¼ŒéªŒè¯ä¸­é•¿çº¿ç›®æ ‡ç¨³å¥æ€§"""
        try:
            import xgboost as xgb
        except Exception:
            return {'status': 'skipped', 'reason': 'xgboost_not_available'}

        y = returns_dict.get('20d')
        if y is None or len(y) != len(X):
            return {'status': 'skipped', 'reason': 'missing_20d_target'}

        unique_groups = np.array(sorted(np.unique(groups)))
        if len(unique_groups) < 80:
            return {'status': 'skipped', 'reason': f'not_enough_groups:{len(unique_groups)}'}

        train_span = 50
        test_span = 10
        step = 10

        folds = []
        for start in range(0, len(unique_groups) - train_span - test_span + 1, step):
            tr_g = unique_groups[start:start + train_span]
            te_g = unique_groups[start + train_span:start + train_span + test_span]
            tr_mask = np.isin(groups, tr_g)
            te_mask = np.isin(groups, te_g)

            X_tr, y_tr = X[tr_mask], y[tr_mask]
            X_te, y_te = X[te_mask], y[te_mask]
            g_te = groups[te_mask]

            valid_tr = ~np.isnan(y_tr)
            valid_te = ~np.isnan(y_te)
            X_tr, y_tr = X_tr[valid_tr], y_tr[valid_tr]
            X_te, y_te, g_te = X_te[valid_te], y_te[valid_te], g_te[valid_te]

            if len(X_tr) < 200 or len(X_te) < 50:
                continue

            model = xgb.XGBRegressor(
                n_estimators=180,
                max_depth=5,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
            )
            model.fit(X_tr, y_tr, verbose=False)
            pred = model.predict(X_te)

            ic = pd.Series(pred).corr(pd.Series(y_te), method='spearman')
            dir_acc = float(((pred > 0) == (y_te > 0)).mean())

            top_returns = []
            for g in np.unique(g_te):
                m = g_te == g
                if m.sum() < 5:
                    continue
                idx = np.argsort(-pred[m])
                k = max(1, int(len(idx) * 0.2))
                top_returns.append(float(np.mean(y_te[m][idx[:k]])))
            top_ret = float(np.mean(top_returns)) if top_returns else np.nan

            folds.append({
                'train_groups': int(len(tr_g)),
                'test_groups': int(len(te_g)),
                'spearman_ic': float(ic) if pd.notna(ic) else np.nan,
                'direction_acc': dir_acc,
                'top20_avg_return': top_ret,
            })

        if not folds:
            return {'status': 'skipped', 'reason': 'no_valid_folds'}

        df = pd.DataFrame(folds)
        return {
            'status': 'ok',
            'n_folds': int(len(df)),
            'avg_spearman_ic': float(df['spearman_ic'].mean(skipna=True)),
            'avg_direction_acc': float(df['direction_acc'].mean(skipna=True)),
            'avg_top20_return': float(df['top20_avg_return'].mean(skipna=True)),
            'folds': folds,
        }
    
    def fetch_and_store_history(self, symbols: List[str], 
                                 days: int = 365,
                                 batch_size: int = 50) -> int:
        """
        æ‹‰å–å¹¶å­˜å‚¨å†å² K çº¿æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨åˆ—è¡¨
            days: æ‹‰å–å¤©æ•°
            batch_size: æ‰¹é‡å¤§å° (é¿å… API é™åˆ¶)
        
        Returns:
            æˆåŠŸå­˜å‚¨çš„è‚¡ç¥¨æ•°
        """
        from db.stock_history import save_stock_history
        from data_fetcher import get_stock_data
        
        print(f"\nğŸ“¥ æ‹‰å– {len(symbols)} åªè‚¡ç¥¨çš„å†å²æ•°æ®...")
        
        success_count = 0
        
        for i, symbol in enumerate(symbols):
            try:
                # API é™æµ
                if i > 0 and i % batch_size == 0:
                    print(f"   è¿›åº¦: {i}/{len(symbols)}, ä¼‘æ¯ 5 ç§’...")
                    time.sleep(5)
                
                df = get_stock_data(symbol, market=self.market, days=days)
                
                if df is not None and len(df) > 60:
                    count = save_stock_history(symbol, self.market, df)
                    success_count += 1
                    
                    if (i + 1) % 100 == 0:
                        print(f"   âœ“ {i+1}/{len(symbols)}: {symbol} ({len(df)} å¤©)")
                
            except Exception as e:
                print(f"   âœ— {symbol}: {e}")
                continue
        
        print(f"âœ… æˆåŠŸå­˜å‚¨ {success_count}/{len(symbols)} åªè‚¡ç¥¨")
        return success_count
    
    def prepare_dataset(self) -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray, List[str], pd.DataFrame]:
        """
        å‡†å¤‡å®Œæ•´æ•°æ®é›†
        
        Returns:
            X: ç‰¹å¾çŸ©é˜µ
            returns_dict: æœªæ¥æ”¶ç›Šç‡å­—å…¸
            drawdowns_dict: æœªæ¥æœ€å¤§å›æ’¤å­—å…¸
            groups: æ—¥æœŸåˆ†ç»„
            feature_names: ç‰¹å¾åç§°
            df: åŸå§‹æ•°æ®
        """
        from db.database import query_scan_results, get_scanned_dates, init_db
        from db.stock_history import get_stock_history, save_stock_history
        from ml.features.feature_calculator import FeatureCalculator, FEATURE_COLUMNS
        from ml.features.fundamental_features import build_fundamental_features
        
        print(f"\nğŸ“Š å‡†å¤‡æ•°æ®é›†...")
        
        # åˆå§‹åŒ–æ•°æ®åº“ (ç¡®ä¿è¡¨å­˜åœ¨)
        try:
            init_db()
        except:
            pass
        
        # 1. è·å–æœ‰ä¿¡å·çš„è‚¡ç¥¨ (è‡ªåŠ¨é€‰æ‹© Supabase æˆ– SQLite)
        dates = get_scanned_dates(market=self.market)
        if not dates:
            print("âŒ æ— æ‰«ææ—¥æœŸæ•°æ®")
            return None, None, None, None, None, None
        
        db_max_date = datetime.strptime(dates[0], '%Y-%m-%d').date()
        end_date = db_max_date - timedelta(days=5)  # ç•™5å¤©ç»™æ ‡ç­¾è®¡ç®—
        start_date = end_date - timedelta(days=self.days_back)
        
        print(f"   æœ€æ–°æ‰«æ: {dates[0]}, æŸ¥è¯¢èŒƒå›´: {start_date} ~ {end_date}")
        
        # æ”¶é›†å¤šå¤©çš„æ‰«æç»“æœ
        all_signals = []
        target_dates = [d for d in dates if start_date.strftime('%Y-%m-%d') <= d <= end_date.strftime('%Y-%m-%d')]
        print(f"   ç›®æ ‡æ—¥æœŸ: {len(target_dates)} å¤©")
        
        for d in target_dates:
            results = query_scan_results(scan_date=d, market=self.market, limit=1000)
            for r in results:
                all_signals.append({
                    'symbol': r.get('symbol', ''),
                    'scan_date': d,
                    'price': float(r.get('price', 0) or 0),
                    'blue_daily': float(r.get('blue_daily', 0) or 0),
                    'blue_weekly': float(r.get('blue_weekly', 0) or 0),
                    'blue_monthly': float(r.get('blue_monthly', 0) or 0),
                    'is_heima': bool(r.get('is_heima', False) or r.get('heima_daily', False)),
                    # åŸºæœ¬é¢/åˆ†å±‚å­—æ®µï¼ˆä¼˜å…ˆç”¨æ‰«æç»“æœè‡ªå¸¦ï¼‰
                    'market_cap': float(r.get('market_cap', 0) or 0),
                    'industry': str(r.get('industry', '') or ''),
                    'cap_category': str(r.get('cap_category', '') or ''),
                })
        
        signals_df = pd.DataFrame(all_signals)
        
        if signals_df.empty:
            print("âŒ æ— ä¿¡å·æ•°æ®")
            return None, None, None, None, None, None
        
        print(f"   ä¿¡å·æ•°: {len(signals_df)}")
        
        # 2. ä¸ºæ¯ä¸ªä¿¡å·è®¡ç®—ç‰¹å¾å’Œæ ‡ç­¾
        calculator = FeatureCalculator()
        
        all_features = []
        all_returns_gross = {f'{d}d': [] for d in [1, 5, 10, 20, 30, 60]}
        all_returns_net = {f'{d}d': [] for d in [1, 5, 10, 20, 30, 60]}
        all_drawdowns = {f'{d}d': [] for d in [5, 20, 30, 60]}
        all_groups = []
        all_info = []
        
        # æŒ‰ä»·æ ¼åˆ†å±‚è¿‡æ»¤
        if 'price' in signals_df.columns and self.price_tier != 'all':
            tiers = self.PRICE_TIERS.get(self.market, self.PRICE_TIERS['US'])
            lo, hi = tiers.get(self.price_tier, (0.01, 9999))
            before_filter = len(signals_df)
            signals_df = signals_df[(signals_df['price'] >= lo) & (signals_df['price'] < hi)]
            filtered_out = before_filter - len(signals_df)
            if filtered_out > 0:
                print(f"   ä»·æ ¼åˆ†å±‚ [{self.price_tier}] ${lo}~${hi}: ä¿ç•™ {len(signals_df)}, æ’é™¤ {filtered_out}")

        symbols = signals_df['symbol'].unique()
        print(f"   è‚¡ç¥¨æ•°: {len(symbols)}")
        
        # ä¼˜å…ˆé€‰æœ‰æœ¬åœ°å†å²æ•°æ®çš„è‚¡ç¥¨ï¼Œæ— é™åˆ¶ï¼›æ— å†å²çš„é™åˆ¶ API è°ƒç”¨é‡
        local_symbols = set()
        try:
            import sqlite3 as _sq3
            from db.stock_history import get_history_db_path
            _hconn = _sq3.connect(get_history_db_path())
            _hcur = _hconn.cursor()
            _ph = ",".join(["?"] * min(len(symbols), 5000))
            _hcur.execute(
                f"SELECT DISTINCT symbol FROM stock_history WHERE market = ? AND symbol IN ({_ph})",
                [self.market] + list(symbols)[:5000],
            )
            local_symbols = set(r[0] for r in _hcur.fetchall())
            _hconn.close()
        except Exception:
            pass

        # æœ‰æœ¬åœ°å†å²çš„å…¨éƒ¨ç”¨ï¼Œæ²¡æœ‰çš„æŒ‰ä¿¡å·é¢‘ç‡å– top 200 (é¿å… API è¶…æ—¶)
        symbol_counts = signals_df['symbol'].value_counts()
        has_hist = [s for s in symbol_counts.index if s in local_symbols]
        no_hist = [s for s in symbol_counts.index if s not in local_symbols]
        max_api_symbols = 200
        symbols = has_hist + no_hist[:max_api_symbols]
        print(f"   æœ‰æœ¬åœ°å†å²: {len(has_hist)}, éœ€ API: {min(len(no_hist), max_api_symbols)}, æ€»è®¡: {len(symbols)}")
        
        # æŒ‰è‚¡ç¥¨å¤„ç†
        # åŸºæœ¬é¢å¤–éƒ¨ API ç¼“å­˜ï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
        fundamental_external_cache: Dict[str, Dict[str, float]] = {}
        processed = 0
        for i, symbol in enumerate(symbols):
            # è·å–å†å²æ•°æ® (ä¼˜å…ˆæœ¬åœ°ï¼Œå¦åˆ™ API)
            history = get_stock_history(symbol, self.market, days=250)
            
            # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œä» API è·å–
            if history.empty or len(history) < 60:
                try:
                    from data_fetcher import get_stock_data
                    history = get_stock_data(symbol, market=self.market, days=250)
                    if history is not None and len(history) >= 60:
                        # ç¡®ä¿ Date æ˜¯åˆ—è€Œä¸æ˜¯ index
                        if history.index.name == 'Date':
                            history = history.reset_index()
                        if 'Date' not in history.columns and history.index.name:
                            history = history.reset_index()
                            history = history.rename(columns={history.columns[0]: 'Date'})
                        # å­˜å‚¨åˆ°æœ¬åœ°
                        save_stock_history(symbol, self.market, history)
                except Exception as e:
                    continue
                
                # API é™æµ
                if (i + 1) % 5 == 0:
                    time.sleep(0.5)
            
            if history is None or history.empty or len(history) < 60:
                continue
            
            # è®¡ç®—ç‰¹å¾
            features_df = calculator.calculate_all(history)
            if features_df.empty:
                continue
            
            # è·å–è¯¥è‚¡ç¥¨çš„ä¿¡å·
            symbol_signals = signals_df[signals_df['symbol'] == symbol]
            
            for _, signal in symbol_signals.iterrows():
                signal_date = pd.to_datetime(signal['scan_date'])
                
                # é˜²æ­¢æœªæ¥æ³„æ¼: ä»…ä½¿ç”¨ signal_date å½“å¤©æˆ–ä¹‹å‰æœ€è¿‘äº¤æ˜“æ—¥
                eligible_idx = features_df.index[features_df['Date'] <= signal_date]
                if len(eligible_idx) == 0:
                    continue
                closest_idx = int(eligible_idx[-1])
                ref_date = features_df.loc[closest_idx, 'Date']
                if (signal_date - ref_date).days > 3:
                    continue
                
                # æå–ç‰¹å¾
                feature_row = features_df.loc[closest_idx]
                
                # æ·»åŠ  BLUE ä¿¡å·ç‰¹å¾
                feature_dict = {col: feature_row.get(col, np.nan) for col in features_df.columns 
                               if col not in ['Date', 'date_diff']}
                feature_dict['blue_daily'] = signal.get('blue_daily', 0)
                feature_dict['blue_weekly'] = signal.get('blue_weekly', 0)
                feature_dict['blue_monthly'] = signal.get('blue_monthly', 0)
                feature_dict['is_heima'] = signal.get('is_heima', 0)

                # åŠ å…¥åŸºæœ¬é¢ç‰¹å¾ï¼ˆä¸­æ–‡æ³¨é‡Šï¼šè¿™é‡Œæ˜¯ä½é¢‘å¿«ç…§ç‰¹å¾ï¼‰
                if self.use_fundamental_features:
                    fund_feats = build_fundamental_features(
                        symbol=symbol,
                        market=self.market,
                        signal_row=signal.to_dict() if hasattr(signal, "to_dict") else dict(signal),
                        enable_external_api=self.enable_fundamental_api,
                        external_cache=fundamental_external_cache,
                    )
                    feature_dict.update(fund_feats)
                
                # è®¡ç®—æœªæ¥æ”¶ç›Š (æ ‡ç­¾)
                # å…¥åœºå£å¾„: ä¿¡å·åçš„ä¸‹ä¸€äº¤æ˜“æ—¥å¼€ç›˜ä»·ï¼Œæ›´æ¥è¿‘çœŸå®æ‰§è¡Œ
                signal_idx = closest_idx
                entry_idx = signal_idx + 1
                if entry_idx >= len(features_df):
                    continue
                entry_price = features_df.loc[entry_idx, 'Open']
                if pd.isna(entry_price) or float(entry_price) <= 0:
                    continue
                
                for days in [1, 5, 10, 20, 30, 60]:
                    # ä»¥å…¥åœºæ—¥ä¸º t0ï¼ŒæŒæœ‰ N å¤©åæŒ‰æ”¶ç›˜ä»·ç¦»åœº
                    future_idx = entry_idx + days
                    if future_idx < len(features_df):
                        future_price = features_df.loc[future_idx, 'Close']
                        if pd.isna(future_price) or float(future_price) <= 0:
                            all_returns_gross[f'{days}d'].append(np.nan)
                            all_returns_net[f'{days}d'].append(np.nan)
                            continue
                        gross_return_pct = (future_price - entry_price) / entry_price * 100
                        net_return_pct = gross_return_pct - self.round_trip_cost_pct
                        all_returns_gross[f'{days}d'].append(gross_return_pct)
                        all_returns_net[f'{days}d'].append(net_return_pct)
                    else:
                        all_returns_gross[f'{days}d'].append(np.nan)
                        all_returns_net[f'{days}d'].append(np.nan)
                
                # è®¡ç®—æœªæ¥æœ€å¤§å›æ’¤
                for days in [5, 20, 30, 60]:
                    future_end = min(entry_idx + days, len(features_df) - 1)
                    if future_end > entry_idx:
                        future_prices = features_df.loc[entry_idx:future_end, 'Close'].values
                        future_prices = np.asarray(future_prices, dtype=float)
                        future_prices = future_prices[~np.isnan(future_prices)]
                        if len(future_prices) == 0:
                            all_drawdowns[f'{days}d'].append(np.nan)
                            continue
                        cummax = np.maximum.accumulate(future_prices)
                        drawdown = (cummax - future_prices) / cummax * 100
                        max_dd = np.max(drawdown)
                        all_drawdowns[f'{days}d'].append(max_dd)
                    else:
                        all_drawdowns[f'{days}d'].append(np.nan)
                
                all_features.append(feature_dict)
                all_groups.append(signal_date.toordinal())  # åŒä¸€å¤©ä¸ºä¸€ç»„
                all_info.append({
                    'symbol': symbol,
                    'scan_date': signal['scan_date'],
                    'price': entry_price
                })
        
        if not all_features:
            print("âŒ æ— æœ‰æ•ˆç‰¹å¾")
            return None, None, None, None, None, None
        
        # è½¬æ¢ä¸ºæ•°ç»„
        features_df = pd.DataFrame(all_features)
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()
        feature_names = [c for c in numeric_cols if c not in ['Date']]
        
        X_raw = features_df[feature_names].values.astype(float)
        X = X_raw.copy()
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        # é™åˆ¶æç«¯å€¼
        X = np.clip(X, -1e6, 1e6)
        
        returns_dict = {k: np.array(v) for k, v in all_returns_net.items()}
        gross_returns_dict = {k: np.array(v) for k, v in all_returns_gross.items()}
        drawdowns_dict = {k: np.array(v) for k, v in all_drawdowns.items()}
        groups = np.array(all_groups)

        # ç»Ÿä¸€è®­ç»ƒç›®æ ‡:
        # 1) å¯¹ 20d/60d ä½¿ç”¨æŒ‰æ—¥æ¨ªæˆªé¢è¶…é¢æ”¶ç›Š
        # 2) åŠ å…¥æœ€å¤§å›æ’¤æƒ©ç½šï¼Œé™ä½â€œé«˜æ”¶ç›Šé«˜æ³¢åŠ¨â€æ ·æœ¬æ’å
        if len(groups) > 0:
            for horizon in ["20d", "60d"]:
                if horizon in returns_dict:
                    excess = self._compute_group_excess(returns_dict[horizon], groups)
                    dd = drawdowns_dict.get(horizon, np.full_like(excess, np.nan))
                    lam = float(self.objective_config["risk_penalty_lambda"].get(horizon, 0.0))
                    adj = excess - lam * np.nan_to_num(dd, nan=0.0)
                    returns_dict[horizon] = adj

        # ç”Ÿæˆç‰¹å¾ç¨³å®šæ€§æŠ¥å‘Š
        self.feature_stability_report = self._build_feature_stability_report(
            X_raw=X_raw,
            feature_names=feature_names,
            groups=groups,
            returns_dict=returns_dict,
        )
        
        info_df = pd.DataFrame(all_info)
        
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(X)}")
        print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"   åˆ†ç»„æ•°: {len(np.unique(groups))}")
        print(
            "   åŸºæœ¬é¢ç‰¹å¾: {} (å¤–éƒ¨API:{})".format(
                "å¯ç”¨" if self.use_fundamental_features else "å…³é—­",
                "å¯ç”¨" if self.enable_fundamental_api else "å…³é—­",
            )
        )
        print(f"   è®­ç»ƒæ ‡ç­¾: å‡€æ”¶ç›Š (å·²æ‰£åŒè¾¹æˆæœ¬ {self.round_trip_cost_pct:.2f}%)")

        # ä¿å­˜æ¯›/å‡€æ”¶ç›Šå¯¹æ¯”ç”»åƒï¼Œä¾› UI å±•ç¤º
        profile = {
            'market': self.market,
            'commission_bps': self.commission_bps,
            'slippage_bps': self.slippage_bps,
            'round_trip_cost_pct': self.round_trip_cost_pct,
            'objective': self.objective_config,
            'horizons': {}
        }
        for k in gross_returns_dict.keys():
            gross = gross_returns_dict[k]
            net = returns_dict[k]
            valid_mask = ~np.isnan(gross) & ~np.isnan(net)
            if valid_mask.sum() == 0:
                continue
            gross_v = gross[valid_mask]
            net_v = net[valid_mask]
            profile['horizons'][k] = {
                'samples': int(valid_mask.sum()),
                'avg_gross_return_pct': float(np.mean(gross_v)),
                'avg_net_return_pct': float(np.mean(net_v)),
                'cost_drag_pct': float(np.mean(gross_v - net_v)),
                'gross_win_rate_pct': float((gross_v > 0).mean() * 100),
                'net_win_rate_pct': float((net_v > 0).mean() * 100),
            }
        self.label_cost_profile = profile
        
        return X, returns_dict, drawdowns_dict, groups, feature_names, info_df
    
    def train_all(self, upload: bool = False) -> Dict:
        """
        è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        
        Args:
            upload: æ˜¯å¦ä¸Šä¼ åˆ° HuggingFace Hub
        
        Returns:
            è®­ç»ƒç»“æœ
        """
        from ml.models.return_predictor import ReturnPredictor
        from ml.models.signal_ranker import SignalRanker
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Coral Creek ML è®­ç»ƒç®¡é“")
        print(f"   å¸‚åœº: {self.market}")
        print(f"   æ•°æ®èŒƒå›´: è¿‘ {self.days_back} å¤©")
        print(f"{'='*60}")
        
        # 1. å‡†å¤‡æ•°æ®
        X, returns_dict, drawdowns_dict, groups, feature_names, info_df = self.prepare_dataset()
        
        if X is None:
            return {'status': 'failed', 'reason': 'æ•°æ®å‡†å¤‡å¤±è´¥'}
        
        results = {'status': 'success', 'samples': len(X), 'features': len(feature_names)}
        
        # 2. è®­ç»ƒæ”¶ç›Šé¢„æµ‹æ¨¡å‹
        print("\n" + "="*60)
        return_predictor = ReturnPredictor()
        return_metrics = return_predictor.train(X, returns_dict, feature_names, groups=groups)
        return_predictor.save(str(self.model_dir))
        results['return_predictor'] = return_metrics
        results['label_cost_profile'] = self.label_cost_profile
        
        # 3. è®­ç»ƒæ’åºæ¨¡å‹
        print("\n" + "="*60)
        ranker = SignalRanker()
        ranker_metrics = ranker.train(X, returns_dict, drawdowns_dict, groups, feature_names)
        ranker.save(str(self.model_dir))
        results['signal_ranker'] = {h.value: m for h, m in ranker_metrics.items()}
        
        # 4. ä¿å­˜ç‰¹å¾åç§°
        import json
        with open(self.model_dir / "feature_names.json", 'w') as f:
            json.dump(feature_names, f)
        if self.label_cost_profile:
            with open(self.model_dir / "training_cost_profile.json", 'w') as f:
                json.dump(self.label_cost_profile, f, indent=2)
        with open(self.model_dir / "training_objective.json", 'w') as f:
            json.dump(self.objective_config, f, indent=2)
        if self.feature_stability_report:
            with open(self.model_dir / "feature_stability_report.json", 'w') as f:
                json.dump(self.feature_stability_report, f, indent=2)

        # Walk-forward ç¨³å¥æ€§éªŒè¯
        self.walk_forward_report = self._run_walk_forward_eval(X, returns_dict, groups)
        with open(self.model_dir / "walk_forward_report.json", 'w') as f:
            json.dump(self.walk_forward_report, f, indent=2)
        results['feature_stability'] = self.feature_stability_report
        results['walk_forward'] = self.walk_forward_report
        
        print(f"\n{'='*60}")
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {self.model_dir}")
        print(f"{'='*60}")
        
        # 5. ä¸Šä¼ åˆ° Hub (å¯é€‰)
        if upload:
            try:
                from ml.model_registry import get_registry
                registry = get_registry()
                # TODO: å®ç°æ‰¹é‡ä¸Šä¼ 
                print("ğŸ“¤ ä¸Šä¼ åŠŸèƒ½å¾…å®ç°")
            except Exception as e:
                print(f"âš ï¸ ä¸Šä¼ å¤±è´¥: {e}")
        
        return results


def train_pipeline(market: str = 'US',
                   days_back: int = 180,
                   upload: bool = False,
                   commission_bps: float = 5.0,
                   slippage_bps: float = 10.0,
                   price_tier: str = 'standard'):
    """ä¾¿æ·è®­ç»ƒå‡½æ•°"""
    pipeline = MLPipeline(
        market=market,
        days_back=days_back,
        commission_bps=commission_bps,
        slippage_bps=slippage_bps,
        price_tier=price_tier,
    )
    return pipeline.train_all(upload=upload)


def train_all_tiers(market: str = 'US', days_back: int = 365, **kwargs):
    """è®­ç»ƒæ‰€æœ‰ä»·æ ¼åˆ†å±‚æ¨¡å‹"""
    all_results = {}
    for tier in ['standard', 'penny']:
        print(f"\n{'#'*60}")
        print(f"## è®­ç»ƒ {tier.upper()} æ¨¡å‹ ({market})")
        print(f"{'#'*60}")
        result = train_pipeline(market=market, days_back=days_back, price_tier=tier, **kwargs)
        all_results[tier] = result
    return all_results


# === å‘½ä»¤è¡Œå…¥å£ ===
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ML è®­ç»ƒç®¡é“')
    parser.add_argument('--market', type=str, default='US', choices=['US', 'CN'])
    parser.add_argument('--days', type=int, default=180, help='æ•°æ®å¤©æ•°')
    parser.add_argument('--upload', action='store_true', help='ä¸Šä¼ åˆ° Hub')
    parser.add_argument('--fetch', action='store_true', help='å…ˆæ‹‰å–å†å²æ•°æ®')
    parser.add_argument('--commission-bps', type=float, default=5.0, help='å•è¾¹æ‰‹ç»­è´¹ (bps)')
    parser.add_argument('--slippage-bps', type=float, default=10.0, help='å•è¾¹æ»‘ç‚¹ (bps)')
    parser.add_argument('--tier', type=str, default='standard', choices=['standard', 'penny', 'all'],
                        help='ä»·æ ¼åˆ†å±‚: standard(>=5), penny(<5), all(å…¨éƒ¨)')
    parser.add_argument('--all-tiers', action='store_true', help='è®­ç»ƒæ‰€æœ‰åˆ†å±‚æ¨¡å‹')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ•°æ®åº“
    from db.database import init_db, query_scan_results, get_scanned_dates
    try:
        init_db()
        print("âœ… å†å²æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ æ•°æ®åº“åˆå§‹åŒ–: {e}")
    
    if args.all_tiers:
        results = train_all_tiers(
            market=args.market, days_back=args.days,
            commission_bps=args.commission_bps, slippage_bps=args.slippage_bps,
        )
        for tier, r in results.items():
            print(f"\n{tier}: samples={r.get('samples')}, status={r.get('status')}")
    else:
        pipeline = MLPipeline(
            market=args.market,
            days_back=args.days,
            commission_bps=args.commission_bps,
            slippage_bps=args.slippage_bps,
            price_tier=args.tier,
        )
        
        if args.fetch:
            dates = get_scanned_dates(market=args.market)
            symbols = set()
            for d in dates[:30]:
                results = query_scan_results(scan_date=d, market=args.market, limit=1000)
                for r in results:
                    symbols.add(r.get('symbol', ''))
            symbols = sorted([s for s in symbols if s])
            print(f"   æ‰¾åˆ° {len(symbols)} åªè‚¡ç¥¨")
            pipeline.fetch_and_store_history(symbols)
        
        results = pipeline.train_all(upload=args.upload)
        print(f"\nç»“æœ: {results}")
