"""
æ¨¡å‹è®­ç»ƒè„šæœ¬ (Model Training Script)
=====================================
åŠ è½½ Feature Pipeline ç”Ÿæˆçš„æ•°æ®é›†ï¼Œè®­ç»ƒ XGBoost Ranker æ¨¡å‹ã€‚
è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ—¶é—´èŒƒå›´å†…çš„æ’åºèƒ½åŠ› (NDCG@10)ã€‚

æµç¨‹:
1. åŠ è½½ data/ml/dataset_v1.joblib
2. åˆå§‹åŒ– SignalRanker (Learning to Rank)
3. è®­ç»ƒæ¨¡å‹ (Short/Mid/Long Term horizons)
4. è¾“å‡ºè¯„ä¼°æŒ‡æ ‡ (NDCG, Top-K Return)
5. ä¿å­˜æ¨¡å‹åˆ° versions/v3/ml/models/
"""

import os
import sys
import joblib
import numpy as np
import pandas as pd
from datetime import datetime

# Add parent dir to path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from ml.models.signal_ranker import SignalRanker

# é…ç½®
DATASET_PATH = os.path.join(parent_dir, 'data', 'ml', 'dataset_v1.joblib')
MODEL_DIR = os.path.join(parent_dir, 'ml', 'models', 'trained')

def train():
    print("ğŸš€ Starting Model Training Pipeline...")
    
    # 1. Load Dataset
    if not os.path.exists(DATASET_PATH):
        print(f"âŒ Dataset not found at {DATASET_PATH}. Run train_data_pipeline.py first.")
        return
        
    print(f"ğŸ“‚ Loading dataset from {DATASET_PATH}...")
    data = joblib.load(DATASET_PATH)
    
    X = data['X']
    returns = data['returns']
    drawdowns = data['drawdowns']
    groups = data['groups']
    feature_names = data['feature_names']
    meta = data['meta']
    
    print(f"âœ… Data loaded. Samples: {X.shape[0]}, Features: {X.shape[1]}")
    print(f"   Unique Groups (Dates): {len(groups)}")
    
    # 2. Add Meta info to X (Wait, SignalRanker doesn't use meta directly for training features, 
    #    but we might need date for TimeSeriesSplit inside train)
    #    Actually SignalRanker internal logic uses groups array to split, assuming groups are chronological.
    #    Since we sorted by Date in pipeline, groups are chronological. This is correct.
    
    # Create per-sample group IDs from counts
    # The ranker expects an array aligned with X, indicating which group each sample belongs to.
    group_counts = groups
    group_ids = []
    for i, count in enumerate(group_counts):
        group_ids.extend([i] * count)
    group_ids = np.array(group_ids)
    
    print(f"   Expanded Groups: {len(group_ids)} samples (aligned with X)")

    # Handle numeric instability
    print("ğŸ§¹ Cleaning infinite values...")
    is_inf = np.isinf(X)
    if np.any(is_inf):
        print(f"   Found {np.sum(is_inf)} infinite values, replacing with NaN")
        X[is_inf] = np.nan

    # 3. Initialize Ranker
    ranker = SignalRanker()
    
    # 4. Train
    print("\nğŸ§  Training SignalRanker (XGBoost LTR)...")
    start_time = datetime.now()
    
    metrics = ranker.train(
        X=X,
        returns_dict=returns,
        drawdowns_dict=drawdowns,
        groups=group_ids,  # Pass per-sample IDs
        feature_names=feature_names
    )
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print(f"\nâœ… Training completed in {duration:.1f}s")
    
    # 5. Show Metrics
    print("\nğŸ† Model Performance (Start Date Split Validation):")
    for horizon, m in metrics.items():
        print(f"\n   Horizon: {horizon}")
        print(f"     NDCG@10:       {m.get('ndcg@10', 0):.4f} (Random guess is usually around 0.3-0.5 depending on distribution)")
        print(f"     Top-10 Return: {m.get('top10_avg_return', 0)*100:.2f}% (Avg return of top 10 predicted stocks)")
        
    # 6. Save Model
    os.makedirs(MODEL_DIR, exist_ok=True)
    save_path = os.path.join(MODEL_DIR, 'signal_ranker_v1')
    ranker.save(save_path)
    print(f"\nğŸ’¾ Model saved to {save_path}")

if __name__ == "__main__":
    train()
