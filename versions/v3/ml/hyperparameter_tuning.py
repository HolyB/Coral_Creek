"""
è¶…å‚æ•°è°ƒä¼˜æ¨¡å—
Hyperparameter Tuning

ä½¿ç”¨ GridSearch / RandomizedSearch æ‰¾æœ€ä¼˜å‚æ•°
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import json
import time

try:
    import xgboost as xgb
    from sklearn.model_selection import (
        GridSearchCV, RandomizedSearchCV, 
        cross_val_score, TimeSeriesSplit
    )
    from sklearn.metrics import make_scorer, mean_squared_error, r2_score
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False


class HyperparameterTuner:
    """è¶…å‚æ•°è°ƒä¼˜å™¨"""
    
    # XGBoost å›å½’å‚æ•°æœç´¢ç©ºé—´
    REGRESSOR_PARAM_GRID = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 4, 5, 6, 7],
        'learning_rate': [0.01, 0.03, 0.05, 0.1],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'min_child_weight': [1, 3, 5],
        'gamma': [0, 0.1, 0.2],
    }
    
    # å¿«é€Ÿæœç´¢ (è¾ƒå°çš„ç©ºé—´)
    REGRESSOR_PARAM_GRID_FAST = {
        'n_estimators': [100, 200],
        'max_depth': [4, 5, 6],
        'learning_rate': [0.03, 0.05, 0.1],
        'subsample': [0.8],
        'colsample_bytree': [0.8],
    }
    
    # XGBoost Ranker å‚æ•°æœç´¢ç©ºé—´
    RANKER_PARAM_GRID = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 4, 5, 6],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
    }
    
    def __init__(self):
        self.best_params: Dict[str, Dict] = {}
        self.cv_results: Dict[str, pd.DataFrame] = {}
        self.tuning_history: List[Dict] = []
        
    def tune_regressor(self, 
                       X: np.ndarray, 
                       y: np.ndarray,
                       horizon: str = '5d',
                       method: str = 'random',
                       n_iter: int = 50,
                       cv: int = 5,
                       fast: bool = True) -> Dict:
        """
        è°ƒä¼˜æ”¶ç›Šé¢„æµ‹æ¨¡å‹
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾
            horizon: é¢„æµ‹å‘¨æœŸ
            method: 'grid' æˆ– 'random'
            n_iter: RandomizedSearch è¿­ä»£æ¬¡æ•°
            cv: äº¤å‰éªŒè¯æŠ˜æ•°
            fast: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæœç´¢ç©ºé—´
        
        Returns:
            æœ€ä¼˜å‚æ•°å’Œè¯„ä¼°ç»“æœ
        """
        if not ML_AVAILABLE:
            raise RuntimeError("sklearn/xgboost æœªå®‰è£…")
        
        print(f"\n{'='*50}")
        print(f"ğŸ”§ è°ƒä¼˜ ReturnPredictor ({horizon})")
        print(f"   æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {X.shape[1]}")
        print(f"   æ–¹æ³•: {method}, CV: {cv} æŠ˜")
        print(f"{'='*50}\n")
        
        # è¿‡æ»¤æ— æ•ˆæ ·æœ¬
        valid_mask = ~np.isnan(y)
        X_valid = X[valid_mask]
        y_valid = y[valid_mask]
        
        if len(X_valid) < 100:
            print("âŒ æ ·æœ¬ä¸è¶³")
            return {}
        
        # åŸºç¡€æ¨¡å‹
        base_model = xgb.XGBRegressor(random_state=42)
        
        # é€‰æ‹©å‚æ•°ç©ºé—´
        param_grid = self.REGRESSOR_PARAM_GRID_FAST if fast else self.REGRESSOR_PARAM_GRID
        
        # è‡ªå®šä¹‰è¯„åˆ†å‡½æ•° (æ–¹å‘å‡†ç¡®ç‡)
        def direction_accuracy(y_true, y_pred):
            return ((y_pred > 0) == (y_true > 0)).mean()
        
        direction_scorer = make_scorer(direction_accuracy)
        
        # æœç´¢
        start_time = time.time()
        
        if method == 'grid':
            search = GridSearchCV(
                base_model,
                param_grid,
                cv=cv,
                scoring={
                    'r2': 'r2',
                    'neg_mse': 'neg_mean_squared_error',
                    'direction': direction_scorer
                },
                refit='direction',  # ä»¥æ–¹å‘å‡†ç¡®ç‡ä¸ºä¸»
                n_jobs=-1,
                verbose=1
            )
        else:
            search = RandomizedSearchCV(
                base_model,
                param_grid,
                n_iter=n_iter,
                cv=cv,
                scoring={
                    'r2': 'r2',
                    'neg_mse': 'neg_mean_squared_error',
                    'direction': direction_scorer
                },
                refit='direction',
                n_jobs=-1,
                verbose=1,
                random_state=42
            )
        
        search.fit(X_valid, y_valid)
        
        elapsed = time.time() - start_time
        
        # ç»“æœ
        best_params = search.best_params_
        best_score = search.best_score_
        
        print(f"\nâœ… è°ƒä¼˜å®Œæˆ ({elapsed:.1f}ç§’)")
        print(f"\nğŸ“Š æœ€ä¼˜å‚æ•°:")
        for k, v in best_params.items():
            print(f"   {k}: {v}")
        
        print(f"\nğŸ“ˆ æœ€ä¼˜å¾—åˆ† (æ–¹å‘å‡†ç¡®ç‡): {best_score:.3f}")
        
        # è·å– CV ç»“æœ
        cv_results = pd.DataFrame(search.cv_results_)
        
        # ä¿å­˜
        self.best_params[f'regressor_{horizon}'] = best_params
        self.cv_results[f'regressor_{horizon}'] = cv_results
        
        # ä¸é»˜è®¤å‚æ•°å¯¹æ¯”
        default_model = xgb.XGBRegressor(
            n_estimators=200, max_depth=5, learning_rate=0.05,
            subsample=0.8, colsample_bytree=0.8, random_state=42
        )
        default_scores = cross_val_score(
            default_model, X_valid, y_valid, cv=cv, scoring=direction_scorer
        )
        
        improvement = (best_score - default_scores.mean()) * 100
        
        print(f"\nğŸ“Š å¯¹æ¯”é»˜è®¤å‚æ•°:")
        print(f"   é»˜è®¤å¾—åˆ†: {default_scores.mean():.3f} (Â±{default_scores.std():.3f})")
        print(f"   æœ€ä¼˜å¾—åˆ†: {best_score:.3f}")
        print(f"   æå‡: {improvement:+.1f}%")
        
        result = {
            'horizon': horizon,
            'best_params': best_params,
            'best_score': best_score,
            'default_score': default_scores.mean(),
            'improvement': improvement,
            'elapsed_seconds': elapsed,
            'n_candidates': len(cv_results)
        }
        
        self.tuning_history.append(result)
        
        return result
    
    def tune_all_regressors(self, 
                            X: np.ndarray, 
                            y_dict: Dict[str, np.ndarray],
                            method: str = 'random',
                            n_iter: int = 30,
                            fast: bool = True) -> Dict[str, Dict]:
        """è°ƒä¼˜æ‰€æœ‰å‘¨æœŸçš„æ”¶ç›Šé¢„æµ‹æ¨¡å‹"""
        
        results = {}
        
        for horizon, y in y_dict.items():
            try:
                result = self.tune_regressor(
                    X, y, horizon, method, n_iter, fast=fast
                )
                results[horizon] = result
            except Exception as e:
                print(f"âŒ {horizon} è°ƒä¼˜å¤±è´¥: {e}")
        
        return results
    
    def get_best_params(self, model_type: str = 'regressor', horizon: str = '5d') -> Dict:
        """è·å–æœ€ä¼˜å‚æ•°"""
        key = f'{model_type}_{horizon}'
        return self.best_params.get(key, {})
    
    def save_results(self, path: str):
        """ä¿å­˜è°ƒä¼˜ç»“æœ"""
        save_dir = Path(path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æœ€ä¼˜å‚æ•°
        with open(save_dir / 'best_params.json', 'w') as f:
            json.dump(self.best_params, f, indent=2)
        
        # ä¿å­˜è°ƒä¼˜å†å²
        with open(save_dir / 'tuning_history.json', 'w') as f:
            json.dump(self.tuning_history, f, indent=2)
        
        # ä¿å­˜ CV ç»“æœ
        for name, df in self.cv_results.items():
            df.to_csv(save_dir / f'cv_results_{name}.csv', index=False)
        
        print(f"âœ… è°ƒä¼˜ç»“æœå·²ä¿å­˜: {path}")
    
    def load_results(self, path: str) -> bool:
        """åŠ è½½è°ƒä¼˜ç»“æœ"""
        save_dir = Path(path)
        
        params_path = save_dir / 'best_params.json'
        if not params_path.exists():
            return False
        
        with open(params_path) as f:
            self.best_params = json.load(f)
        
        history_path = save_dir / 'tuning_history.json'
        if history_path.exists():
            with open(history_path) as f:
                self.tuning_history = json.load(f)
        
        print(f"âœ… å·²åŠ è½½è°ƒä¼˜ç»“æœ: {list(self.best_params.keys())}")
        return True


def run_tuning(market: str = 'US', fast: bool = True) -> Dict:
    """
    è¿è¡Œå®Œæ•´çš„è¶…å‚æ•°è°ƒä¼˜æµç¨‹
    
    Args:
        market: å¸‚åœº (US/CN)
        fast: æ˜¯å¦å¿«é€Ÿæ¨¡å¼
    
    Returns:
        è°ƒä¼˜ç»“æœ
    """
    from ml.pipeline import MLPipeline
    
    print("\n" + "="*60)
    print("ğŸ”§ å¼€å§‹è¶…å‚æ•°è°ƒä¼˜")
    print("="*60)
    
    # 1. å‡†å¤‡æ•°æ®
    print("\nğŸ“¦ å‡†å¤‡æ•°æ®é›†...")
    pipeline = MLPipeline(market=market)
    X, returns_dict, drawdowns_dict, groups, feature_names, signals_df = pipeline.prepare_dataset()
    
    if X is None or len(X) == 0:
        print("âŒ æ— æ³•å‡†å¤‡æ•°æ®")
        return {}
    
    print(f"   æ ·æœ¬æ•°: {len(X)}")
    print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
    
    # 2. è°ƒä¼˜
    tuner = HyperparameterTuner()
    
    results = tuner.tune_all_regressors(
        X, returns_dict,
        method='random',
        n_iter=30 if fast else 100,
        fast=fast
    )
    
    # 3. ä¿å­˜ç»“æœ
    save_path = Path(__file__).parent / 'tuning_results' / f'{market.lower()}'
    tuner.save_results(str(save_path))
    
    # 4. æ±‡æ€»
    print("\n" + "="*60)
    print("ğŸ“Š è°ƒä¼˜ç»“æœæ±‡æ€»")
    print("="*60)
    
    for horizon, result in results.items():
        if result:
            print(f"\n{horizon}:")
            print(f"  æœ€ä¼˜å‚æ•°: {result['best_params']}")
            print(f"  æ–¹å‘å‡†ç¡®ç‡: {result['best_score']:.3f} (æå‡ {result['improvement']:+.1f}%)")
    
    return results


# === å‘½ä»¤è¡Œå…¥å£ ===
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è¶…å‚æ•°è°ƒä¼˜')
    parser.add_argument('--market', type=str, default='US', help='å¸‚åœº')
    parser.add_argument('--fast', action='store_true', help='å¿«é€Ÿæ¨¡å¼')
    
    args = parser.parse_args()
    
    run_tuning(market=args.market, fast=args.fast)
