name: ML Model Training

on:
  schedule:
    # æ¯å‘¨ä¸€å‡Œæ™¨3ç‚¹ (UTC) è®­ç»ƒä¸€æ¬¡
    - cron: '0 3 * * 1'
  workflow_dispatch:  # å…è®¸æ‰‹åŠ¨è§¦å‘

# æƒé™ - å…è®¸å†™å…¥ä»“åº“ (æäº¤æ¨¡å‹æ–‡ä»¶)
permissions:
  contents: write

jobs:
  train:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: main
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install xgboost lightgbm scikit-learn pandas numpy joblib
          pip install polygon-api-client requests yfinance tqdm
          pip install supabase
      
      - name: Train SmartPicker Models (pipeline.py)
        working-directory: versions/v3
        env:
          PYTHONPATH: ${{ github.workspace }}/versions/v3
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "ğŸš€ Training SmartPicker models via ml/pipeline.py"
          echo "   This trains ReturnPredictor + SignalRanker (101 features)"
          echo ""
          
          python ml/pipeline.py --market US --days 180
          
          echo ""
          echo "ğŸ“‚ Model files:"
          ls -la ml/saved_models/v2_us/ || echo "No v2_us models"
      
      - name: Verify models
        working-directory: versions/v3
        run: |
          python -c "
          import joblib, json, os
          
          model_dir = 'ml/saved_models/v2_us'
          
          # æ£€æŸ¥å…³é”®æ¨¡å‹æ–‡ä»¶
          required = [
              'return_5d.joblib',
              'ranker_short.joblib',
              'ranker_medium.joblib',
              'ranker_long.joblib',
              'feature_names.json',
              'training_cost_profile.json'
          ]
          missing = []
          for f in required:
              path = os.path.join(model_dir, f)
              if os.path.exists(path):
                  size = os.path.getsize(path)
                  print(f'  âœ… {f} ({size:,} bytes)')
              else:
                  print(f'  âŒ MISSING: {f}')
                  missing.append(f)
          
          # éªŒè¯ç‰¹å¾æ•°
          fn_path = os.path.join(model_dir, 'feature_names.json')
          if os.path.exists(fn_path):
              with open(fn_path) as f:
                  features = json.load(f)
              print(f'  ğŸ“Š Features: {len(features)}')
          
          # éªŒè¯æ¨¡å‹å¯åŠ è½½
          model_path = os.path.join(model_dir, 'return_5d.joblib')
          if os.path.exists(model_path):
              model = joblib.load(model_path)
              print(f'  âœ… return_5d model loaded: {type(model).__name__}')

          if missing:
              raise SystemExit(f'âŒ Missing required artifacts: {missing}')
          "
      
      - name: Commit updated models
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          git add versions/v3/ml/saved_models/ || true
          
          if git diff --staged --quiet; then
            echo "No model changes to commit"
          else
            git commit -m "ğŸ§  Auto-update: ML models retrained $(date +%Y-%m-%d)"
            
            if ! git pull --rebase origin main; then
              git rebase --abort || true
              git pull -X ours origin main || true
            fi
            git push origin HEAD:main
          fi
      
      - name: Upload models to HuggingFace Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        working-directory: versions/v3
        run: |
          pip install huggingface_hub
          python -c "
          from huggingface_hub import HfApi, login, create_repo
          import os, glob
          
          token = os.environ.get('HF_TOKEN')
          if not token:
              print('âš ï¸ HF_TOKEN not set, skipping upload')
              exit(0)
          
          login(token=token)
          api = HfApi()
          
          repo_id = 'HolyBert/coral-creek-models'
          
          try:
              create_repo(repo_id, repo_type='model', private=True, exist_ok=True)
          except Exception as e:
              print(f'Repo check: {e}')
          
          # ä¸Šä¼  v2_us æ¨¡å‹ç›®å½•
          model_dir = 'ml/saved_models/v2_us'
          if os.path.exists(model_dir):
              for f in os.listdir(model_dir):
                  fpath = os.path.join(model_dir, f)
                  if os.path.isfile(fpath):
                      api.upload_file(
                          path_or_fileobj=fpath,
                          path_in_repo=f'v2_us/{f}',
                          repo_id=repo_id,
                          repo_type='model'
                      )
                      print(f'  âœ… Uploaded: v2_us/{f}')
          
          print('ğŸ‰ Upload complete!')
          "
      
      - name: Summary
        run: |
          echo "ğŸ‰ ML Training Complete!"
          echo "Models: ReturnPredictor (1d/5d/10d/30d) + SignalRanker (short/medium/long)"
          echo "Features: 101 technical indicators"
          echo "Used by: SmartPicker in AIä¸­å¿ƒ and ä»Šæ—¥ç²¾é€‰"
