name: ML Model Training

on:
  schedule:
    # æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹ (UTC) = åŒ—äº¬æ—¶é—´å‘¨æ—¥ 11:00 AM
    - cron: '0 3 * * 0'
  workflow_dispatch:  # å…è®¸æ‰‹åŠ¨è§¦å‘
    inputs:
      days_back:
        description: 'è®­ç»ƒæ•°æ®å¤©æ•° (0=å…¨éƒ¨)'
        required: false
        default: '0'
      tier:
        description: 'ä»·æ ¼åˆ†å±‚ (all-tiers/standard/penny)'
        required: false
        default: 'all-tiers'

# æƒé™ - å…è®¸å†™å…¥ä»“åº“ (æäº¤æ¨¡å‹æ–‡ä»¶)
permissions:
  contents: write

jobs:
  train:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: main
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install xgboost lightgbm scikit-learn pandas numpy joblib
          pip install polygon-api-client requests yfinance tqdm
          pip install supabase
      
      - name: Download database from Supabase
        working-directory: versions/v3
        env:
          PYTHONPATH: ${{ github.workspace }}/versions/v3
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "ğŸ“¥ Syncing scan_results from Supabase..."
          python -c "
          from db.database import init_db
          init_db()
          print('âœ… Database initialized')
          "
      
      - name: Fetch stock history
        working-directory: versions/v3
        env:
          PYTHONPATH: ${{ github.workspace }}/versions/v3
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "ğŸ“ˆ Fetching stock history for training..."
          python -c "
          from db.database import init_db, get_scanned_dates, query_scan_results
          from ml.pipeline import MLPipeline
          from collections import Counter
          
          init_db()
          dates = get_scanned_dates(market='US')
          
          # æ”¶é›†æœ€é¢‘ç¹çš„ 200 ä¸ªæ ‡çš„
          counts = Counter()
          for d in dates[:60]:
              results = query_scan_results(scan_date=d, market='US', limit=500)
              for r in results:
                  sym = r.get('symbol', '')
                  price = float(r.get('price', 0) or 0)
                  if sym and price > 0:
                      counts[sym] += 1
          
          symbols = [s for s, _ in counts.most_common(300)]
          print(f'Fetching history for {len(symbols)} symbols...')
          
          pipeline = MLPipeline(market='US')
          pipeline.fetch_and_store_history(symbols, days=400, batch_size=10)
          "
      
      - name: Train models
        working-directory: versions/v3
        env:
          PYTHONPATH: ${{ github.workspace }}/versions/v3
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          DAYS="${{ github.event.inputs.days_back || '9999' }}"
          TIER="${{ github.event.inputs.tier || 'all-tiers' }}"
          
          echo "ğŸš€ Training ML models"
          echo "   Days: $DAYS"
          echo "   Tier: $TIER"
          
          if [ "$TIER" = "all-tiers" ]; then
            python ml/pipeline.py --market US --days "$DAYS" --all-tiers
          else
            python ml/pipeline.py --market US --days "$DAYS" --tier "$TIER"
          fi
          
          echo ""
          echo "ğŸ“‚ Model files:"
          ls -la ml/saved_models/v2_us/ || echo "No v2_us models"
          ls -la ml/saved_models/v2_us_penny/ 2>/dev/null || echo "No penny models"
      
      - name: Verify models
        working-directory: versions/v3
        run: |
          python -c "
          import joblib, json, os
          
          for tier_dir in ['ml/saved_models/v2_us', 'ml/saved_models/v2_us_penny']:
              if not os.path.exists(tier_dir):
                  continue
              tier = tier_dir.split('/')[-1]
              print(f'\nğŸ“Š {tier}:')
              
              for f in sorted(os.listdir(tier_dir)):
                  path = os.path.join(tier_dir, f)
                  size = os.path.getsize(path)
                  print(f'  âœ… {f} ({size:,} bytes)')
              
              fn_path = os.path.join(tier_dir, 'feature_names.json')
              if os.path.exists(fn_path):
                  with open(fn_path) as fh:
                      features = json.load(fh)
                  print(f'  Features: {len(features)}')
              
              model_path = os.path.join(tier_dir, 'return_5d.joblib')
              if os.path.exists(model_path):
                  model = joblib.load(model_path)
                  print(f'  Model loaded: {type(model).__name__}')
          "
      
      - name: Commit updated models
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          git add versions/v3/ml/saved_models/ || true
          
          if git diff --staged --quiet; then
            echo "No model changes to commit"
          else
            COMMIT_MSG="ğŸ§  Auto-retrain: ML models $(date +%Y-%m-%d)"
            git commit -m "$COMMIT_MSG"
            
            if ! git pull --rebase origin main; then
              git rebase --abort || true
              git pull -X ours origin main || true
            fi
            git push origin HEAD:main
          fi
      
      - name: Upload models to HuggingFace Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        working-directory: versions/v3
        run: |
          pip install huggingface_hub
          python -c "
          from huggingface_hub import HfApi, login, create_repo
          import os
          
          token = os.environ.get('HF_TOKEN')
          if not token:
              print('âš ï¸ HF_TOKEN not set, skipping upload')
              exit(0)
          
          login(token=token)
          api = HfApi()
          repo_id = 'HolyBert/coral-creek-models'
          
          try:
              create_repo(repo_id, repo_type='model', private=True, exist_ok=True)
          except Exception as e:
              print(f'Repo check: {e}')
          
          for tier_dir in ['ml/saved_models/v2_us', 'ml/saved_models/v2_us_penny']:
              if not os.path.exists(tier_dir):
                  continue
              tier = tier_dir.split('/')[-1]
              for f in os.listdir(tier_dir):
                  fpath = os.path.join(tier_dir, f)
                  if os.path.isfile(fpath):
                      api.upload_file(
                          path_or_fileobj=fpath,
                          path_in_repo=f'{tier}/{f}',
                          repo_id=repo_id,
                          repo_type='model'
                      )
                      print(f'  âœ… Uploaded: {tier}/{f}')
          
          print('ğŸ‰ Upload complete!')
          "
      
      - name: Summary
        run: |
          echo "ğŸ‰ ML Training Complete!"
          echo "Models: Standard (>=\$5) + Penny (<\$5)"
          echo "Horizons: ReturnPredictor (5d/20d) + SignalRanker (short/medium)"
          echo "Features: 110 technical + fundamental indicators"
          echo "Used by: SmartPicker (auto-selects model by stock price)"
